- [2 OLMo 2 Furious](https://arxiv.org/pdf/2501.00656)
- [1.58-bit FLUX](https://www.arxiv.org/pdf/2412.18653)
- [Memory Layers at Scale](https://arxiv.org/pdf/2412.09764)
- [Agents Are Not Enough](https://arxiv.org/html/2412.16241v1)
- [Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs](https://arxiv.org/pdf/2412.21187)
- [IsarStep: a Benchmark for High-level Mathematical Reasoning](https://arxiv.org/pdf/2006.09265)
- [DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/pdf/2412.17498)
- [LearnLM: Improving Gemini for Learning](https://www.arxiv.org/pdf/2412.16429)
- [DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)
- [Large Concept Models: Language Modeling in a Sentence Representation Space](https://arxiv.org/pdf/2412.08821)
- [Explore Theory of Mind: Program-guided adversarial data generation for theory of mind reasoning](https://arxiv.org/pdf/2412.12175)
- [Reinforcement Learning: An Overview](https://arxiv.org/pdf/2412.05265)
- [GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations](https://arxiv.org/pdf/1907.13052)
- [AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation](https://arxiv.org/pdf/2410.06943)
- [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/pdf/2412.14161)
- [Alignment faking in large language models](https://arxiv.org/pdf/2412.14093)
- [Qwen2.5 Technical Report](https://arxiv.org/pdf/2412.15115)
- [Precise Length Control in Large Language Models](https://arxiv.org/pdf/2412.11937)
- [MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification](https://arxiv.org/pdf/2412.04494)
- [AutoReason: Automatic Few-Shot Reasoning Decomposition](https://arxiv.org/pdf/2412.06975)
- [Does RLHF Scale? Exploring the Impacts from Data, Model, and Method](https://arxiv.org/html/2412.06000#:~:text=As%20a%20result%2C%20current%20RLHF,for%20reinforcement%20learning%20of%20LLMs.)
- [Phi-4 Technical Report](https://arxiv.org/html/2412.08905v1)
- [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/pdf/2412.09871)
- [A Survey on LLM-as-a-Judge](https://arxiv.org/pdf/2411.15594)
- [Nvidia launches new AI development tools for autonomous robots and vehicles](https://cur.at/xmbDGsT?m=web)

-------------
- [FaceXBench: Evaluating Multimodal LLMs on Face Understanding](http://arxiv.org/abs/2501.10360)
