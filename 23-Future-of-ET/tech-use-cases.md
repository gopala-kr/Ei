

-----

- [highscalability](http://highscalability.com/)
- [scalability](https://github.com/binhnguyennus/awesome-scalability)
- [digitalistmag](https://www.digitalistmag.com/)
- [techcrunch](https://techcrunch.com/)
- [Gartner cio](https://www.gartner.com/en/information-technology/insights/trending-topics)
- [joelonsoftware](https://www.joelonsoftware.com/)
- [ctovision](https://ctovision.com/)
- [cioinsight](https://www.cioinsight.com/)
- [CIO Reddit](https://old.reddit.com/r/CIO/)
- [cio.com](https://www.cio.com/in/)
- [CIOs, IT & Security](https://www.vertitechit.com/best-it-blogs-50-must-read-blogs-for-cios-and-it-pros-1/)
- [cisoforum](https://www.cisoforum.com/blog/)
- [avoa.com/](https://avoa.com/)
- [futureofcio](http://futureofcio.blogspot.com/)
- [starcio](https://blogs.starcio.com/)
- [wsj-cio-journal](https://www.wsj.com/news/cio-journal)
- [forrester-cio-insights](https://go.forrester.com/blogs/category/chief-information-officer-cio/)
- [cxotalk](https://www.cxotalk.com/)
- [moderncto](https://moderncto.io/)
- [thectoadvisor](https://thectoadvisor.com/blog/)
- [ctothink](https://www.ctothink.com/episodes)
- [searchcio](https://searchcio.techtarget.com/)
- [dynamiccio](https://www.dynamiccio.com/)
- [theceo](https://theceo.in/)
- [chiefexecutive](https://chiefexecutive.net/)
- [entrepreneur-ceos](https://www.entrepreneur.com/topic/ceos)
- [forbes](https://www.forbes.com/worlds-billionaires/?sh=812fae158649)
- [Technology Industry](https://www.cio.com/category/technology-business/)
- [zdnet](https://www.zdnet.com/topic/enterprise-software/)
- [gigaom](https://gigaom.com/)
- [diginomica](https://diginomica.com/)
- [computerworld](https://www.computerworld.com/in/)
- [hbr.org](https://hbr.org/)
- [technologyreview](https://www.technologyreview.com/)
- [biztechmagazine](https://biztechmagazine.com/)
- [thenextweb](https://thenextweb.com/)
- [cio.economictimes](https://cio.economictimes.indiatimes.com/)
- [IDG-cio](https://www.idg.com/our-brands/cio/)
- [ciostory](https://ciostory.com/)
- [enterprisersproject](https://enterprisersproject.com/)

------

- [knowledge.wharton](https://knowledge.wharton.upenn.edu/topic/finance/)
- [studyfinance](https://studyfinance.com/finance/)
- [supercfo](https://supercfo.com/)
- [mycfo](https://mycfo.in/)
- [cfo](https://www.cfo.com/)
- [cfo-magazine](https://www.cfo.com/cfo-magazine/)
- [cfodailynews](https://www.cfodailynews.com/)
- [cfodive](https://www.cfodive.com/)
- [cfocentre](https://www.cfocentre.co.in/our-approach/)
- [globalcfo](https://globalcfo.in/blog/)
- [starterscfo](https://www.starterscfo.com/)
- [fingurus](https://fingurus.com/)
- [smecfo](http://www.smecfo.asia/what-we-do/)
- [nowcfo](https://www.nowcfo.com/services/)
- [financialdirector](https://www.financialdirector.co.uk/)
- [adamodar](http://pages.stern.nyu.edu/~adamodar/) 
- [AswathDamodaranon](https://www.youtube.com/c/AswathDamodaranonValuation/playlists)

------------

- https://www.iresearchservices.com/
- https://endpts.com/
- https://opsdog.com/
- https://www.statista.com/
- https://pitchbook.com/
- https://www.ibisworld.com/
- https://www.cbinsights.com/
- https://www.thinkwithgoogle.com/
- https://adage.com/
- https://www.oecd-ilibrary.org/
- https://www.crunchbase.com/
- https://www.idc.com/
- https://www.gsma.com/
- https://go.forrester.com/
- https://www.emarketer.com/
- https://energycentral.com/
- https://professional.dowjones.com/factiva/
- https://www.cta.tech/
- https://www.computereconomics.com/
- https://www.autonews.com/
- https://hbr.org/
- https://www.economist.com/
- https://fortune.com/fortune500/
- https://www.wsj.com/
- https://www.newsweek.com/
- https://www.factset.com/
- https://www.mckinsey.com/quarterly/overview#
- https://sloanreview.mit.edu/
- https://www.goldmansachs.com/insights/
- https://www.moodys.com/
- https://remesh.ai/
- https://www.spglobal.com/en/
- https://www.esg-global.com/research
- https://mattermark.com/
- https://a16z.com/
- https://www.bccresearch.com/
- https://www.techstars.com/communities/startup-digest

-----


[intelligence.weforum.org/](https://intelligence.weforum.org/) |
[zdnet.com/](https://www.zdnet.com/) | [techrepublic.com/](https://www.techrepublic.com/) | [iotworldtoday.com/](https://www.iotworldtoday.com/) | [cnet.com/](https://www.cnet.com/) | [techcrunch.com/](https://techcrunch.com/) | [techspot.com/](https://www.techspot.com/) | [macrumors.com/](https://www.macrumors.com/) | [https://www.techradar.com/in](techradar.com/in)
| [theverge.com/tech](https://www.theverge.com/tech)
| [9to5mac.com/](https://9to5mac.com/)
| [howtogeek.com/](https://www.howtogeek.com/)
| [digitaltrends.com/](https://www.digitaltrends.com/)
| [gizmodo.com/latest](https://gizmodo.com/latest)
| [informationweek.com/](https://www.informationweek.com/)
| [thenextweb.com/latest/](https://thenextweb.com/latest/)
| [tomshardware.com/](https://www.tomshardware.com/)
| [pcmag.com/](https://in.pcmag.com/)
| [bloomberg.com](https://www.bloomberg.com)

------------------
--------------------

Cloud Office

Essential Services at Government of British Columbia

Government of British Columbia, Office of CIO established the BC Developers’ Exchange (BCDevExchange) to support collaboration between government and citizens using open source tools. It provides a cloud native experience for services such as education, healthcare, social services and economic development to its 4.6 million residents

-------------


Confidence to securely collaborate from anywhere

Agri-food leader Amador used Forcepoint Email Security Cloud to eliminate spam and malware before it reaches the company network and Web Security to protect both office and remote workers from phishing sites, spyware, and malicious code.

---------


Process improvement at Scottish Waters

Scottish Waters leverages Oracle ERP Cloud to run its critical back-office functions leading shorter order lead times, reduced customer complaints, and improved customer satisfaction levels.



-----------

Amazon Chime

Brooks Brothers is the oldest men’s clothier in the United States, with 270 stores in the US and Canada. The company faced challenges regarding speed of deployment for new SAP implementation, and were overly reliant on email. Amazon Chime provides its users with a single application for online meetings, calls, and chat, available across all devices. Brooks Brothers was also able to retire multiple legacy applications. 

-------------


Google G Suite

Morrisons is one of the largest supermarket chains in the United Kingdom, with 120,000 staff across more than 500 sites. Previously, Morrisons would print half a million sheets of paper each week and file them in over 3,000 filing cabinets. Using Google G Suite, employees could now collaborate in Docs and collect information rapidly using Forms, reducing the paperwork required. Instead of VPNs and outdated handheld devices, employees could use G Suite on any available device.

----------------

IBM Connections

Cloudhop, the managed service provider, develops and delivers solutions that help organizations across Africa and the Middle East enhance productivity, reduce costs and uncover valuable insights in their data. Cloudhop wanted to offer social collaboration tools that could scale to support multinational organizations, in order to attract larger clients, and keep them. Using IBM Connections Cloud and IBM Watson Workspace, integrated and security-rich capabilities are delivered, supporting teams of 10 to 10,000.


---------

MERGER & ACQUISITIONS

Shell used a combination of Salesforce platform, Microsoft Azure and Microsoft Office 365 to divest selected downstream businesses in and enabled these businesses to operate on a new IT platform, separate from the Shell global IT landscape. Using these predominant cloud platforms, Shell achieved a successful IT merger and acquisition implementation, allowing the deals to be implemented in a faster, more flexible and cost-effective way than would have been possible using a traditional approach.


--------------------


VR Training

Walmart, the American multinational retailer, is using VR headsets to train employees across the United States, in partnership with VR start-up STRIVR. Employees at a Walmart training academy can experience real-world scenarios using an Oculus headset, with the headset linked to a video screen so that the instructor and other students can also view the experience. 

---------------

Augmented Reality

Although it has been discontinued in the consumer market, Google Glass has been used in the manufacturing industry to assist workers. Employees at ACGO, an American agricultural equipment manufacturer, use Google Glass in case they forget a set of instructions while working on a piece of equipment, or to bring up a required manual, photo or video.

----------------

VR Collaboration

Volkswagen partnered with Innoactive, the German VR training company, to train 10,000 staff across several brands and locations. The Innoactive HB SDK is used and optimized for the HTC Vive Pro and the Volkswagen Group Digital Reality Hub is a VR platform that helps employees collaborate across the Audi, SEAT, Skoda and Volkswagen brands. Multiple users  can collaborate simultaneously while located physically in different offices. There are now more than 30 VR training simulations.


--------------------

DNA Storage

Catalog claims to be the first company to have developed a solution to make DNA data storage commercially viable. Catalog uses enzymes to create proprietary-encoded DNA strands and is currently developing the way to scale up such a system.
The first prototype machine, which is on schedule to be completed by early 2019, will be able to encode a full terabit per day and by 2022, the company expects to be able to increase that to a petabit — a 1000-fold improvement. That should make the process competitive for archival storage, such as medical and legal records as well as storing film databases at movie studios.



----------------------

Swarm Technology at NASA: Building Resilient Systems

Developing space systems is a complex task, driven by standards and safety requirements to ensure reliability of sophisticated hardware and software. Future NASA missions will focus on the development of swarm-based spacecraft systems comprising multiple self-organizing and autonomous spacecraft.


----------------------------

Using Boston Dynamics Spot for Healthcare monitoring

Boston Dynamics, an autonomous robot making company is contributing to the pandemic through autonomous robot-dog named Spot to help protect healthcare workers from Covid-19 at Brigham and Women’s Hospital in Massachusetts. With the help of an iPad and two-way radio on the robot, healthcare workers video conference with patients and remotely control Spot as the robot dog walks through rooms with infected or potentially infected patients


---------------------------
Nuro launches autonomous last-mile delivery services

Nuro allows millions of people to have groceries and other goods delivered by autonomous vehicles, potentially reducing traffic. Its driverless R1 pod is comprised of compartments that can be filled with goods for delivery to homes. Nuro is breaking new ground and building their AV from scratch. This US Department of Transportation (DOT) and the National Highway Traffic Safety Administration (NHTSA) approved a regulatory exemption for R2, Nuro’s second-generation vehicle.


--------------------------
Autonomous Disinfection at Gerald R Ford airport Michigan

Pratt Miller Mobility demonstrated its Large Area Autonomous Disinfecting (LAAD) vehicle at the Gerald R. Ford International Airport. LAAD is the first vehicle of its kind to be deployed in the U.S. Described as a “connected, electric and autonomous disinfecting robot,” LAAD uses a multi-head electrostatic sprayer array to dispense disinfecting materials.


-------------------

NASA to deploy self driving tech on mars

NASA is expected to launch its latest Mars rover, Perseverance, on a first-of-its-kind mission to collect and store geological samples. Perseverance is significantly more autonomous than any of NASA’s previous four rovers and is designed be a “self-driving car on Mars.” Like the ones on Earth, it will navigate using an array of sensors feeding data to machine vision algorithms.

https://www.wired.com/story/how-nasa-built-a-self-driving-car-for-its-next-mars-mission/

--------------------------
Environmental monitoring through biomimics

The Korea Research Institute of Ships and Ocean Engineering designed a 1,400-pound autonomous crab that can reach an underwater depth of 656 feet. It doesn't stir up clouds of silt and debris, such as propeller-driven remotely operated underwater vehicles. The crab also has a doppler radar-based navigation system so that it can feel around in murky conditions. With its several cameras it can zoom in on essential objects and species to see events at the deepest depths.


--------------------------
Autonomous Cars at BMW

The BMW Research, New Technologies & Innovations team in Garching (Germany) is working on neuromorphic computing, cognitive modeling for knowledge representation of automotive scenes and machine learning in the context of automated driving.


-------------------

TRAFFIC OPTIMIZATION

The management and optimization of traffic (road, rail, air, etc.), the operation of vehicle fleets and the management of autonomous vehicles are promising fields where the inherent qualities of Quantum could well express their potential. Several companies are evaluating this around the optimization of filling of airline fleets in near real time and in a global way.


--------------------------
UBER REDEFINES THE RIDESHARE BUSINESS MODEL

Mobile technology enabled Uber to reinvent the taxi model and create a supply of drivers and vehicles and bring significant efficiencies in pricing and driver-rider pairing. Uber is now testing autonomous vehicles so the company can own a fleet of vehicles to service customers.  
Uber’s business model has already evolved into logistics from pure transportation with the addition of food delivery with UberEats. Operations transformation is seeing the developing and testing of autonomous vehicles through a partnership with Toyota and Volvo. Disruptive technologies deployed to manage and operate a fleet will include AI, Deep Learning, Automation, IoT, 5G & Pico Cells, Data Storage Density. 


-------------------

Unmanned Aerial Vehicles

Amazon's drones will be able to pick up a package and track the location of the customer it is being delivered to by pulling data from their smartphone. They expect to deliver up to 400 million packages per year at a price of one dollar per delivery. 

--------------------------

Autonomous Vehicles/Connected Car

5G provides new opportunities to deploy autonomous driving and connected car applications. Use cases include vehicle data transmission and remote autonomous vehicle control on any road, in any conditions, without a human on board - requiring 5G network characteristics including broad coverage, high data throughput, ultra-low latency, and core network slicing. Automated vehicle start-up Pelaton would benefit from increased 5G speeds, as would connected car start-up Mojio. The Tesla’s, Waymo’s and Uber’s of the world would benefit from better communication with sensor-enabled infrastructure/traffic systems.

-------------------

V2X Communications

Connecting cars to each other and their surroundings, often known as V2X communications, is a key step in the safe, large-scale deployment of self-driving cars. Jaguar Land Rover is testing smart, connected cars on UK roads to prepare for self-driving cars. The tests on over 40 miles of public roads are part of a $12.6 million project aimed at creating the UK’s first fully self-driving car connected infrastructure. Vehicles will benefit from a combination of wireless technologies (3/4G, WiFi, fibre optic networks). This technology can provide warnings to drivers that a car too far ahead to see has applied its breaks, for example, to avoid potential accidents. Audi partnered with Huawei and plans to launch 5G-connected cars in early 2020, allowing for V2X vehicle communications for safer, semi-autonomous driving. The car will be able to communicate with traffic lights, cameras and road signs, and direct the user to the specified destination.

--------------------------

Autonomous Mining Trucks

In manufacturing and industry, OEMs are doing a lot to drive uptake – Caterpillar, the construction machinery and equipment company, first deployed autonomous mining trucks in 2013. The Cat Command platform implements autonomous hauling solutions for truck fleets. Komatsu, another heavy equipment provider, makes hauling trucks for surface mining with autonomous capabilities – in 2016, the company unveiled a prototype cab-less mining truck. BHP Billiton, the mining company, deploys a range of autonomous vehicles, including underground trains and surface mine trucks.



-------------------

Autonomous Passenger Cars

Autonomous passenger cars, whereby individuals own/lease private cars is a key potential use case for AV, providing opportunities to reduce accidents caused by human error, and for better traffic congestion management. Usage-based insurance will have more data to analyse with customized insurance products tailored to individual customers. Several companies have made significant investments to assemble the portfolio of technologies needed to realize their autonomous vehicle plans. Intel is acquiring Mobileye for $15 billion, Ford motor company is investing $1 billion in Argo AI, Uber acquired Otto, and General Motors invested $500 million in Lyft for autonomous vehicle development and acquired Cruise Automation.


--------------------------

Autonomous Cargo Truck Platooning

Automated cargo truck platooning (a truck platoon with 3-4 driverless trailer trucks) will reduce number of human drivers needed and tackle challenges related to variable fuel costs and rising insurance costs caused by human error (accidents). In most scenarios, a lead truck is equipped with a 2x4K camera in the front and 2x4K cameras in the back, while trailer trucks are equipped with 4x4K cameras placed on the sides.


-------------------

Autonomous Public Transport

Automated shuttle busses, used for specific public facilities, such as airports and campuses (known routes) would service to reduce number of human drivers needed, reduce salary costs and provide flexibility in adding more buses to address capacity. A remote human operator would likely be required to see the full status of a bus en route as well as performance indicators. Autonomous public busses, with many routes and frequent route changes, could also be used to reduce accidents and insurance costs, and maintenance costs caused by poor driving. Autonomous taxi services - in countries like Japan and Korea, taxi fleets are owned by corporations and scheduling could follow an Uber-like cloud-based architecture.


--------------------------

Autonomous Police Patrol Vehicles

Autonomous police patrol vehicles, used for crime prevention, provide opportunities to enhance the mobile police force by providing another layer of patrolling, meaning human police officers can be better allocated and there is better coverage of the population.


-------------------

Electric Car Charging

RWE Generation is currently one of the leading power generation companies in Europe. RWE is using blockchain to explore how charging electric vehicles could be transformed. The first one of these projects is an autonomous electric charging station, integrating a smart contract that allows users to rent the station, put up a deposit, charge their car, then get their deposit back.


--------------------------

DELIVERY

Rio Tinto plans complete autonomous mining operations. It has deployed 75 Komatsu 930E autonomous trucks across its Pilbara iron ore operations and plans to fully automate its Silvergrass operation. Its AutoHaul autonomous train system is also being deployed across Western Australia. Rio Tinto estimates about 50% of its train operations being completed autonomously and aims to have a fully autonomous train network by late 2018.

-------------------

DELIVERY

Autonomous trucks built and operated by the start-up Embark have been hauling Frigidaire refrigerators 650 miles from a warehouse in El Paso, Texas, to a distribution centre in Palm Springs, California. A human driver rides in the cab to monitor the computer chauffeur for now, but the goal is a fully autonomous truck.

--------------------------

TRANSPORT

Alphabet Waymo is testing Chrysler Pacifica hybrids for deployment across Phoenix, Arizona as self-driving taxis. The company has more than 4 million miles of testing on public roads and considers the technology road ready.

-------------------

MILITARY TRANSPORT

Lockheed Martin is developing a multi-platform kit that utilizes sensors and control systems that can enable autonomous functions on military tactical vehicles. However, like the current self-driving robot cars we see on the road, full autonomy isn’t yet a reality. Therefore, military is working on a solution: a leader-follower-type system (lead vehicle followed by up to seven other vehicles)

--------------------------

SHIPPING

Kongsberg is working on YARA Birkeland - autonomous ship delivery project. It will be a fully electric and autonomous container ship, with zero emissions. KONGSBERG is responsible for all key enabling technologies including the sensors and integration required for remote and autonomous operations.



-------------------

DELIVERY

Pizza Hut and Toyota forged a global partnership to explore the autonomous pizza delivery vehicle - kiosk cruising the streets in the next few years.

--------------------------

Autonomous Vehicles for retail

Autonomous vehicles will automate the movement of goods between physical locations, including to the end consumer, throughout the supply chain. Autonomous vehicles will become mobile, on-demand transporters that deliver customers the products they need nearly instantaneously. Autonomous vehicles will automate the transport of goods across the supply chain network.

----------------
-------------------

Product Engineering

BMW has adopted VR to aid design and development of new vehicles. Through the introduction of this technology, engineers and designers will be able to immerse themselves in a virtual world that feels as close to a true driving situation as is currently possible. Staff will be using the HTC Vive headset to visualize new interiors and other physical features. Once they've been implemented in VR, designers will be able to simulate a city and test whether the driver has enough visibility behind the wheel. Engineers can also evaluate whether the dashboard and controls can be grasped properly from a seated position.

-----------------

CONNECTED CAR

AT&T and Vodafone Business are working together to accelerate Internet of Things (IoT) connectivity and innovation in the automotive industry. The companies will bring together their industry-leading expertise to develop superior and consistent connected car solutions and experiences for customers across their combined footprints in North America, Europe and Africa.

-------------------

5G & AUTONOMOUS VEHICLES

The CTO of Waymo, which started life as the Google Self-Driving Car Project in 2009, believes that 5G is a crucial “enabler”, when it comes to developing the company’s autonomous car fleets. O2 has also now announced a project to trial driverless cars in London using its 5G network.  The UK's second-biggest phone network has partnered with the Smart Mobility Living Lab - a research organisation comprised of experts from the Transport Research Laboratory (TRL), DG Cities, Cisco, and Loughborough University - to develop what it claims to be the ‘most advanced driverless testbed in the world’. The aim is to develop a road management system with the focus on a 10 percent reduction in the time that motorists spend in traffic. Other figures include a positive benefit to the economy of £880m a year from improved productivity as well as the reduction of CO2 emissions by 370,000 metric tonnes a year.

-------------

TRAFFIC FLOW OPTIMIZATION IN VOLKSWAGEN

The Volkswagen Group is the world’s first automaker to publicly use quantum computers, further expanding its digital capabilities. The company is cooperating with quantum computing company D-Wave Systems on a research project for traffic flow optimization. The project involves publicly available data from Beijing taxis that were used to plot optimal routes.

---------------

ROUTE OPTIMIZATION AT FORD
Ford is currently working with NASA (National Aeronautics and Space Administration) in the U.S. to examine how quantum computing can improve its business. One idea being explored is routing diesel delivery vehicles in such a way as to reduce harmful pollutants by ensuring their drive cycles are operating within an optimal range. Another idea is improving productivity in its manufacturing plants.

-------------

FLEET OPTIMIZATION AT DENSO

Denso, the global automotive components manufacturer headquartered in Japan, is working on the optimization of the operations of a fleet of electrical delivery vehicles. Partnering with Toyota, the research uses vehicle location data from 130,000 commercial vehicles and cloud-based quantum systems to analyze the information and improve efficiency. 


------------

Autonomous Cars at BMW

The BMW Research, New Technologies & Innovations team in Garching (Germany) is working on neuromorphic computing, cognitive modeling for knowledge representation of automotive scenes and machine learning in the context of automated driving. 


----------

HIGH-PERFORMANCE BATTERIES AT VOLKSWAGEN

Volkswagen has two in-house data labs for quantum computing in San Francisco and Munich. Volkswagen has also partnered with Google to explore structures for new materials, especially high-performance batteries for electric vehicles.

------------

BATTERY CELL RESEARCH AT DAIMLER

Daimler, in partnership with IBM, is investigating quantum chemistry calculations for new battery technology. Daimler is also working with Google to create new materials for quantum technology and the development of new battery cells. 

------------

REINFORCEMENT LEARNING FOR VW AUTONOMOUS CARS

Volkswagen is experimenting with QML to prove the value of reinforcement learning in Self Driving Cars, by taking feedback directly from the environment. However, reinforcement learning is a type of machine learning whose hunger for data is even greater than supervised learning and it is really difficult to get enough data for reinforcement learning algorithms.

-------------

Polyglot Digital transformation at Volkswagen

Volkswagen use OpenStack infrastructure and the Cloud Foundry application development environment to achieve a scalable platform deployable across private and public clouds supporting polyglot cloud native applications to ease transformation of existing applications within a span of few weeks. 


-------------

Self-Driving at BMW

BMW’S autonomous driving campus in Unterschleissheim makes it the first company in the automotive industry to apply the agile working model for autonomous driving and driver assistance (from the research phase all the way through to series production development). The open campus structure offers 'excellent conditions and exceptional support for scrum teams, and therefore lends itself perfectly to the agile software development process.

--------------

IBM Quality assistant for connected manufacturing

IBM Engineering Requirements Quality Assistant uses Watson AI to improve the quality of project requirements through augmented software analysis
A version of it called Vehicles/Auto Requirements Quality Assistant provides requirements analysis over thousands of engineering requirements for IoT enabled connected applications such as automotive.

-------------

Cognitive Assistance at Honda

Honda introduced its Honda Connect: a mobile car app and Driving Coaching System: a cognitive assistant. Through the combination of IBM Watson Data Platform and IoT technology, Honda has created the Driver Coaching System prototype, built on the Bluemix platform that can can adapt and adjust coaching to fit any driving behavior. The system is able to protect drivers through early warnings of dangerous situations.


--------------

STRATEGIC DISASTER RECOVERY

Hyundai Heavy Industries implemented Resilient Enterprise Blueprint (REB) with IBM Business Resiliency Services. The proactive  response framework helps streamline core system shutdowns and other serious impacts on IT services.

---------------

DISASTER RECOVERY ON AZURE

Daimler uses Azure Site Recovery to engineer disaster recovery between Azure regional datacenters. The company runs two SAP HANA instances, primary and standby for each production system, and uses Site Recovery and SUSE Linux clustering software to orchestrate failover between different Azure regions.


-------------

PROTECTING BUSINESS & IP

McLaren partnered with Secureworks to deploy Advanced Endpoint Threat Prevention (AETP) and other security solutions to protect the group in real-time against threats to its IP, data and business processes.

-------------

CLOUD-BASED IGA

Toyota Connected upgraded its identity governance and administration (IGA) solution for its cloud-based digital Connected Mobility Intelligence Platform. Saviynt’s Identity Governance and Administration (IGA) solution enabled a CI/CD environnent for DevOps by quickly detecting and remediating excessive access and other anomalous conditions.


-------------
MULTIPLE IDENTITIES FROM ANYWHERE

Toyota Connected updated its identity governance and administration (IGA) solution for its cloud-based digital Connected Mobility Intelligence Platform with Saviynt. The company manages an agile DevOps environment across multiple identities operating from multiple access points. 


-----------

SKELETON TECHNOLOGIES ENERGY-STORAGE SYSTEMS 

Skeleton Technologies is a global leader of graphene-based ultracapacitors and energy-storage systems with 4 x the power density of the nearest competitor. Skeleton deliver high power and energy solutions across multiple industries. Ultracapacitors are ideal solutions when energy needs to be stored and discharged rapidly and efficiently with low heat and weight requirements, making them a great complement to traditional battery and carbon-based fuel systems. Skeleton partnered with Adgero SAS to develop a Kinetic Energy Recovery System (KERS). This partnership allows Skeleton to bring its proven Graphene-based ultracapacitors to the automotive, industrial, aerospace and transportation industry. Incumbents have achieved only 25% of the energy density of graphene-based solutions. Operations Transformations: KERS allows the transfer of energy during braking to charge the ultracapacitors, which energy can power connected electric motors. By using energy captured during breaking to accelerate the vehicles, fuel consumption can be significantly reduced.  
Impact: Smaller batteries required to power vehicles. 25% reduction in consumption in diesel applications. 


------------

Automotive engine assembly

Renault Trucks has been testing the use of MR to improve quality control processes during its engine assembly. It has selected 20 employees to collaborate with its technology partner Immersion to design the application prototype. The operators would wear Microsoft’s HoloLens which stores the digitalized version of all the engine parts. This would help operators to replace the paper-based instruction with on-screen instruction to help with real-time decision-making.


-------------

Printing Titanium Wheels

HRE Wheels and GE Additive put their heads together to improve on the previous design of the HRE3D wheel. The second generation wheel is 3D printed in titanium in two different sizes for the rear and front of the car. The wheel parts were made using an EBM 3D printer by Arcam AB and the Concept Laser X LINE 2000R. On average HRE is saving £4 per wheel compared to the previous model. 

-----------

Serial 3DP Production at BMW

The BMW Group has launched what promises to be a landmark project for the introduction of serial additive manufacturing into the automotive industry. The overall goal is to be able to produce at least 50,000 components per year in mass production using additive manufacturing, and over 10,000 individual and spare parts. The company plans to cut manual activities along the process chain from 35% down to just 5%, with a cost reduction for metal parts at 50%.

--------------

Car Wiring

For vehicle wiring, Bosch invests in CelLink’s innovative technology that can provide up to 70% weight reduction and up to 90% volume reduction by replacing round wire bundles with flat flexible circuits. These savings have – according to Bosch – the potential to power widespread adoption across next-generation electrical systems.



---------------

Aerodynamics

Companies like Airbus are researching smart materials that react to temperature to cool jet engines and wings that morph according to aerodynamic conditions to decrease air resistance. Briggs Automotive Company is developing a morphable wing for its supercar that adjusts to external weather conditions and automatically adjusts in order to provide maximum downforce to the car.


----------

K&N Filters Digital Factory 

Problem Statement: 
•Lacked data and visibility of accurate plant metrics (e.g. OEE, machine downtime, output)
•Tribal knowledge and “this is how we’ve done it for years” was pervasive across the plant
•Wanted fact-based insights to better manage the plant and improve performance.


----------


GLASS MANUFACTURER M&A STRATEGY REVIEW AND TARGET SCREENING

A leading global glass manufacturer was determined to become an Automotive Operator of Essential Service (OES), defining which areas to grow and how to generate €300m-€800m as additional top line for the next 3 to 5 years.

------------------

TATA MOTORS ANALYTICS-LED BUSINESS TRANSFORMATION

The objective was to leverage analytics to develop industry differentiation and a competitive advantage.
In terms of results, organizational analytical capability (for sales and service teams) has been increased from 1.5 to 3.0 on the Delta Maturity Scale. Client is first in the Indian automotive space to establish Analytical COE model which is now benchmark across industries. 39MN USD impact delivered.


-----------------

REAL-TIME DECISION-MAKING

American Axle: The Plex Manufacturing Cloud helps American Axle Manufacturing drive innovation on its shop floors, improving efficiencies with cloud ERP, streamlining processes and providing a single source of visibility. Through its partnership with Baker Tilly, AAM achieved a seamless go-live process, avoiding downtime for its high-output metal formed products division.

-------------

Kyocera's haptivity technology 

Kyocera has entered into a license agreement with Robert Bosch Car Multimedia GmbH, which authorizes Bosch to use Kyocera's haptic feedback technology. Kyocera has been producing haptic technologies through its own research and development since 2008. Over the years, Kyocera has developed ergonomics and virtual reality technologies and through the use of these technologies and proprietary haptic feedback technology, Kyocera's haptivity technology contains a virtual reality innovation that enables real touch sensations at every man-machine interface. 

-------------

The automotive sector moves ahead with MR labs

BMW and MINI’s mixed reality lab pairs custom hardware with virtual reality to bring a whole new level of real-time experience to the production cycle. Using a mixed reality design experience with Unreal Engine, VR, and specialized cutaway elements of physical car hardware, the team is able to experiment and make key decisions earlier in the process, leaving more time for iteration and refinement before locking in its designs.


------------

Automotive sector uses HoloLens for training sessions

Mercedes-Benz uses Microsoft HoloLens to conduct faster, more engaging training sessions. The training center is now equipped with more than 100 HoloLens units. By introducing mixed reality participants can consolidate complex, technical concepts effectively simplifying their lessons in visual and relatable ways.



------------

VR Collaboration

Volkswagen partnered with Innoactive, the German VR training company, to train 10,000 staff across several brands and locations. The Innoactive HB SDK is used and optimized for the HTC Vive Pro and the Volkswagen Group Digital Reality Hub is a VR platform that helps employees collaborate across the Audi, SEAT, Skoda and Volkswagen brands. Multiple users  can collaborate simultaneously while located physically in different offices. There are now more than 30 VR training simulations.

------------

Data security 

Williams Martini Racing used Symantec’ Endpoint Protection Solution to gain protection for high- value data and intellectual property and multi tiered protection to detect latest threats. 

--------------

SPATIAL COMPUTING

Concepts such as Bosch's "third living space" are revolutionizing the automotive space by developing the car interior as an extension of the personal space. Short from being immersive, a nevertheless ubiquitous placement of displays (of all sizes and form factors, bendable, as well as heads-up displays for AR) can create a spatial canvas for content. 


-----------

Data security 

Williams Martini Racing used Symantec’ Endpoint Protection Solution to gain protection for high- value data and intellectual property and multi tiered protection to detect latest threats. 

-----------

Fraud Detection

Uber uses real-time systems to combat fraud across it’s software platform. Fraud prevention is one of the fastest growing areas of research and development at Uber – rider fraud, driver fraud, or both. Success relies on several groups; (i) data scientists and machine learning experts who build complex models to detect patterns with live and real-time data, (ii) data engineers who build online analytics platforms and pipelines to process rapidly growing data, (iii) solutions specialists who find the best methodology for diffusing real-life threats, (vi) UX experts who design and build applications and tolls to standardize the anti-fraud process in global operations.



------------

Sales Forecasting

Peugeot used data from millions of real world car trips to calculate average journey time undertaken by customers in order to optimize sales forecasting. Pitney Bowes used MapInfo Pro to provide insights on best performing dealers, increase response on fleet orders, improve accuracy of drive time estimates, gain a more realistic revenue potential view, learn more about traffic patterns and road conditions. The automotive company could therefore determine how accessible by car each of its showrooms and service centres are within standard opening hours, so that customers could be advised accordingly.



-------------

Advertising Revenue Optimization

Autotrader.com is one of the largest automotive marketplaces and consumer information websites, which aggregates millions of cars from thousands of dealers and private sellers. Autotrader worked with Informatica to drive increased advertising revenue and cost avoidance using MDM. The business was also able to improve data governance, having been previously reliant on a homegrown, outmoded MDM system (for example, if a dealer bought ads for all 100 of its dealers, the system forced the manual creation of 100 dealers). 



---------------

Data Processing

Ford, the American multinational automaker, used Alteryx to reduce data processing time by 80%. The Ford IT Analytics Architecture Team started to work on an Enterprise Technology Refresh Program in February 2016, with the objective of simplifying the technology footprint and renewal process across the company in support of IT strategic initiatives. Alteryx was used to blend dozens of data sources in order to prepare the data for analysis, to visualize the current technology state and gain insights, and to perform analysis for resource prioritization and activities planning. Team members were trained and able to use the software in a week, time was saved, errors were reduced and business customer satisfaction increased, according to Ford.


------------
Connected Car

Bluetooth 5 will allow for more effective communication between a user’s smartphone and a car’s audio system, from companies such as Pioneer or Kenwood, or GPS navigation devices from Garmin or TomTom.


--------------

Autonomous Vehicles/Connected Car

5G provides new opportunities to deploy autonomous driving and connected car applications. Use cases include vehicle data transmission and remote autonomous vehicle control on any road, in any conditions, without a human on board - requiring 5G network characteristics including broad coverage, high data throughput, ultra-low latency, and core network slicing. Automated vehicle start-up Pelaton would benefit from increased 5G speeds, as would connected car start-up Mojio. The Tesla’s, Waymo’s and Uber’s of the world would benefit from better communication with sensor-enabled infrastructure/traffic systems.


-------------------

Facial Recognition/Voice Biometrics

Fiat Chrysler Automobiles released a new concept autonomous vehicle, Portal, at CES 2017, designed ‘for millennials by millennials’. A key focus of the design is the digital user experience; working alongside the Panasonic Advanced Engineering team, the connected car uses facial recognition and voice biometric software to identify who is in the vehicle and to customize the environment for the occupants.



----------------

Connected Car Diagnostics

AT&T partnered with Harman, the connected car and products company, with the aim of to ‘turning any car into a connected car’. Users can plug the Harman Spark into the car on-board diagnostics and download an app for the car to become connected. Harman collaborated with Tantalum, an automotive software company, to enable Harman Spark to have features such as emergency crash assistance, virtual mechanic, WatchIt (which informs users if their car is bumped or moved when they are not around), roadside assistant manager, and driving scores and tips. 



---------------

Cybersecurity

The connected car is a target for hackers, who could potentially take control of a vehicle while it is being driven, or lock down a vehicle and demand a ransom. C2a Security, the security and analytics company, announced AutoArmor, a product focussed on protecting onboard auto testing functionalities from cyberattacks. AutoArmor works by discovering all of the engine control units (ECUs) in the vehicle, aggregating diagnostics and anomalies from these ECU, and then performing mitigation according to the OEM’s policies. It can also perform software updates on remote ECUs and special remote invocations, to further protect against cyberattacks. 



------------

V2X Communications

Connecting cars to each other and their surroundings, often known as V2X communications, is a key step in the safe, large-scale deployment of self-driving cars. Jaguar Land Rover is testing smart, connected cars on UK roads to prepare for self-driving cars. The tests on over 40 miles of public roads are part of a $12.6 million project aimed at creating the UK’s first fully self-driving car connected infrastructure. Vehicles will benefit from a combination of wireless technologies (3/4G, WiFi, fibre optic networks). This technology can provide warnings to drivers that a car too far ahead to see has applied its breaks, for example, to avoid potential accidents. Audi partnered with Huawei and plans to launch 5G-connected cars in early 2020, allowing for V2X vehicle communications for safer, semi-autonomous driving. The car will be able to communicate with traffic lights, cameras and road signs, and direct the user to the specified destination.



-----------------

Process Automation

Automotive manufacturers such as Benteler Automobiltechnik GmbH are using smart manufacturing to evolve their factories. The German company operates 70 plants in 29 countries, so in order to improve the agility and efficiency of factory floors, created a fully modular next-generation production environment of automated, self-running plants. In partnership with Cisco and Cisco’s Application Centric Infrastructure (ACI) platform, this allowed the company to realize faster changeovers and increased operational efficiencies. 

---------------

Predictive Analysis

The Hirotec Group is one of the largest private production companies in the global automotive market. The Japanese company wanted to leverage Industry 4.0 technologies to tackle unplanned downtime issues. Combining an IoT platform from PTC, a partner of Hewlett Packard Enterprise (HPE), with HPE Edgeline systems, operations technology capabilities were boosted using predictive analytics. Three pilots of the IoT platform allowed Hirotec to gain real-time visibility of business operations, addressing issues that impacted efficiency and throughput, utilizing machine learning to predict and prevent failures in critical systems.


---------------

Smart Parking

As urbanisation continues apace and the number of cars on the roads rises steadily, finding a place to park can be a major headache for business and leisure travellers alike. If drivers knew in advance which spaces are vacant, they would spend less time and waste less fuel searching for a place to park.
Using new low power wide area (LPWA) mobile technologies, it will become cost-effective for telecoms operators to relay real-time information on parking availability to drivers’ smartphones or dashboard computers. Deutsche Telekom has tested a prototype LPWA – based parking system on its campus in Bonn in Germany, and trialled it on its commercial network in the Netherlands.

------------

Predictive Maintenance

Using Microsoft’s Azure IoT Suite to collect data, and Cortana Intelligence Suite to derive insights, Rolls-Royce goes beyond predictive maintenance and into metrics that it can pass onto operations teams at airlines as a value added service. These insights are aimed at helping airlines to be more efficient when it comes to maintenance, aircraft choice and route selection.



-------------

Connected Car

Electronic control units (ECUs) are used in the automotive industry to monitor and control engine performance and exhaust emissions, using sensors. ECUs have also improved vehicle safety through systems such as traction control, anti-lock breaks, airbag deployment. This, alongside the now seamless integration of mobile devices with the vehicle, means that there is an increasing need to secure these systems. Electric car manufacturer Tesla could diagnose vehicle faults and even send software upgrades to those vehicles when it was safe to do so. When engineers discovered that a fault with the vehicle charger mechanism could cause a vehicle fire, Tesla was able to upgrade almost 30,000 vehicles over the air to correct the error, rather than prompt an expensive recall.


---------------

Design for General Motors

General Motors is exploring the use of software based on generative design and AI-driven algorithms to optimize shape and weight of automotive structural components. As a result, the components often showcase organic-looking, intricate geometries, which can be produced using additive manufacturing technologies.



-------------

Manufacturing

Eindhoven University of Technology has a team dedicated to the development of compact, efficient city cars made from sustainable materials. To build “Noah,” TU/ecomotive they worked with 3D printing service bureau Oceanz to 3D print both interior and exterior components. They used Oceanz’s EcoPowder material, a durable 3D printing material that works in a circular economy.


--------------

Data security

Groupe Renault announced a prototype for a digital car maintenance book built in collaboration with Microsoft and VISEO. Blockchain technology ensures that data in the car passport is stored in a secure and transparent manner. 


--------------
Jet Engine Optimization

GE’s Predix platform was developed to connect industrial equipment, analyse data and deliver real-time insights. It is an aggregation of microservices that are useful in building, deploying and managing industrial internet applications. GE has used Predix and digital twins to analyse jet engine data – using statistical models and physics-based simulations to understand what is happening, when and how. Simulations can be run on-site or in the cloud at scale. GE leverages software from ANSYS, a global leader in engineering simulation software, when rolling out digital twins. 


-----------

Wind Turbine Optimization

GE Power use a digital twin to provide accurate operational pictures of wind farm assets in real time. Digital twins can be used to identify underutilized devices and drive optimal usage. GE Power were able to get 5% more output from a wind farm without making wholesale changes - the turbines were optimized for changing wind conditions and the team orchestrated the interaction of individual twins on-site.

------------------

Iot Enabled Architecture at Arup

Arup adopted MASA methodology to align it’s IT systems, design teams, contractors and procurement team to deliver complex infrastructure projects – airports, major sporting venues and world-class smart cities projects. This includes managing interconnected systems on a multibillion dollar smart city project in Qatar and planning of a new baggage handling system for a major airport operator.


-------------------

Predictive Maintenance at Schindler

Schindler, one of the world’s largest elevator and escalator suppliers, partnered with GE to implement Predix Platform to better connect equipment, leverage advanced analytics and machine learning capabilities, and keep its equipment running smoothly and securely. AI enables maintenance workers to target repairs in a predictive ad-hoc way, rather than time consuming and redundant scheduled maintenance.


----------------

Equipment Repair
 
German elevator manufacturer ThyssenKrupp has adopted Microsoft’s HoloLens to help its employees fix faulty elevators with higher accuracy and speed. With this MR device ThyssenKrupp engineers can scan the machinery parts and get all info about them that’s needed to service them. They also get to see how to fix any faulty parts more efficiently.


---------------
Surgical Implants

German medical device manufacturer EIT recently demonstrated the vast potential of medical additive manufacturing by supplying the first anatomically adapted, 3D-printed titanium fusion implant to a patient with a degenerative cervical spine condition. The technology behind this pioneering effort was Direct Metal Printing (DMP) , which is capable of building metal objects layer by layer in a variety of metals, in this case biocompatible titanium.

---------------

Sourcing and Procurement 

Cummins Engine, a provider of diesel- and natural gas-powered engines, needed a common sourcing process that would support their direct purchasing needs around the world. Employees were using multiple systems to manage huge amounts of data. The overall solution involved Cloud strategic procurement to E-Business Suite R12, rolled out in phases, including PaaS extensions integrated to Cloud sourcing. More specifically, PaaS extensions focused on:

high volume uploading
price breaks
line level attributes 


-----------------

Digital documenting

Balfour Beatty is a multinational infrastructure group based in the United Kingdom. It’s US subsidiary wanted to enable digital documentation across all of it’s operations in order to help manage an entire lifecycle of complex projects and create better workflow. An enterprise hybrid solution from Egntye (including Egnyte Connect, Egnyte Hybrid via Storage Sync, integrated business apps e.g. Bluebeam, GoodReader, PlanGrid) was used to achieve these goals. The digital documentation process was rolled out across the entire organization to thousands of users and external contributors with multiple methods of content protection (e.g. flexible administration and permissions hierarchy, insight into content residency). The solution has been deployed to 2,500 US vertical construction employees, allowing for more efficient communication and collaboration.

---------------

IMMERSIVE WORKSPACES - Augmented Reality

Although it has been discontinued in the consumer market, Google Glass has been used in the manufacturing industry to assist workers. Employees at ACGO, an American agricultural equipment manufacturer, use Google Glass in case they forget a set of instructions while working on a piece of equipment, or to bring up a required manual, photo or video.


-----------

Connected Industrial Equipment

Caterpillar, the American construction machinery and equipment company, uses telematics solutions to enable its customers to get fast and accurate information about the location, utilization and condition of their equipment through remote monitoring, helping them improve efficiency and costs across their operations. 



-------------
--------------

R&D FOR DRUG DISCOVERY

IBM has partnered with 15 universities in Africa, and will use IBM Q to research drug discovery based on Africa’s genetic diversity that could lead to new treatments for diseases like HIV or TB.


-----------------

NEW MATERIALS DISCOVERY

One of the most promising uses of quantum computing is the simulation and discovery of the properties of new materials and active products (for chemistry and pharmacy), through the simulation of atomic interactions. Projects based around this notion are in progress at Dow Chemicals and at Airbus.


-----------------

DRUG DISCOVERY AT QU&CO

Qu&Co is a Dutch-based quantum consulting firm which offers quantum-computational chemistry services to accurately simulate electron-dynamics on a large scale, for the discovery of new drugs and new chemicals and materials.


-------------


HIGH-PERFORMANCE BATTERIES AT VOLKSWAGEN

Volkswagen has two in-house data labs for quantum computing in San Francisco and Munich. Volkswagen has also partnered with Google to explore structures for new materials, especially high-performance batteries for electric vehicles.


--------------

BATTERY CELL RESEARCH AT DAIMLER

Daimler, in partnership with IBM, is investigating quantum chemistry calculations for new battery technology. Daimler is also working with Google to create new materials for quantum technology and the development of new battery cells. 


--------------


SUSTAINABLE FERTILIZERS

Current artificial fertilizers are based on ammonia, made via the Haber process. They use high temperatures and pressures and are energy-intensive. Quantum computing could help scientists understand the Haber process at the molecular level, and start-up Rigetti points to its value in making fertilizers.


-----------------------

Pfizer deploys Mabu care robot

Pfizer launched a one-year pilot program with robotics company Catalia Health with its care robot Mabu, that coaches' patients on health and prescription drugs. Mabu uses voice interactions powered by conversational AI to assess a user’s mood, record data, manage symptoms, and provide helpful information. The robot then supplies information back to medical professionals — like caregivers or clinicians — such as the frequency of medication usage or questions the robot was unable to answer. Mabu is also able to supply personalized responses and deploy affective computing to predict a user’s emotional state

--------------


Creating Social twins using computer vision

Researchers from the NYU Tandon School of Engineering are working on a new approach called “behavioral teleporting,” which could lead to a more in-depth understanding of social behavior, interactions between invasive and native species, as well as better human/robot interfaces. The researchers relied on an automated tracking system to score the locomotory patterns of a live fish, which were used to control the robotic replica in another other tank.

------------

R&D AT BIOGEN

As a leading biotech company, Biogen is seeking to advance the development of new drugs for neurological and neurodegenerative diseases. 

-----------
R&D AT AMGEN

Amgen Inc., a biopharmaceutical company, would be naturally interested in using quantum computers for molecular simulations. IBM has recently shown the potential of quantum-based computational chemistry with the record-breaking simulation of the BeH2 molecule (beryllium hydride).

---------
REINFORCEMENT LEARNING TO ENGINEER NOVEL THERAPEUTICS

The biotech firm ProteinQure creates computational R&D tools to perform drug design in silico. They leverage quantum computing, molecular simulations and reinforcement learning to engineer novel therapeutics.


---------------

DRUG DISCOVERY 

A quantum computer would be able to map out trillions of molecular combinations and quickly identify the ones that would most likely work, significantly cutting down the cost and the time of drug development.


----------

GENOME SEQUENCING

Quantum computing could be used to accelerate the sequencing of DNA genes, disrupting the healthcare industry.

---------

CRISPR to treat genetic conditions 

Carnegie Mellon University and Yale University researchers use Gene-editing techniques such as CRISPR/Cas9 to treat genetic conditions during fetal development in mice. They observed correction of 6% of the mutations in the gene causing beta thalassemia

----------------


CRISPR Therapeutics studies gene editing drug in humans
 
Cambridge biotech CRISPR Therapeutics and Boston-based Vertex Pharmaceuticals have launched the first company-sponsored study of a gene editing drug in humans. The treatment targets beta thalassemia, a rare blood disorder that reduces the amount of oxygen-carrying red blood cells in a person's body, causing fatigue, life-threatening anemia and other serious complications.


-------------

Bioprinting 

Aspect Biosystems (Vancouver, B.C.) has a 3D bioprinting platform that can print tissue for life sciences. Johnson & Johnson has worked with Aspect on the development of knee replacement tissue.


---------------


Medical Research

Graph analytics can also be used when conducting research in Life Sciences, or bioinformatics, including medical research, disease pathologies, epidemiology, genome research, for example.


----------
----------

Mining oceans through Patania

Patania II built by the Belgian company Global Sea Mineral Resources(GSMR)
will descend four kilometers into the ocean in the Clarion-Clipperton Zone and try to suck up metal-rich nodules through four vacuums as it traverses a 400-meter-long strip of sea floor. GSMR plans to launch a third test vehicle in 2023 that will be able to bring the nodules up to awaiting surface ships.

------------

Remote Surveillance

Guardian S Snake Robot is a compact, camera-equipped snake-like device that slithers atop its magnetized tracks, can manoeuvre between constrained spaces and provide surveillance for inhabitable terrain. Packed with sensors, the snake bot feeds still images, video and real-time environmental data to Microsoft Azure based remote terminal.

----------

Security

General Dynamics Bluefin underwater security robot: The Knifefish; a medium-class mine countermeasure UUV can detect, classify and identify potential mines, at a variety of depths, each of which would pose a unique threat to naval vessels operating in a mission area.

---------

Construction work aided with exoskeletons

Hilti North America unveiled EXO-O1 wearable exoskeleton to help commercial contractors, tradesman and management tackle health and safety as well as labor shortage challenges. To create the EXO-O1, Hilti Group partnered with Ottobock SE & Co. KGaA


---------------

Smart Contracts & Assets

Hyperledger was created in December 2015 by the Linux Foundation. The open source collaboration is focused on ledgers designed to support global business transactions, including major technological, financial, and supply chain companies. Hyperledger has launched industry working groups, such as healthcare, social impact, and supply chain. In 2019, it also launched a Smart Contracts working group.


----------

CANCER TREATMENT 

Quantum computing could be used to the optimization of treatment in radiotherapy and better and faster detection of brain tumors, which could be done in seconds instead of hours or weeks. This could mean reduced exposure to X-rays by simulating and optimizing the movement of waves in the human body.



-----------

Improving patient care

VRHealth has partnered with Oculus to offer non-invasive VR solutions that help in cognitive testing, pain management, and psychological assessment. Its VR content takes patients to an environment where they can experience treatment in a more positive light, and medical treatment eventually becomes a fun activity for them. This can especially help in pain management for birthing mothers, and alleviating anxiety during chemotherapy for cancer patients. The patient data is updated on a portal real-time to help personalize their treatment plan.

---------------

Built-in ECG's to measure Heart Rate

Smart clothing is becoming more common in healthcare – most notably compression shirts with built-in ECGs (electrocardiogram) to measure heart rate. Additional built-in sensors can monitor movement, position, respiratory rate and body temperature – companies such as Hexoskin and OMSignal are key players in this space. Owlet’s Smart Socks 2 are designed for babies and track heart rate and blood oxygen saturation, helping parents to learn more about their baby’s health.

-------------

Surgery training using Haptx gloves

Haptx has designed a pair of gloves that can be used in VR-based trainings by surgeons to perform surgeries. The device enables the surgeons to use their complete hands and fingers to simulate the actual surgery and enhance the overall learning quality for aspiring surgeons. Further, these gloves are compatible with any laptop, VR headset, or haptic device.

-----------------
4D Printing in Healthcare

Researchers at the University of Michigan have developed a 3D printed splint that can hold open airways of new-born children for two to three years – it is then absorbed into the body. This device was successfully implanted in four babies. Researchers are also studying how to use 4D biomaterials to help adults correct skeletal applications such as facial reconstruction or rebuilding ears.


------------


SMARTERP

Kern Health: SmartERP developed a complex data manipulation in the Microsoft SQL server code for Kern Health, allowing the client to benefit from faster report creation and performance, reduced software development costs and enhancing the end-user experience.

-------------

HoloLens for people with visual impairment 

Researchers at California Institute of Technology created a new HoloLens app that helps people with visual impairments navigate through buildings and gain a better sense of their surroundings. Researchers did this by taking advantage of the device’s real-time room and object mapping capability, as well as speakers that can make audio seem to be coming from different points in three-dimensional space.

------------
The Body VR revolutionizes healthcare through virtual reality

The Body VR is an educational virtual reality experience. It creates interactive 3D builds of traditionally 2D medical imaging, like CT scans and MRIs, to provide a more intuitive view of medical conditions. Similarly, Oxford researchers have created VR models of genetic data to better visualize what happens within living cells.

-----------

Fitness and Lifestyle Tracking

Companies are more frequently encouraging employees to use activity trackers, such as Fitbit, to drive adoption of healthier and more productive lifestyles, as well as bringing down organizational insurance costs. Personal healthcare offerings include predictive fitness tracking, personalized medical and dietary recommendations, remote health diagnostics, life-coach, relationship and mental health services, from vendors including Spire, Adidas MiCoach, Tictrac and Strava.


--------------

Remote Patient Monitoring

Healthcare providers can use 5G solutions to enhance the quality of patient care/medical services. Fast data flows and guaranteed service levels will enable doctors to remotely monitor patient status and conduct real-time diagnostic evaluations. Surgeons could remotely operate robots and equipment that depend on ultra-fast, low-latency 5G networks in surgical procedures. Ultra-low latencies also enable AR experiences in an operating room setting. Telemedicine services such as InTouch Health could benefit from 5G speeds, while health monitoring systems such as VitalConnect, EarlySense and Proteus could rely on 5G to transmit large amounts of patient vitals data in real time.



---------------
Ingestible Sensors

Ingestible sensors are another example of IoT in healthcare. Proteus Digital Health, the digital medicine company, trialled an antipsychotic and a hypertension pill to monitor user adherence and whether or not they were taking the pill correctly. The pill dissolves in the stomach and produces a small signal which is picked up by a sensor worn on the body, which relays the data to a smartphone app.


-------------

Enabling Extreme Wildlife Tracking

To protect threatened species, conservationists need to fully understand their behaviour and which habitats are key to their survival. To that end, Vodafone is working with the Sea Mammal Research Unit (SMRU) in Scotland to track the movements and health of harbour seals, which have suffered a precipitous drop in numbers in the past decade. Vodafone and the SMRU will explore the use new low power wide area (LPWA) connectivity to build up a comprehensive picture of the seals’ behaviour, help understand why the population is in decline and then take remedial action. Monitors attached to the seals can use the cellular networks to feed location, activity and environmental data into SMRU scientists’ computer systems.


-----------

Glucose Monitoring

Companies in the pharmaceutical industry are developing IoT solutions to assist people with diabetes. In 2016, Roche, the healthcare company, acquired distribution rights to an implantable long-term continuous glucose monitoring (CGM) device which uses a 90 day sensor below the patient’s skin. The device operates by communicating with a smart transmitter which then sends blood glucose levels to an app belonging to the user. 


-----------
Activity Tracking

The IoT can be used to track the activities of patients during cancer treatment. The Memorial Sloan Kettering Cancer Centre (MSK) and cloud research firm Medidata tested activity trackers which gather lifestyle data on patients being treated for multiple myeloma. Patients wear an activity tracker before and during the course of treatments, and the tracker will assist in logging activity levels and fatigue. Appetite can also be logged directly, and data is saved to Medidata’s Patient Cloud ePRO app on the users smartphone.

----------

Connected Contact Lenses

Alcon, the Swiss medical company specializing in eye care products, licensed Google’s smart lens technology, involving non-invasive sensors embedded within contact lenses. These lenses could one day be used to measure the glucose levels of diabetes patients via their tears, or be developed for those with presbyopia, helping to restore the eye’s focus.


-----------
Connected Inhalers

By connecting devices to mobile apps, patients can receive reminders and check on their own usage statistics. Novartis, the Swiss multinational pharmaceutical, undertook connected inhaler research with both Qualcomm and Propeller Health, developing inhalers for patients with chronic obstructive pulmonary disease (COPD). The Propeller Breezhaler device connects to a digital platform via a sensor, passively recording and transmitting usage data.

-----------
Wearables

Wearables, such as the Apple Watch, are key components of the digital health ecosystem. Takeda, the Japanese pharmaceutical company, tested the use of an Apple Watch app to help patients with major depressive disorder (MDD). The app was developed alongside Cambridge Cognition, a provider of cognitive assessment software, and is designed to monitor and assess cognitive function.

-----------

Land Ownership

A project set up last year by the Lantmäteriet, Sweden’s land registry authority, to trial blockchain technology for recording property deals, has just moved to its second phase. Where the first phase was essentially a presentation of the technology’s potential, this latest phase involved the creation of smart contracts that automate transactions on a blockchain in a larger-scale environment. 

---------------
-----------

Sensor fusion for data center

Data centers are already well-instrumented, with sensors that provide a lot of real-time and historical data on IT performance and environmental factors. In 2018 Google moved closer to a self-driving data center cooling system by taking  snapshot of the data center cooling system with thousands of sensors every five minutes and feeding it into an AI system in the cloud. The fusion AI predicts how potential actions will affect future energy consumption and picks the best option. This is sent to the data center, verified by the local control system, and then implemented.

-----------------


Harvard’s Robobee faces energy efficiency challenges

Inspired by biological phenomena, Harvard’s School of Engineering and Applied Sciences (SEAS) along with the Wyss Institute for Biologically Inspired Engineering researchers are developing RoboBees prototypes, which can perform various disaster relief and agriculture-related tasks. It is the “lightest insect-scale aerial vehicle so far to have achieved sustained, untethered flight”

-------------
Training for nuclear plants

Accessing Nuclear Power plants could be unsafe and hazardous for an untrained staff. This makes it challenging for companies to train their new employees, given the risk of a possible mishandling. GE Power has overcome this challenge by using VR to train its employees. It involves training employees using a 3D representation of steam turbines that spin generators inside nuclear power plants. The 3D visualization helps new engineers learn assembling and dismantling of turbines, and to troubleshoot any issues.


---------------


Wearables

Flexible batteries are being developed for use in the wearables market – often small devices which can be incumbered by large battery packs. For example, engineers at the Columbia University School of Engineering and Applied Sciences developed a prototype in 2018 of a high-performance lithium-ion battery which, shaped like the human spine, allowed for high degrees of flexibility, energy density and a stable voltage. 

---------
Foldable Devices

Flexible electronic devices are often hindered by the lack of a flexible energy supply. Samsung’s Galaxy Fold was released in 2019 to mixed reviews – but provides an insight into a potential future of foldable devices. 

Researchers at ETH Zurich developed a prototype in 2019 for a flexible, thin-film battery that can be bent, stretched or twisted thanks to a new hydrogel electrolyte.  

------------

Internet-of-Things (IoT)

Flexible batteries are well suited to the IoT – small, high-volume applications such as smart labels, smart tags, medical patches, pill bottle trackers, temperature tracking, medicine-delivery pens. Companies like Imprint Energy are developing flexible batteries which enable devices to communicate over short of long distances, particularly for LoRa or LPWAN.  

--------------

Leveraging sensor data at Woodside Plant

Woodside’s Pluto LNG facility in Western Australia is now equipped with 200,000 sensors that monitor operations 24/7. The company used sensor data to build an algorithm that allows the team to predict and prevent foaming in the Acid Gas Removal Unit, a critical part of the production process that cannot be monitored directly. 
Currently, Woodside runs more than 6,000 algorithms on the sensor data from its Pluto plant. 

--------------

Amazon’s NLP & AI platform, Alexa

Solution for a European energy provider using Amazon’s NLP & AI platform, Alexa. An Alexa Voice skill was developed and tailored to allow energy customers to manage their energy account through Amazon’s voice service.

Developed key features suitable for use by energy customers using Alexa’s voice skill.
Mobilised and delivered the solution to meet the Amazon Echo UK launch (end of Sep 16) achieving Amazon certification first time. 
Upskilled the client Team on Alexa Skill Kit and AWS Lambda Skills.

Impact of VA implementation:

20% uplift in brand benefit due to Amazon strength
2,350 account balance checks completed 6 months post go-live
730 contract end date checks completed 6 months post go-live

--------------

Location Services

Bluetooth 5 has the potential for adoption in many scenarios, particularly where location accuracy and navigation is important. This could include more efficient airport navigation experiences, asset tracking of inventory in manufacturing and smart city infrastructure that could help those with impairments be more mobile. 
Bluetooth 5 can also play a role in Beacon technology, which allows businesses, in retail for example, to beam messages to nearby potential customers with deal offers or advertisements. Beacon technology can also facilitate better indoor navigation, or be used in online payments or banking, among many other things. Current examples of beacon technology include Apple’s iBeacon or Google’s Eddystone, which both use Bluetooth Low Energy communication technology.

----------------

Smart Grid Analytics Platform

Smart Grid Analytics, Advanced Grid operations and Outage management are realizing the value of the investments in the Smart (digitized) Grid. Clients in Utilities have high expectations of the benefits from IoT solutions. With Smart Grid Analytics Platform  could deliver the capital avoidance to 8%, OPEX reduction to 15%, leakage reduction to 10%, labor productivity to 20%-3%, energy bills to 10%, data & IT costs to 40% with annual savings of $50M.


------------
Fabrication

By making the basic building blocks of batteries out of ink, Harvard materials scientist Jennifer Lewis is laying the groundwork for lithium-ion batteries and other high-performing electronics that can be produced with 3-D printers.


--------------

Electric Car Charging

RWE Generation is currently one of the leading power generation companies in Europe. RWE wants to trim costs by lowering expenses related to energy transmissions. The working prototype improves customers experiences. Users can authenticate, place a deposit or get their deposit back. They can perform payment processing and loyalty point assignments.!


--------
Smart Electricity Meters

EMS Invirotel provides enterprise software development. Their goal is to respond to the growing need for smart energy solutions. The company launched a service allowing consumers to buy electricity in bitcoin, leveraging new smart meters with integrated services.


-------------

Insurance

Elering AS and WePower are teaming up to offer a blockchain-based green energy trading platform that aims to drive global green energy adoption and promote sustainable living, today announced a pilot project to test the large-scale tokenization of Energy data on the blockchain in Estonia.


---------

MERGER & ACQUISITIONS

Shell used a combination of Salesforce platform, Microsoft Azure and Microsoft Office 365 to divest selected downstream businesses in and enabled these businesses to operate on a new IT platform, separate from the Shell global IT landscape. Using these predominant cloud platforms, Shell achieved a successful IT merger and acquisition implementation, allowing the deals to be implemented in a faster, more flexible and cost-effective way than would have been possible using a traditional approach.

-------

Billing and Revenue Management

Endesa, one of the largest electric power companies in the world with more than 25 million customers in Spain and Latin America, has rolled out smart meter infrastructure which provides metering, billing and contract management information, aiming to install more than 13m smart meters in Spain by 2018.

------------

CyberHawk will monitor for wildfires

CyberHawk has won a drone inspection contract with a major California utility provider to provide inspection services to maintain and detect any issues with the state’s electrical infrastructure. The contract will mainly focus on supporting the critical wildfire prevention and reliability campaign. The drones will be inspecting thousands of lattice steel towers and wood electricity poles that deliver power to much of California’s residents and buildings.


-----------------
OPTIMIZATION OF UTILITIES NETWORK SUPPLY QUALITY 
 
The Dubai Electricity and Water Authority (DEWA) is experimenting around distribution and optimization of the water and electricity networks, partnering with Microsoft in quantum.


---------------

ANALYSIS OF US ELECTRICAL GRID DATA 

Researchers at Purdue University are working on a solution to problems that arise from the huge amount of data generated by the United States electrical grid. Sensors collect 3 petabytes of data every two seconds, meaning that data analysis on that scale is extremely challenging. A potential solution combines quantum algorithms with classical computing on small-scale quantum computers to speed up database accessibility.

--------------
REAL-TIME THREAT MONITORING

Danish utilities company NRGi selected IBM QRadar SIEM solution to consolidate and analyze log events from across the network. The solution helps NRGi detect potentially illicit activity on its network by monitoring all web servers in its DMZ (demilitarized zone), as well as all primary servers within the parameter. 

-----------

Real time dynamic pricing for American oil & gas company 

Real time dynamic pricing allows to optimize price setting across large assortments by simulating the impact of new prices with live data (supply, demand etc.)
For an American oil & gas company  big data, machine learning and mixed integer programming are leveraged to derive the appropriate price based on :

Driver information : Prices, demand, supply, external data, Geography
Prediction model : Predicts terminal demand, and impact of different rack prices 
Optimization engine : Uses mixed-integer programming, to recommend prices intra-day based on expected demand for each terminal and channel

-------------------
Site Planning and Demand Modelling

Endesa provides utilities services to over 22m customers in Spain. The business realized a growing need for a location-based solution that could make site planning and demand modelling decisions accessible to various stakeholders throughout its international organization. Endesa, with CARTO, used complex, interactive demand models that measured and evaluated potential sites for expansion, so that the business could open customer care centres based on demand, expand market share in competitive areas and lead targeted geo-marketing campaigns using location data.


-------------

Network Analysis and Optimization

Connectivity analysis can be used to determine weaknesses in a networks such as a utility power grids, while also enabling comparisons of connectivity across networks. Telecommunications companies, another example, can also use graph analysis to understand the quantity and location of towers to ensure maximum coverage. This type of analysis can also be used to help prevent cyber-crime attacks on computer networks


-----------

Usage Analysis

Companies in the utilities space are using personal analytics to provide analysis of household level usage against best and average usage by comparable nearby households. Connected home technologies are also leveraging personal analytics to provide prescriptive courses of action and enhanced value from user/sensor data.

--------------

Smart Meters/Cities

The IoT can help utilities reduce overheads by reading meters remotely instead of sending personnel out to do it. Companies like SparkCognition, which offers AI solutions to predict energy consumption patterns, will benefit from increased 5G data streams. Petra Systems provide smart street light systems that allows cities to cut costs while generating revenue, utilizing motion sensors that minimize energy costs in addition to cellular antenna that can be licensed to wireless carriers. These small-cell antennas offer the ideal infrastructure required for 5G networks.



-------

Customer Service

During Southern California Edison’s (SCE) smart meter deployment of 5 million residential and small business accounts, the utility proactively engaged consumers by posting 68 FAQs organized in four categories—smart meters, privacy, home and business area networks, and opting out—and providing a consumer portal as part of a My Account program to track energy usage by hour, day, month or year. 

------------

Energy Efficient Gamification

On its journey to deploy 2.2m smart meters, CenterPoint Energy created an annual Biggest Energy Saver contest, to encourage consumers to reduce monthly energy usage compared to similar periods in the prior year. As a result of its engagement efforts, a 2011 survey of CenterPoint Energy’s consumers indicated 80 percent approved of the utility’s smart grid, 70 percent were interested in smart appliances/thermostats and 89 percent valued energy savings enabled by smart meters.

--------
Customer Service

Through the installation of 823,000 smart meters and creation of the SmartHours program, Oklahoma Gas & Electric (OG&E) was able to reduce its load by 70 megawatts. SmartHours provides participating consumers with notice of the next day’s peak price via phone, text or e-mail. Consumers could then take action to shift their usage out of the peak period.


-----------

IT As-a-service

Utilities companies are looking at how technology can reduce costs, drive efficiencies, enhance competitive advantage and support emerging business models. Cloud helped one of the large utilities company to streamline its inventory system, meet compliance and eliminate legal risks. The objective is to optimize the configuration of existing software products, eliminate legal risks and minimize software licensing expenditures. introduction of Softline system integrator which developed a concept for cloud service use, virtualization and thin clients. This concept is expected to increase the efficiency of the company's IT infrastructure by 30%, the efficiency of resource use by 40% and speed up making decisions online. And the company gained access to more powerful software inventory and analytical tools, ensured maximum coverage of IT assets by specialized software, and brought its regulatory documentation into compliance with the ISO standard and existing corporate policies.

------------
Surveillance

Segway Nimbo: can be pre-programmed to patrol specific routes or self-optimized routes while analysing human activities and surrounding environments in real time. It is designed to detect security violations or anomalies and approach the area with light/audio/video warnings that correspond to the situation. It collects HD video evidence and pushes notifications, including live video streaming, to security personnel.

--------
Delivery Robot

Boeing heavy lifting drone: Boeing has designed a prototype for an electric, unmanned cargo air vehicle that can lift as much as 500 pounds. The system would help deliver to places like oil rigs or islands connected to the mainland only by ferries. The drone navigates and looks for obstacles using components and software provided by Near Earth Autonomy.


-------------

NASA invests into swarms for space exploration

Robots will be used in research projects that NASA’s Autonomy Research Center for STEAHM is collaborating for missions such as exploring the Martian surface and voids. A swarm of simple, small robots can complete complex tasks much more efficiently and effectively than a single large rover.


-----------------

Monitoring aquatic environments using swarms

The EU-funded project in Venice lagoon has three different bio-inspired robot species: an aPad robot floating on the water surface, an aFish swimming in shallow waters and an aMussel robot covering the seabed. The robots are designed to talk and listen to each other and to develop as a “self-organizing underwater swarm”, using bio-inspired algorithms influenced by nature.


-------------------

Drones to enforce social distancing

With the increasing need for social distancing and the usual budgetary stressors, local police departments are being forced to do more with less manpower.  The town of Linn, Wisconsin worked with a consulting firm to start a drone program. The department has been able to utilize the drones to patrol its large jurisdiction while maintaining social distancing and reducing the number of officers in the field.


--------------
Oman establishes innovation center for Liquid Robotics’ autonomous vessel

Oman has established an innovation center built around the Wave Glider, a solar-powered autonomous vessel developed by Liquid Robotics, a Boeing company. It supports a wide variety of missions for environmental and maritime services, ranging from marine environmental monitoring and assessment to maritime surveillance and a variety of offshore commercial, security and defense applications


-----------------

F35 fighter jets sensing is next gen

The Lockheed Martin F-35 Lightning II’s calling card is being a mobile sensor fusion package that can pack effectiveness of an entire fleet by sharing data and coordinating information across platforms such that the situational awareness of every warfighter present is increased exponentially. The fusion engine takes the myriad data the F-35 collects with its different sensors and combines it into a holistic picture that can be fed to the pilot — or other combatants


-------------

REDUCE FRAUDULENT CLAIMS

German state department, Landesverwaltungsamt Berlin, used IBM’s Watson® Fraud Cockpit built on IBM Watson Analytics, to identify potential cases of fraud among millions of legitimate claims for reimbursement. The solution helps avoid misuse of public money by enabling targeted investigations into potential fraud.


------------------
STOP ATTACKER MOVEMENT

US State Department of Revenue deployed Illusive Networks Attack Detection System to detect and stop in-network attacks and malicious insider activity. Because the servers’ traffic was east-west and wasn’t necessarily malicious in nature, none of the Department’s EDR or antivirus tools picked up on the activity.


------------------
Ottawa’s AI-based chatbots

The City of Ottawa will pilot Microsoft’s Power Virtual Agent to increase the accessibility of its 311 services.
The AI chatbot pilot driven in collaboration with Microsoft will begin in the first quarter of 2020. Users will be able to enter questions about the city’s services and receive immediate answers in a conversational format. The idea is to complement human instead of replacing them.

----------------

4D Printing in the Army

The U.S. Army Research Center is creating a range of applications using 4D printing, including a solder’s uniform that can alter is camouflage or provide more effective protection against poisonous gases or shrapnel. The U.S. Army also gave a grant to Harvard University, the University of Pittsburgh and the University of Illinois to investigate ways that the military could use self-assembly for shelters or bridges that assemble themselves.


----------------
THE DWP CASE


Novel payment models could enable HM Treasury (HMT) and the Department for Work and Pensions (DWP) to distribute welfare support more efficiently while improving policy delivery. The DWP currently pays out roughly £166 billion of taxpayer’s money in welfare support each year.
Some £3.5 billion of that sum is overpaid through fraud (£1.2 billion), claimant error (£1.5 billion) and official error (£0.7 billion) of which £930 million is recovered. Adding the fraud and error that exists in the current tax credit system, which will be moving to DWP over the next few years as an element of the new Universal Credit regime, there is a total baseline of over £5 billion per year in gross overpayments.
Taxpayers also bear the cost of post-payment intervention (debt collection, investigation and prosecution, claimant queries and dispute resolution).
How Blockchain technology could help

A large number of welfare claimants are un- or under-banked and face barriers to greater financial inclusion. Distributed Ledger Technology (DLT) offers a cheap and supportive means of getting these claimants into the benefits system.
Digital identities could be confirmed through DLs running on securely-encoded devices — or software on a mobile device —allowing end-users to receive benefits directly, at reduced transaction costs to banks or local authorities. Such a solution could also be linked with other systems to reduce the level of fraud and official error in the delivery of benefits, as identities would be more difficult to forge.
It would be possible to set rules at both the recipient and merchant ends of welfare transactions. Ministers could consider options for achieving better policy outcomes from the distribution of welfare support by agreeing or setting rules around the use of benefits.

By applying DLTs in the registration and payment processes for government grants and benefits, DWP will be better equipped to

Prevent financial losses through fraud and error
Support the most vulnerable citizens through full financial inclusion
Offer good value for money and place public expenditure on a sustainable footing
Enable government to assure taxpayers that public funds are being effectively for the purposes of meeting genuine need.

REQUIREMENTS FOR IMPLEMENTATION

Requires much education for the recipients
Requires some integration of sterling onto a distributed ledger for benefits
May create a subset of the economy with a stigma attached to ‘benefit coins’

Sources: UK Government Office for Science

------------------


Voice-Based Processing of Tax Payments

A European Revenue Agency wanted to enable its citizens with voice-basedprocessing of tax payment. 


User authentication
End-to-end payment processing, including processing payment details
Answering questions about the local tax policies
Account PIN reset functionality


Bespoke Persona: Designing a friendly Virtual Agent persona to relate to the agency user base
Telephony Integration: Integration with telephony using Twilio to enable users talk to the Virtual Agent on a call
Voice and Chat Web Interface: Web interface allowing users to interact with the Virtual Agent either by voice or chatting


Tags: Customer Insight & Growth Strategy , Virtual Agents , European Revenue Agency , Citizen Service , Twilio


-------------------

Crime Pattern Detection

The Geneva Police Service uses location intelligence to help with crime pattern detection, using available data for strategic and operational analysis. Galigeo’s solution combined data from the data warehouse with GIS tools, giving users access to an integrative mapping tool, allowing for strengthening of resource allocation efficiency, and the capability to establish preventive and dissuasive actions. Geospatial data, in combination with predictive analysis tools, can be used to identify trends in criminal behaviours and understand and predict effectively when and where certain types of crimes are most likely to occur.


--------------
-----------------------
RECRUIT’S QUANTUM MACHINE LEARNING FOR MARKETING AND ADVERTISING APPS

Japan’s subsidiary Recruit Communications is testing quantum computing for marketing and advertising applications, specifically to improve recommendation systems with machine learning methods.

-----------

Forbes mobile goes PWA

Forbes launched its mobile web experience for Forbes.com that has a similar look and feel to Snapchat stories. The new mobile experience was developed as a Progressive Web App that offers an improved user experience that includes faster load times, app-like navigation and highly visual content


---------------

Verizon Media tests cloud CI/CD platform

Combining Screwdriver, an internally built, open source, CI/CD platform with the Sauce Labs cloud-based testing platform, Verizon Media—home to leading brands such as AOL, HuffPost, TechCrunch, Yahoo Finance and Yahoo Sports— implemented a powerful solution for continuous testing and integration.


--------------

Vodafone goes cloud native for IoT & 5G

With users in 25 different countries Vodafone began building a centralized platform for the product teams delivering digital experience in the different markets to use across different public cloud platforms. This provided for deployment of services across edge sites over 5G networks from a single location.

---------------
QUANTUM NETWORKS

KPN has implemented end-to-end quantum key distribution (QKD) in its network between KPN datacenters in The Hague and Rotterdam, the Netherlands. It is collaborating with ID Quantique, a Swiss company specialized in quantum encryption.


---------------

MANAGED SECURITY 

Align Communications deployed MDR solution from eSentire, esNETWORK, to provide network monitoring services for its clients in financial services domain.


-----------
360-DEGREE APPLICATION PROTECTION

UK-based telecom provider EE, part of BT Group, used Imperva FlexProtect for Applications to protect its consumer web pages and B2C e-commerce applications. The solution added additional SaaS services such as DDoS Protection to safeguard against large-scale botnet attacks, and Attack Analytics, which uses machine learning to distill thousands of security events into several, actionable narratives.

------------
PROTECT INTELLECTUAL PROPERTY

A large global semiconductor manufacturer used Attivo Networks’ ThreatDefend Platform to gain visibility into  in-network threats, stolen credential attacks, and replace false positives with substantiated alerts. With immediate visibility, the team  receives alerts only on suspicious or malicious activity in the network without extensive investigations.

-----------
China takes QKD seriously 

Most of China’s massive investment in quantum has gone toward quantum security itself, including the development of quantum key distribution (QKD). China looks to be all in on QKD networks. The country has already built a 1,263-mile link between Beijing and Shanghai to deliver quantum keys. And a successful QKD demonstration by the Chinese Micius satellite was reported across the 4,700 miles between Beijing and Vienna.


----------------
Technicolor’s transformation with WinAutomation

Using the desktop-based WinAutomation solution, Technicolor began automating the top priority processes. For example, the project team built a process capable of logging into a FedEx portal, downloading invoices for deliveries, creating a combined PDF file, and send it to another email account for further processing. This completely eliminated a time-consuming daily task for the Accounts Payable team.


-----------------

DaaS to support Toppan Merrill’s M&A strategy

Toppan Merrill is a $14 billion Japanese-owned printing company, headquartered in New York. The organization made a substantial investment in new virtual desktop and back-end IT infrastructure to support business growth through strategic acquisitions. After a number of acquisitions, the organization effectively doubled in size, and this is why it decided to invest in Virtual Desktop Infrastructure (VDI) and Desktop as a Service. 


--------------------

CSP ROAMING TRANSACTION RECONCILIATION

In the highly regulated and competitive telecom industry, Communication Service Providers (CSPs) engage in complex roaming works to extend the range of their coverage but roaming traffic is difficult to monitor and reconcile. Third-party Data Clearing Houses (DCHs) serve as middlemen to reconcile roaming transactions but keeping track of this process remains time-intensive and expensive. A lengthy reconciliation process can enable roaming fraud, currently accounting for over $10 billion revenue loss. The solution: Blockchain-based smart contracts can replace middlemen, enabling automatic reconciliation and payment across multiple roaming partners. Reducing costs from DCH intermediaries, preventing fraud, smart contracts automatically execute the terms of an agreement after a transaction is broadcast on the blockchain. Their dynamic nature make them a valuable solution for networks that are continually evolving. A blockchain solution enforces disparate contracts in complex roaming networks that cross geographic and regulatory boundaries. 


----------------------
5G AND PICO CELL TECHNOLOGIES 

5G and Pico Cell technologies will allow for greater speed and more reliability among devices requiring a cellular connection. Enhancements will improve data transfer speeds, transmit/receive latency and connectivity. 
Mobile Data, Replacing Wireless Networks: Instead of having a mobile data plan and home network, consumers will consolidate and only have a mobile data plan. The speed of the new mobile data will be as fast if not faster than current network speeds.  Low Latency: IoT connectivity can be further improved by reducing the communication times and improving transmit and receive speeds. Advancements will improve how cars communicate with each other, feedback between assembly line robots. Improvements will be seen in the reliability of connected devices in both industrial and personal environments. 

-----------------

GESI SUSTAINABILITY IMPACT QUANTIFICATION 

GeSI, the Global e-Sustainability initiative, has a clear vision: to build a sustainable world through responsible, Information and Communication. Technology (ICT)-enabled transformation. They approached to quantify the sustainability impact of digital solutions using analytics and actual data and to define methodology to steer activities of digital industry to maximize positive impacts. To date, the client has committed to elevate positive impacts, flip negative impacts and innovate for all Sustainable Development Goals (SDGs).


--------------

Exploring Titanic shipwreck using VR

Immersive VR Education has designed a new education game that lets the users explore the wreckage of Titanic from the eyes of a survivor. The game uses motion capture reality and includes missions and storytelling. In this 6-hour long game players are required to navigate the wreck of the sunken ship. Further, the game includes dive missions and lab missions to recover artefacts. Such high-quality content are likely to fuel the growth and adoption of VR.
 


----------------
Interactive bot at Dixons Carphone

Working with Microsoft Services, Dixons Carphone investigated the capabilities of the Microsoft Bot Framework and Microsoft Cognitive Services to build AI into its customer interactions. The Bot Framework is a platform for building, connecting, testing, and deploying powerful and intelligent bots—apps that customers interact with in a conversational way.


----------------

Multi-Tenant Big Data Management Solution for a Media Broadcasting Corp. 

Germany's Media Broadcasting corporation modernizes its legacy system to manage large discrete media files of over 4 PB/year and reduces data storage costs by 93%, using Multi-Tenant Big Data Management solution. To build this robust and cost-efficient solution, the Azure development experts have leveraged Azure PaaS including Azure Storage, Azure Health Monitor, Azure Traffic Manager, Azure Active Directory, Azure Media Service, and Azure App Service.


---------------
BT WORKS TO SECURE ITS NETWORK

BT announced an ‘unhackable’ quantum-secured network in June 2018. The collaborative project was led by the Quantum Communications Hub, part of the UK National Quantum Technologies Programme. 
The quantum link itself is said to be virtually “unhackable” because it relies on the use of single particles of light (photons), to transmit data encryption “keys” across the fibre. Should this communication be intercepted, the sender will be able to tell that the link has been tampered with and the stolen photons cannot then be used as part of the key, rendering the data stream incomprehensible to the hacker.

-----------

QUANTUM SAFE SYSTEM ON DEUTSCHE TELEKOM‘S TRIAL NETWORK

SK Telecom has applied its quantum safe system on Deutsche Telekom’s trial network and plans to expand deployment to parts of commercial networks this year.
The South Korean telco's quantum-safe system consists of quantum key distribution, quantum random number generator, and an operating system.

-------------------

KPN WORKS TO MAKE QUANTUM-SAFE INTERNET A REALITY

KPN will be providing the infrastructure and locations that QuTech, a collaboration between TU Delft and TNO, will use for research and development. A desired application of a quantum internet is to provide virtually unbreakable privacy and a foundation of secure communication, guaranteed by the fundamental laws of physics.


---------------------

MNP CHURN PREVENTION

Prevent churn of SIMs (Subscriber Identity Module) with high MNP (Mobile Number Portability) churn risk.
Value-driven “urgency prevention” for high MNP risk customers leveraging MNP risk scoring model, customer behavioral triggers and offer personalization.


-----------------
FACEBOOK USER EXPERIENCE OPTIMIZATION THROUGH BROADBAND CAPACITY INNOVATIONS

Facebook unveiled Terragraph and Project Aries, two land-based connectivity systems in April 2016. Expansion of 4K, 360 and VR/AR content coupled with expected traffic and data usage growth suggest the need for Facebook to keep pace by investing in 5G capabilities.
Terragraph’s faster connectivity provides seamless sharing; a low-cost alternative to fiber and bringing street-level coverage of gigabit Wi-Fi (5G)
Project ARIES (Antennae Radio Integration for Efficiency in Spectrum) Facebook aims to solve the problem of finite bandwidth and growing data usage by improving spectral efficiency.
Impact: 5G through Terragraph will improve sharing and network efficiency in data centers. Through Project ARIES, Facebook can continue to enable high quality streaming amid data usage trends.


--------------------

FACEBOOK INVESTS IN GLOBAL CONNECTIVITY 

With approximately 40% of the world connected to the internet, Traditional models are infrastructure intensive and lack the scalability needed to reach more remote or impoverished areas. Facebook’s Aquila program will utilize a fleet of Unmanned Aerial Vehicles (UAVs) and new breakthrough laser technologies to bring internet to everyone on Earth. 
The Business Model evolves from infrastructure-heavy investments to agile UAVs offering connectivity to people previously inaccessible. Operations Transformation sees use of laser technology and integration of Messenger, WhatsApp and Free Basics provided free to users instead of traditional mobile models.
Impact: 4 billion new people connected to internet will have access to Facebook’s services. Facebook will own this larger value chain.

-----------------

Real-time location data

Vodafone’s office in London includes technology that provides accurate and real-time data to drive efficiencies in the office. For example, sensors provide accurate and anonymous locations of people within each floor. Real-time information on desk status gives employees the ability to locate colleagues via smartphone, or search for groups of available desks. Integration with google maps is also available and  the system takes a data ‘snap’ every 10 minutes.


----------------

Call Centre Efficiency, Customer Service

Vodafone, one of the world’s largest mobile telecommunications companies, wanted to increase call centre efficiency and improve overall customer service quality. Providing a unified and current view of all customer data while minimizing data replication was challenging, due to inefficient processes, often obsolete data and high training costs. Vodafone worked with Denodo to connect source systems that included databases containing customer data related to billing, subscriptions and incident management systems. Real-time delivery of data allowed agents to work with the most current version of the data and there was a 66% reduction in service response time from 6 minutes to 2 minutes.

----------------

Customer Query Response Time

The pay-TV provider Sky wanted to be able to deliver split-second responses to customer queries, meaning that it required direct access to data generated at a number of touchpoints across the company and stored in a number of databases. The business required flexible data integration capabilities with very high data security, and used the Talend Data Management Platform to support an architectural layer that aggregated data and accessed a staging layer to deliver data to the user platforms. Better data integration and interfacing processes meant that the data quality was improved considerably. As the fast-access layer of the front-end database is constantly updated, it gave Sky an extremely realistic picture of the current state of its business.


------------------

KPI Analysis

Telecom Italia, the telco provider, required a flexible, user-friendly solution for visualizing large amounts of data. The telco wanted to improve customer service by extending and reinforcing its ability to monitor network service, by analysing key performance indicators for mobile network voice and data traffic (e.g. accessibility, drop rate, call setup time, data throughput). SAS Visual Analytics allowed the company to display data in a user-friendly format, meaning decisions could be made quickly based on up-to-the-minute trends. Telcom Italia added in-memory analytics and advanced data visualization to the provider’s geo-marketing system, simplifying the decision-support and operational processes that go into technical and commercial planning.



----------------------
---------------

Neuralink surgical robot enables next gen BCI

The Neuralink surgical robot, revealed by Elon musk in August 2020, is supposed to manipulate and insert the tiny brain computer interface threads into the brain. The robot is able to both capture your brain and then with almost a sewing machine-like, micro-precise needle and thread, place the neural threads in the exact right location based on the surgeon decisions around what the safe locations are for the threads to be inserted


------------------
Using Boston Dynamics Spot for Healthcare monitoring

Boston Dynamics, an autonomous robot making company is contributing to the pandemic through autonomous robot-dog named Spot to help protect healthcare workers from Covid-19 at Brigham and Women’s Hospital in Massachusetts. With the help of an iPad and two-way radio on the robot, healthcare workers video conference with patients and remotely control Spot as the robot dog walks through rooms with infected or potentially infected patients


-----------------

NASA to deploy self driving tech on mars

NASA is expected to launch its latest Mars rover, Perseverance, on a first-of-its-kind mission to collect and store geological samples. Perseverance is significantly more autonomous than any of NASA’s previous four rovers and is designed be a “self-driving car on Mars.” Like the ones on Earth, it will navigate using an array of sensors feeding data to machine vision algorithms


----------------
Russia, India launch Robonauts

Several countries have been working on humanoids for space exploration. In India, Vyommitra, a female humanoid robot, is set to launch on an uncrewed spaceflight in December 2020. Fedor, or Final Experimental Demonstration Object Research, was a Russian remote-controlled humanoid that flew to the International Space Station (ISS) in 2019, where it simulated repairs during a spacewalk, and later returned back to Earth.

-------------------

ELEVATOR IOT CONNECTIVITY

Telefónica and Schindler have formed a global partnership for digital connectivity. Under the agreement, Telefónica will be the IoT and network connectivity partner for Schindler’s digital offering for smart elevators and escalators – Schindler Ahead. The cloud-based digital platform Schindler Ahead is the world’s fully digital closed-loop maintenance, emergency services and information system.  Telefónica, a supplier of IoT solutions, is providing Schindler with IoT connectivity services via its global roaming network, ensuring high-quality service and guaranteed connectivity for Schindler Ahead customers worldwide.

------------------

KINGSPAN SENSOR & VODAFONE

Kingspan Sensor, a provider of monitoring solutions for the fuel storage sector, has implemented Vodafone’s Managed IoT connectivity platform to offer a global monitoring service. The company supplies measuring and monitoring solutions to customers in the commercial, industrial, agriculture and domestic markets.

------------
HIGH-PERFORMANCE BATTERIES AT VOLKSWAGEN

Volkswagen has two in-house data labs for quantum computing in San Francisco and Munich. Volkswagen has also partnered with Google to explore structures for new materials, especially high-performance batteries for electric vehicles.

--------------
BATTERY CELL RESEARCH AT DAIMLER

Daimler, in partnership with IBM, is investigating quantum chemistry calculations for new battery technology. Daimler is also working with Google to create new materials for quantum technology and the development of new battery cells. 


----------------
COMPROMISED USERS AND HOST DETECTION

Aruba, an HPE company, has integrated multi-dimensional user and behavioral analytics, diverse data sources, and forensics to detect credential-based advanced attacks. The UEBA tool automates the detection  of compromised users and hosts with analytics-driven visibility. 



------------------
Unified Digital Experience

HP selected Ping Intelligent Identity Platform from Ping Identity to power HP Identity (HP ID). HP ID is an enterprise-wide unified identity management ecosystem designed to up level customer and partner experiences by providing them with a single identity across all of their HP applications.

--------------
Secure mobile communications

Mitsubishi Electric has developed the world’s first “one-time pad software,” an advanced encryption technique for mobile phones to ensure that telephone conversations remain confidential. Additionally, the company is involved in implementing their technology in a project being conducted by the National Institute of Information and Communications Technology to test the viability of mobile communications over a quantum secure network.


--------------

Mobile Devices

There has been initial adoption of Bluetooth 5 in the smartphones market – the first device to ship with Bluetooth 5 was the Samsung Galaxy S8 in March 2017. The iPhone 8, 8 Plus and X, which launched in September 2017, also had Bluetooth 5 support. One example of use amongst consumers is connecting a smartphone to wireless headphones for listening to music or phone conversations.


----------------
Digital sign

Blackberry: Launched a solution which will allow software to be digitally signed using a scheme that will be hard to break with a quantum computer. The Solution is actually the product of a partnership between BlackBerry and Isara Corporation, a company whose mission is to build quantum-safe security solutions. BlackBerry is using Isara’s cryptographic libraries to help sign and protect code as security evolves.


----------------
LOW-COST OPTIONS FOR PRODUCT DEVELOPMENT 

Field-Programmable Gate Arrays (FPGAs) offer low-cost options for product development and can be used in conjunction with traditional processors, external sensors, servos, data storage and IoT to allow organizations to deploy new technologies into the field and help improve processes and drive insight.
Rapid Prototyping: with FPGA’s development, processes can become more agile as additional components/requirements can be quickly coded and bolted on without the need to restart the development process.
Low Risk of Obsolescence: Due to the ability to reprogram the chip sets in FPGAs, they are ideal for Original Equipment Manufacturer (OEM) situations where processes are constantly refined. Capability to reprogram chips via WiFI or Bluetooth means low downtime and minimal process interruption.


----------------

SKELETON TECHNOLOGIES ENERGY-STORAGE SYSTEMS 

Skeleton Technologies is a global leader of graphene-based ultracapacitors and energy-storage systems with 4 x the power density of the nearest competitor. Skeleton deliver high power and energy solutions across multiple industries. Ultracapacitors are ideal solutions when energy needs to be stored and discharged rapidly and efficiently with low heat and weight requirements, making them a great complement to traditional battery and carbon-based fuel systems. Skeleton partnered with Adgero SAS to develop a Kinetic Energy Recovery System (KERS). This partnership allows Skeleton to bring its proven Graphene-based ultracapacitors to the automotive, industrial, aerospace and transportation industry. Incumbents have achieved only 25% of the energy density of graphene-based solutions. Operations Transformations: KERS allows the transfer of energy during braking to charge the ultracapacitors, which energy can power connected electric motors. By using energy captured during breaking to accelerate the vehicles, fuel consumption can be significantly reduced.  
Impact: Smaller batteries required to power vehicles. 25% reduction in consumption in diesel applications.


-----------------

Google launching AI-enabled automation tools

Contact Center AI, Vision Product Search, and Document Understanding AI are a beta version solutions which natively integrate with automation solutions i.e. From UiPath, , Iron Mountain, and others. Google’s AI and machine learning products and services portfolio keeps growing.



---------------
Kyocera accelerates pricing approval process with Bizagi

Kyocera, the printer and copier manufacturer, completed a project to digitize and automate a special pricing approval process for large accounts such as banks and governmental organizations.
The automated process has not only reduced the process time by 85% to accelerate turnaround time and win Kyocera new business, but it has also helped to free up the time of employees. The process, which was previously conducted via email and Excel spreadsheets was modeled in Bizagi and integrated with SAP to directly activate the opportunity to purchase.


-----------------

Making AI more inclusive

Google is taking a two-pronged approach. It is expanding the data set that it is training its "Image Search" algorithms on to be more expansive and inclusive, but the company also launched a competition to source improved algorithms that mitigate the bias of the algorithm itself. The competition received submissions from over 100 participants, and winners were able to make a small step in improvement.


---------------
Internet Access to underserved regions

Project Loon is designed to provide internet service to underserved regions of the world through a system of giant balloons hovering in the stratosphere. Their navigational systems employ Gaussian processes to predict where in the stratified and highly variable winds aloft the balloons need to go. Each balloon then moves into a layer of wind blowing in the right direction, arranging themselves to form one large communication network. 



------------------
Machine Common Sense 

DARPA is investing $2 billion in AI research. In its Machine Common Sense (MCS) program, researchers will create models that mimic core domains of human cognition, including “the domains of objects (intuitive physics), places (spatial navigation), and agents (intentional actors).”


-----------------

Making Holographic calls

Huawei recently used a combination of MR and 5G to make Russia’s first holographic call. In this type of call a real-time 3D image of the call participant is generated at the other end of the connection. Users require a MR headset to view this hologram/3D representation. Holographic calls can not only provide an enriching experience to call participants, but also help to monitor objects remotely in 3D.


-----------
3D Printed Antennae for Mobile Devices

Taiwanese electronics company, LITE-ON, has been using Optomec’s Aerosol Jet Technology to 3D print antennae and sensors for consumer electronic devices since 2016. 


---------------
Wearables

Apple Watch Series 6 will mark a switch to flexible circuit boards made of liquid crystal polymer. These new circuit boards should make the Series 6 noticeably faster than the Series 5.


-------------
Solar-powered Drone

Printed circuit supplier Trackwise has produced an 85-foot-long multilayer flexible circuit—said to be the world’s longest—for distributing power and control signals across the wings of a solar-powered drone. The flexible printed circuit (FPC) is one of more than 50 supplied by the company for drones.


---------------

PARTNER INTEGRATION

Dell: QAD builds Dell Boomi into its software suite to help global manufacturers create seamless systems integration and ensure data quality. Boomi gives QAD and its customers rapid, agile cloud-native integration to connect an ever-changing mix of on-premise and cloud services. It also provide Master Data Hub for ensuring data quality and synchronizing data among multiple applications.


---------------

Categorizing Contact Centre Transcripts

ACER America, a division of Acer Computers, the provider of personal computer products, peripherals and accessories, was challenged to obtain more value from recordings of customer calls to their contact centre. The business was concerned about efficiency and value gained from the analysis process, and leveraged a Clarabridge text analytics solution for an automated approach to categorizing unstructured text for analysis of data from call transcripts and agent case notes.


---------------
Dashboarding

Lenovo, the global technology company, used visual data discovery to drive a 95% efficiency improvement in reporting, alongside a more targeted e-commerce strategy, while transforming 100 reports to strategic dashboards in HR. With Tableau, Lenovo’s Analytics BI and Visualization team created a flexible sales dashboard that different departments could adapt for ad-hoc analyses. Customer engagement metrics could be better analysed in order to craft a better online experience, leading to better brand perception and increased revenue. Before this, Lenovo had worked off of one single sales report in Excel, delivered to 28 different countries.


----------------
Connected Appliances

IBM’s Watson IoT services offer integrated capabilities, delivering services by leveraging a  pool of consultants, data scientists, designers, security experts and domain specialists. Whirlpool, the multinational manufacturer of home appliances, used the Watson IoT platform to drive a 70% decrease in customer services time for connected home appliances.


----------------
Prescriptive Bid Analysis

Effective contract auditing must be backed by a compliance monitoring mechanism. As per procurement community Suppliers know that contracts are difficult to verify and would be helpful to have something to auto detect non-compliance and highlight.  created Contract compliance analytics which are powered by text analytics. Compliance Analysis that is compliant vs non-compliant transactions were done and the initial analysis showed the items that can be acted upon. Therefore, prioritization by Value at Risk, higher vs lower, favourable vs unfavourable is used to filter the critical ones and minimize the potential cost of non-compliant transactions.

--------------------
ASSEMBLY ROBOTS

Cytosurge’s new pinpoint metal 3D printing is possible due to two state-of-the-art, high-resolution cameras that have been integrated directly into the FluidFM µ3Dprinter. The FluidFM μ3Dprinter’s unique new function of 3D printing structures, with exacting, pinpoint accuracy, directly onto surfaces and objects could potentially revolutionize micromanufacturing by combining traditional microfabrication techniques with 3D printing to create complex metal objects.


------------

Treatment

MIT’s magnetic objects are 3D printed on a specially designed platform, from a newly-formulated ink infused with magnetic particles. This allows for instance to put a structure around a blood vessel to control the pumping of blood, or use a magnet to guide a device through the GI tract to take images.

---------

Intel Warehouse Picking

Using the Ubimax Pick-by-Vision solution, Intel conducted a two-month pilot of the xPick smart glasses in its worldwide distribution centre located in Chandler, Arizona. An immediate impact was displayed. Order pickers without any training were 15% faster during first-time use of the platform (vs. hand-held scanners). In the end, Intel was able to reduce its “pick time per box” by 29%.

---------------

CUSTOMER SERVICE

HP built a virtual agent using the Microsoft Dynamics 365 AI solution for customer service. Customers can now interact conversationally with the assistant to solve common problems, support staff use the Microsoft AI solution for instant access to a wealth of troubleshooting information and the company gains deeper insights into common customer issues.

--------------------
DNA-size computer chips

IBM Research and the California Institute of Technology researchers have found a way to control the placement of silicon nanowires, carbon nanotubes and other tiny components on microchips through the use of synthetic DNA and tiny lithographic templates. The scientists used the DNA molecules to function as scaffolding, to help deposit millions of nanosize particles in precise patterns. 

----------------------
SPACE EXPLORATION 

NASA has partnered with Google to research quantum computational methods for exoplanet detection from analysis of telescopic observations, alongside analysis of optimization and planning problems.

------------------
Swarm Technology at NASA: Building Resilient Systems

Developing space systems is a complex task, driven by standards and safety requirements to ensure reliability of sophisticated hardware and software. Future NASA missions will focus on the development of swarm-based spacecraft systems comprising multiple self-organizing and autonomous spacecraft.


--------------
Istio project - an open service fabric for microservices

The Istio project is a collaboration between IBM, Google and Lyft. It was created as an open service fabric to simplify the integration and management of microservices. It decreases the coupling between application code, the underlying platform, and policy which makes it simpler for operators to move application deployments between environments or to new policy schemes. Applications become inherently more portable as a result.


-------------

Cloud Infrastructure Management at Splunk

Splunk uses Ansible for automating the resource-intensive and time-consuming tasks associated with managing its cloud infrastructure. Its built-in queuing system ensures jobs are run efficiently. Ansible Tower’s powerful features also helps Splunk more easily manage and track its entire cloud inventory, keeping it in sync. Tower’s powerful provisioning callbacks allow nodes to request configuration on demand, enabling autoscaling.


----------------

Polyglot Runtime Vm at Oracle

Oracle GraalVM GraalVM is an embeddable high-performance polyglot virtual machine that aims to run all programming languages very fast. It has the capability to run Java and JVM languages (via bytecode) as well as full support for javascript and Node.JS, with beta support for Ruby, Python, R and LLVM bitcode.


---------------
Redhat Openshift enables polyglot clouds

OpenShift is a polyglot platform that supports multiple languages, including Java, Node.js, .NET, Ruby, Python, PHP etc. OpenShift enables developers to build cloud-native apps on a cloud-based container platform without having to worry about the inherent complexity of provisioning, managing and scaling applications


--------------

Google’s Bazel is a open source polyglot build tool

Google's Bazel was designed to provide optimized dependency analysis and parallel execution for multiple code bases in a given application. Bazel already has support for building a variety of languages, including C/C++, Rust, Scala, Java, Objective-C, etc..


--------------------
Adobe enlists Self healing AI

Adobe’s goal for AI in ITSM was to develop a “self-healing framework. the self-healing platform can find operational issues and automatically solve them, while also providing detailed data and stats to the IT team. In instances where the platform can’t heal itself, it can alert IT workers to issues faster and improve the time to recovery


--------------

IBM Watson AutoAI 

Watson AutoAI helps design and shape the development of AI technologies. It provides Automated Data Preparation, Automated feature engineering, Hyperparameter optimization and Automated model selection to help shape best case solutions.
It helps workaround AI building challenges of regulatory compliant datasets for model validation and tracking and measure of indicators for business success. 

-------------

Paypal Devsecops

Paypal embedded several repeatable proactive security practices in their product life cycle. They also instituted auto-enabled security controls, implemented secure frameworks and security tools, and used threat modeling for uncommon features and web or mobile apps on their standardized, secure frameworks. PayPal’s strategy also included institutionalizing risk-based thinking and processes, requiring security to be integrated by default at all layers, and they focused on automation.


-------------------

Google continues to drive towards PWA

Being one of the pioneers of PWA’s Google recently moved its drive and chat to PWA. Other Google progressive apps  include YouTube Music, Stadia, Photos, Messages etc. Its PWA ecosystem is compatible across any operating system running Chrome 73 or higher.


-----------

Facebook uses Aroma and Getafix

Facebook created Aroma, a code-to-code search and recommendation tool that uses machine learning (ML) to make the process of gaining insights from big codebases much easier. Aroma enables engineers to make a search query with the code snippet itself. The results are returned as code recommendations
Facebook has built a tool called Getafix that automatically finds fixes for bugs and offers them to engineers to approve. Getafix learns from engineers’ past code fixes, its recommendations are intuitive for engineers to review.


----------------
Ubisoft AI-based coding assistant 

Ubisoft has partnered with Mozilla to develop Clever-Commit, an AI-based coding assistant that learns from apps code base’s bug and regression data to analyze and flag potential new bugs as new code is committed. Ubisoft already uses this tool internally and Mozilla  will deploy it to spot bugs in its Firefox code

--------------

Code autocomplete tool from Codota

AI-assisted software development platform company Codota acquired code prediction tool TabNine.It is a code autocomplete tool that predicts the rest of the code programmers are typing — much like word or phrase autocomplete in a Google search window. 
The acquisition creates a comprehensive AI platform for software authoring, as well as the only one which fully supports nearly all popular programming languages


----------------

NASA Brings Mars to Earth with Kinvey’s Telerik DevCraft

NASA has a massive archive of data collected from the Curiosity’s daily missions, but even for experienced scientists, identifying key results and visualizing information was a challenge.
Analyst’s Notebook multiexperience app created with Devcraft to provide a sleek, fast and consistent UI across all web, desktop and mobile platforms
DevCraft Ultimate provided NASA team the ability to work more efficiently with data across devices and turn out the same amount of work without any extra manpower.


-----------------

Application Performance Mgmt at Instana

Instana offers automatic discovery of the complete application stack and infrastructure and tracing of all requests and intelligent AI-powered performance and quality management. It uses an agent-based technology to offer high levels of visibility into what’s going on in the IT environment and the ability to filter down and investigate the most critical issues that require action, and can even automate the responses.
.

--------------
Application Performance Mgmt at Park Place Technologies 

Park Place Technologies, a data center maintenance provider - ensures low human effort by using BMC TrueSight, an AIOps platform powered by machine learning and analytics, to elevate their IT operations and improve the quality and performance of digital services that run in their environments.

--------------

Test Automation at Activision

Activision leveraged the Cloudtestr platform to identify the scope for test automation in line with the upgrade/patches applied by Oracle for Fusion HCM. A robust regression-testing suite was developed to test any system changes. Additionally, test cases leveraging a keyword driven framework and a test data template, was also developed to make it easy to maintain and keep updated.

----------------

Self Driving Cloud Infrastructure at Juniper

Juniper Appformix’s cloud operations platform, leverages big data analytics and machine learning to provide clouds with what Juniper calls "self-driving infrastructure”. It enables cloud native applications like telemetry and operations management across software-defined infrastructures and application software layers. AppFormix's technology is designed to provide with real-time and historic monitoring, visibility, and dynamic performance optimization in both OpenStack and Kubernetes environments

-------------

Alphasense became cloud native from cloud resident

Alphasense was always in the cloud in AWS, but at the same time, wasn’t so much cloud native.  They realized that for a AI platform they needed to change infrastructure because it couldn't keep up with the pace of their development. The company, which had already broken down most of its monolith into microservices, decided to adopt Kubernetes for orchestration, along with Spinnaker for continuous delivery and Prometheus and Grafana for monitoring and alerting


---------------------

Adform moving beyond cloud native

Adform has an OpenStack-based private cloud infrastructure running on 1,100 physical servers in 7 data centers around the world. Their private cloud approach required using local VM’s for each service, leading to a connected inflexible cloud environment. The team, which had already been using Prometheus for monitoring, embraced Kubernetes and cloud native practices. A proof of concept project was started, with a Kubernetes cluster leading to  cost savings of 4-5x due to less hardware and fewer man hours. 


----------------------

SECURE DYNAMIC DEVOPS 

Pivvot embedded security in the development process with Trend Micro™ Deep Security™ Smart Check. The solution performs build scanning, image registry scanning at pre-runtime, secures the host, and provides Docker® and Kubernetes® protection at runtime.

-----------------

INNOVATIVE DATA INTELLIGENCE

Intel deployed  Splunk Enterprise Security that uses machine learning to detect threats so they can be quickly eliminated. The solution helped in reducing the detection of sophisticated threats to minutes or hours versus days or weeks.

--------------

QUANTUM SAFE NETWORKING

QuantLR used Ultimately Secure Quantum Key Distribution (QKD) solutions to secure data communication at a lower cost. QuantLR successfully deployed a POC showing that QKD is ready for mass deployment at a cost reduction of 90% over all previously deployed solutions.

--------------------

SECURITY IN MICRO-SEGMENTED DATACENTER

Nebraska-based cloud hosting and managed services provider FNTS partnered with Palo Alto Networks to align information security with a micro-segmented, software defined data center. Palo Alto Networks Security Operating Platform provided unified next-generation security capabilities and integrated global threat intelligence to automate cyber protections across the network, endpoints, and public cloud environments.


----------------
SECURITY POLICY AUTOMATION

Global IT Services PSF selected Palo Alto Networks to automate security policy provisioning and management for private cloud-hosted tenant environments. The solution from Palo Alto Networks provided granular visibility, intelligent control and advanced threat protection for applications and data residing within VMware NSX-powered multi-tenant hosted software-defined data center.

----------------

AUGMENTED INTELLIGENCE

Airbus cybersecurity selected ThreatQ platform from ThreatQuotient to enhance its threat intelligence service. The ThreatQ platform is complementary to an existing managed security solution that allows customer to build up their own knowledge base adapted with their context.


----------------
AI SOCIAL ERP

Orenda Software Solutions: migrated its social media measurement solution to an IBM Cloud hosting environment, using the IBM Cloud development platform to incorporate the IBM Watson Personality Insights service. This enables Orenda’s social media monitoring platform and to provide cognitive capabilities to social data.

-------

Workplace engagement

GoDaddy, the American Internet domain registrar and web hosting company, uses Jive-powered Interactive Intranet (known as ‘The Planet’) to engage it’s 5,000 global employees in a ‘single marketplace of ideas’. On The Planet, GoDaddy’s employees can find the latest company news, find HR resources, get to know new colleagues, recognize each other’s accomplishments and take part in groups and discussions on an array of topics. Employee engagement increased by 81% in the company’s customer care group, within months of launching in July 2015.

----------------
Microsoft Office 365

X was looking to improve the efficiency of its IT services, without adding extra costs. X had been using a suite of on-premises services that included Microsoft Office and Office Communicator, but came to realize that running services on-premises was an expensive option. Avanade and X, in under 12 months, completed one of the biggest ever deployments of Microsoft Office 365, involving more than 290,000 users and 11,000 sites across 54 countries. 13,000 Microsoft SharePoint Online sites were created, 130 TBs of data moved with users experiencing additional functionality promptly.

--------------
IBM Connections

Cloudhop, the managed service provider, develops and delivers solutions that help organizations across Africa and the Middle East enhance productivity, reduce costs and uncover valuable insights in their data. Cloudhop wanted to offer social collaboration tools that could scale to support multinational organizations, in order to attract larger clients, and keep them. Using IBM Connections Cloud and IBM Watson Workspace, integrated and security-rich capabilities are delivered, supporting teams of 10 to 10,000.


---------------

Asana & Form Integration

Sophos, the British security software and hardware company, has over 3,000 employees across 119 countries, with an extremely large and interconnected marketing team. The marketing organization is split into various sub-teams, e.g. product marketing, field marketing, marketing operations, corporate marketing. In order to improve communication, which was often slow and unreliable, Sophos worked with Asana to replace the existing email alias with integration with Google Forms which would route any requests to the correct project, creating a ‘welcome desk’ of sorts. Analysis of the data in Asana also helped to optimize the marketing organization, by assessing and managing the volume of requests and spotting any duplicate work.


------------------
Cisco Webex Meeting Solutions

IBM Norway was going through a significant amount of transformation to become an integrated service and solutions company focused on data and AI. The process involved integration of companies and technologies, the quick onboarding of more than 1,000 employees while enabling and accelerating collaboration. Cisco Webex was used to transform internal and external interactions and create flexibility to bring teams together via completely secured solutions.

----------
Career Management & Coaching

Cisco, the US multinational technology firm, co-developed the Ascendify Aspire app, a virtual career coach that learns about an employee’s skills, passions and interests in order to best help the employee navigate their career. Cisco’s ‘My Development Space’, which uses Ascendify’s intelligent technologies, allows employees to ‘choose their own adventure’ by finding challenging work within Cisco and searching for career advancement across the company.


-------------

Customer Review Analysis

Sage, a leading global supplier of business management software and services, was looking to improve customer loyalty, strengthen customer care and monitor and measure key strategic metrics such as Net Promoter Score (NPS). The business realized that there was a wealth of valuable information in the free-form customer comments section, or unstructured data, but there were too many comments to read and analyse one by one. Sage used text analytics to automate the process of unstructured survey analysis and better understand customer feedback.


-----------

Marketing Campaign Effectiveness

Citrix is a leader in mobile workspaces, providing virtualization, networking and cloud services. Citrix worked with Informatica to increase lead-to-opportunity conversion rates by 20%, by improving the quality of channel partner, enterprise and SMB prospect data, and providing a view into the total customer relationship across product lines. The business also achieved a 50% increase in quality of data at the point of entry, and a 50% reduction in the rate of junk and duplicate data for prospects, existing accounts and contacts.


-----------------

Subscription-Based Licensing Models

Autodesk is a leader in 3D design, engineering and entertainment software. The company decided to transform its revenue model from a conventional perpetual licensing one to a more modern subscription-based licensing model, to increase profits and propel growth. Autodesk’s existing BI system could not support a critical change to the revenue model. Using the Denodo Platform as a logical data warehouse, Autodesk was able to create a single, unified enterprise access point for any data used within the company. This enabled external partners to directly connect to Autodesk SAP applications using web services, as well as a reduction in data copying (less ETL) and a single point of enforcement for security.


----------------
INFRASTRUCTURE AUTOMATION FOR SOFTWARE OPERATIONS

Using AWS Lambda: Autodesk uses AWS Lambda for many purposes and reported use cases related to operation automation of its AWS environments used to serve its customers and ecosystem. Autodesk developed a microservices framework of over 20 functions that automates the onboarding and offboarding of AWS accounts; thereby turning what was once a two-week process into just 10 minutes.


------------
EASING INTEGRATION OF MULTIPLE EDGE DEVICES

Using Microsoft Azure Functions: Plexure, a provider of CRM platform, has a challenge in providing an easy integration mechanism with a retailer's existing digital ecosystem. Microsoft Azure Functions have allowed Plexure's customers to create "code as content" (functions) which the Plexure platform triggers at appropriate points to effect interactions with other digital systems.

------------
IOT CONNECTIVITY FOR INDUSTRY 4.0.

Orange Business Services and Microsoft are collaborating to deliver large-scale, end-to-end Internet of Things (IoT) solutions that boost the digital processes of companies in the manufacturing sector. Enterprises can use the Orange modular IoT solution, Datavenue, strengthened by Microsoft Azure IoT Suite, to transition to Industry 4.0 and optimize the entire manufacturing value chain.

----------

Continuous Deployment at Amelco

Amelco Leverage Ansible’s agentless automation framework to simplify the process of deploying, operating and upgrading applications – across disparate environments, in one simple language. Using Ansible and Ansible Tower to automate application deployment, Amelco is able to reduce complexity, speed solution delivery and ensure continuous delivery.

----------
Obeta uses covarients dynamic picking arms

Covariant is tackling the issue of picking any size package with relatively simple hardware, including an off-the-shelf industrial arm , a suction gripper and a 2D camera system that doesn’t rely on lasers or pattern projection. What makes the system work is a neural network that identifies the object to be picked up. 
The system is deployed in a warehouse that handles logistics for a Obeta, which overnights orders of electrical supplies to electricians in Germany

----------------

Fedex Memphis accelerates robot arm deployment

FedEx installed a quartet of robotic arms from Yaskawa America with the express intent of helping sort the massive numbers of parcels that pass through its Memphis facility. With the advent of Covid19, a majority of its shipments are being routed through this facility to protect its workforce from contact-based infections. The company says several members of its Small Package Sort System team — who previously did the sorting themselves — are operating as supervisors for the new robotic employees.


-------------------

Roll Royce Swarm inspection

As part of its Intelligent Engine vision, Rolls-Royce demonstrated plans for both a robotic snake and swarm of cockroach-like miniature robots that, in theory, will work together to inspect the interior of aircraft engines without removing the entire engine
In partnership with Harvard University and the University of Nottingham, Rolls-Royce is working to build 10mm miniature, collaborative robots — called SWARM — that will be able to provide to the human operator a live video feed of an engine interior via small cameras

----------------
Samsung’s Digital Humanoids

Samsung Technology and Advanced Research (STAR) Labs’ has developed Neons, AI-powered beings with unique personalities and looks. These artificial humans are not designed to answer any questions like Alexa or Siri, but are supposed to show emotions, learn from experiences, and have real conversations. Each Neon is computer-generated and not necessisarly based on real people, and each can be customized for a different role, like a virtual doctor or a yoga instructor.


--------------
Agility Robotics and Ford team up for humanoid delivery
 
The robot, called Digit, has arms and legs and has been designed to work with humans and in human spaces. The team expects that, as Digit will be part of a package delivery service, this communication channel will also provide delivery specific information, such as where a customer prefers packages to be left, or other individual package delivery needs


--------------

CONNECTED FITNESS

Under Armour can scale its Connected Fitness apps to meet the demands of more than 180 million global users, innovate and deliver new products and features more quickly, and expand internationally by taking advantage of the reliability and high availability of AWS. The company is a global leader in performance footwear, apparel, and equipment. Under Armour runs its growing Connected Fitness app platform on the AWS Cloud.


----------------
Employee-driven innovation

Nestlé, the Swiss food and drink company, launched an internal crowdsourcing campaign to meet the future challenges faced by its global supply chain. Ingenius is an ‘accelerator program within Nestlé, focusing on employee driven innovation’ and provides a forum for employees to collaborate and grow ideas into tangible business opportunities. More than 43,000 Nestlé employees from 110 countries have created 2,300 new ideas to-date.


-------------

Interactive marketing campaigns

PepsiCo, the American multinational food and drinks company, launched the ‘Do US a Flavor’ campaign in response to declining sales amongst younger demographics. PepsiCo asked customers for their view on new potato chip flavors for their Lay’s brand (Walker’s in the UK). Over 14 million submissions were received and ‘Cheesy Garlic Bread’ was selected as the winner. An 8% increase in sales was recorded in the three months following the launch of the new flavor.


------------

Sustainability initiatives

Unilever, the British-Dutch consumer goods company, launched The Unilever Foundry in order to reduce the company’s environmental impact and improve the health, well-being and livelihoods of people across the globe. The Foundry allows start-ups and innovators to submit ideas which, if successful following a review stage, are pitched in-person to an expert panel. Successful entrants co-create pilots with Unilever. One example is Mobile Movies, launched by Unilever Foundry and Singapore start-up Next Billion. Mobile Movies is an initiative that is driven by networks of local agents who organize free film screenings to communities in rural areas, helping to empower informed attitudes to hygiene. 


--------------
On-demand Talent Marketplaces

Procter & Gamble has experimented with on-demand talent marketplaces. During a recent pilot program, the multinational consumer goods organization began augmenting its current workforce with freelance business talent, using Upwork enterprise. Products from the pilot program were delivered faster and a lower cost than conventional methods 60% of the time. Labor platforms like Upwork allow for teams of internal employees and freelancers with the right skills to be quickly assembled to complete projects and then dispersed when the job is done.


------------
Product design

LEGO, through the LEGO Ideas platform, has created an online community of almost one million members and uses crowdsourcing to select new product ideas and give fans an outlet to share their creations. Users can also vote and provide feedback on ideas submitted by others. Ideas with over 10,000 votes are reviewed by LEGO and if selected, the successful candidate can work with the LEGO team on the product and also receive royalties on any sales.

------------

Web and Mobile Application provider

Web and Mobile Application provider used IBM’s Security AppScan Source and Security AppScan Enterprise to do security testing and in the development cycle. Also, it combined static application security testing with traditional dynamic application security testing to detect and fix vulnerabilities in its web and mobile applications.

--------------
ShopDirect Monitor and detect threats

ShopDirect used Vectra Network’s Cognito – an AI-based automated threat detection solution to automate threat detection and response. Cognito automates the labor intensive work of threat hunting and shows where attackers are hiding and what they are doing thereby making threat hunting more efficient. Threats are also scored and correlated with compromised hosts and specific threat behaviors are correlated across hosts to provide the SOC team with a narrative of developing attacks.


------------
Luxury Retailer

Luxury Retailer replaced its existing SIEM solution with Splunk’s SIEM (Splunk Enterprise and Enterprise Security) and got added capabilities to prevent security breaches, mitigate fraud, ensure Payment Card Industry (PCI) compliance and the ability to protect customer data.


-------------
Booktopia

Booktopia used Forter’s Fraud prevention solution to help the company identify suspicious transactions which were undercutting the efficacy of the checkout process. The solution helped in improving the decline rate and at the same time enhanced the fraud review with real time automation.

---------

Protect IT, data, endpoints and compliance 

Cahartt used Trend Micro’s Smart Protection for Endpoints, to reduce the number of threats coming into its environment through various endpoints. Following the implementation of the solution, the company was able to stop 20 to 25 malware attacks over six months, and between 3,000 and 4,000 web reputation events.

--------------

Continuous backup

TaylorMade used Code42 for automatic, continuous backup and remote restore capabilities along with protection from ransomware attacks. After moving to Code42, the company was able to get automatic, continuous and near real-time backup of all endpoint data and got protection when hit by ransomware as well. 


-------------


Critical data backup

Australian Paper adopted Veeam’s Backup & Replication. The company’s server state backups have the ability to recover or duplicate a working server from a snapshot. The incremental backups have allowed for a reduction in business disruption and provided the ability to create synthetic full backups from them


-------------

Handling

Bosch developed APAS – collaborative robot with tactile receptors to detect impact, security perimeter that reduces the speed of the robot in close human proximity and 2D-3D vision.

-------------
Home Security

EQL technology Qubi: With an animated face displayed on a small screen, this is a smart vacuum with a host of home security features. It has built-in Simultaneous Localization and Mapping (SLAM) technology to understand its environment and its own movements in the space and can send security notifications to the user.


------------
Sports Security

Sharp INTELLOS: is a multi-terrain, mobile sensor, data-gathering robot that can capture video, audio and environmental information, while providing a visible deterrent, without the aid of a human driver. It utilizes a navigation surveillance platform to patrol predefined routes. It can also act as a sentry and has a semi-autonomous mode for incident response.

----------
ASSISTANCE

Roombots is a novel self-reconfiguring modular robotic system developed by Biorobotics Laboratory. The autonomous modular robots can assembly into robot that can alter its shape to adapt to a given task and working environment, such as self-assembly and reconfiguration of static objects like furniture in the day-to-day environment.


--------------

Microsoft's cognitive services applied

McDonald’s is using cognitive technology to convert drive-through orders to text that can be fed directly into the outlet’s point-of-sale system. An undisclosed Microsoft partner was working with McDonald’s on the app, which takes advantage of Microsoft’s cognitive services.

--------------

Boeing plans using Cobot copilots 

Aurora Flight Sciences, a Boeing company, uses a UR3 arm as part of its Robotic Copilot concept. The goal is “to create a portable and extensible hardware and software toolbox introducing of new levels of automation across a wide variety of military and civilian aircraft that ultimately reduce crew requirements.”
The Aircrew Labor In-Cockpit Automation System, or ALIAS, combines sensors, a tablet interface for pilots, and flight-control and mission software. The Cobot enables it to interact with cockpit controls designed for human pilots.

---------------------

Nuro launches autonomous last-mile delivery services

Nuro allows millions of people to have groceries and other goods delivered by autonomous vehicles, potentially reducing traffic. Its driverless R1 pod is comprised of compartments that can be filled with goods for delivery to homes. Nuro is breaking new ground and building their AV from scratch. This US Department of Transportation (DOT) and the National Highway Traffic Safety Administration (NHTSA) approved a regulatory exemption for R2, Nuro’s second-generation vehicle

-------------------

EasyJet does aircraft maintenance using drones

Blue Bear Systems has developed a maintenance, repair and overhaul drone called RAPID (Remote Automated Plane Inspection & Dissemination) aircraft inspection system for Easyjet. RAPID can quickly acquire inspection data which can be analysed and the information passed on using an airline’s reporting systems. The system can reduce aircraft inspection times by up to 90%. This is particularly useful when aircrafts are grounded due to Covid19 and maintenance staff is limited.


--------------


Rolls Royce & Intel sail seas autonomously

Rolls-Royce partnered with Intel as part of the engineering firm’s mission to build fully autonomous ships. The company has been working on autonomous shipping technology since the early 2010s, and it said last year that it wants to launch its first uncrewed ocean-going vessels by 2025. As a first step toward that, it launched its Intelligent Awareness system for crewed vessels earlier this year.

-----------------
Globoconnect envisions ‘future fleet mgmt.’ with Eyesight tech

Israeli company Eyesight Technologies, will leverage AI and computer-vision technologies for a safe driving experience for fleet management conglomerate GloboConnect. The joint venture will focus efforts towards developing a monitoring device, tailored for the Oil & Gas industry’s fleets


----------

Tesla, Waymo fuse sensors

Tesla uses a combination of camera vision and radar to identify a scene while Waymo uses a close connection between lidar and camera data and  radar. Both different approaches but use a software-based 3d plane identification through sensor fusion. For now, Tesla and the industry’s unsolved challenge is still merging streams of “sensor fusion” data from into a coherent plan of action to identify intent like how a human would process the same traffic data

---------------
SMART PARKING

Operating from New Zealand, Australia, and the United Kingdom, Smart Parking provides end-to-end smart parking management/smart city solutions for cities and businesses globally. Using Google Cloud Platform, Smart Parking has become a data-intelligence solutions business provider running an IoT platform to seize opportunities in smart city developments.


------------

POSTAL ROUTE OPTIMIZATION

Applied to a system or process, digital twins can eliminate the need for physical experimentation while optimizing performance under different conditions. For example, X worked with Ireland’s An Post, a public postal service, to create a digital twin of its hundreds of vehicles, delivery routes, multiple sorting centers, and different processes to evaluate the impact of new technologies and test new approaches on throughput and timeliness.


-----------

ROUTE OPTIMIZATION AT AIRBUS

Airbus is already exploring the use of quantum computing in areas such as “route optimization and satellite imagery." Moreover, in January 2019 the aircraft giant announced a global competition to encourage developers to find ways quantum computing can be applied to “aircraft design”.


------------
FLEET OPTIMIZATION AT DENSO

Denso, the global automotive components manufacturer headquartered in Japan, is working on the optimization of the operations of a fleet of electrical delivery vehicles. Partnering with Toyota, the research uses vehicle location data from 130,000 commercial vehicles and cloud-based quantum systems to analyze the information and improve efficiency.

-------------
TRAFFIC OPTIMIZATION

The management and optimization of traffic (road, rail, air, etc.), the operation of vehicle fleets and the management of autonomous vehicles are promising fields where the inherent qualities of Quantum could well express their potential. Several companies are evaluating this around the optimization of filling of airline fleets in near real time and in a global way.

-----------------

Uber breaks a monolith

Uber decided to break its monolithic architecture into multiple codebases to form a microservice architecture. Introduced API Gateway through which all the drivers and passengers are connected. All the features were now scaled individually i.e. The interdependency between each and every feature was removed.


-------------

Trivago runs on PWA’s

Trivago is a big name in hotel search engine marketing. The availability of Trivago’s PWA is in 55 countries worldwide with 33 languages. After the adoption of PWA, the percentage of user engagement has increased by up to 50% and click rate has increased by 97%. 

----------
Intelligent Log Insights at British Airways

British Airways uses Logz.io, an AI-powered log analysis enterprise platform, to enable AI accelerated Devops using advanced log and enterprise data analysis. Logz.io is an intelligent log analytics platform that combines advanced machine learning with the powerful open-source ELK (Elasticsearch, Logstash, Kibana) stack and synthesizes machine data, user behavior, and community knowledge into actionable insights.

---------


Intelligent Log Insight at GrabTaxi

GrabTaxi ,the taxi-hailing app,  uses Scalyr to perform log analysis and intelligent alerts. Scalyr’s provides comprehensive log aggregation, high-speed searching, metric analysis, and full-spectrum alerting features. It searches the entire log surface from a cloud-based UI in a few seconds that earlier took minutes or hours. 

----------

LOG MANAGEMENT

Global hospitality company Hyatt, selected Splunk for a centralized solution to monitor and troubleshoot server issues and improve application delivery. Hyatt’s platform engineering team uses Splunk Enterprise for log management as well as analyzing and combining data from multiple sources. It also uses Splunk Enterprise to monitor changes in its own software updates, reservations and its booking engine. 

------------
AIRBNB DISRUPTION OF HOSPITALITY INDUSTRY

Founded in 2008 in San Francisco, California, AirBnB is a trusted community marketplace connecting people to global travel experiences at any price point. It recently launched Sonoma Select and Guidebooks
Sonoma Select looks to highlight rental properties whose owners want to provide a more hotel-like experience. Guidebooks seeks to attract users to AirBnB who are looking for business recommendations and reviews, taking aim at Yelp, Tripadvisor and Foursquare. Operations Transformations sees a mix of IoT and Blockchain, letting Airbnb provide a more seamless approach to booking, payments and access processes. AI & Deep learning along with data storage density provide more information about properties and popular areas.
The result is more profitable rental properties and more information along with a more tailored service for renters.


----------------
TELCO REDUCE CALLS FOR TRAVEL FEATURE

International/Global transactions were costing a Telco client $160m annually. They wanted to reduce cost due to care/retail assisted transactions, repeat call-in rate, post transaction credits, and transfer rates. 
With the client, set out to process Global and International Rebranding. Policy: Removal of Proration and Feature Expiration. The need was identified to completely revamp the global roaming system to make it customer-friendly.

------------

Aerodynamics

Companies like Airbus are researching smart materials that react to temperature to cool jet engines and wings that morph according to aerodynamic conditions to decrease air resistance. Briggs Automotive Company is developing a morphable wing for its supercar that adjusts to external weather conditions and automatically adjusts in order to provide maximum downforce to the car.

-------------

E-textiles in Travel and Safety

E-textiles have an important role to play in travel and safety industries. Lumos is a bicycle helmet with brake lights and turn signals, allowing cyclists to be more visible when on the road. The product uses 14 super-bright white LEDs at the front and 16 at the rear. The user also has access to a connected wearable with a built-in accelerometer to measure speed.


----------------
ROYAL CARIBBEAN DIGITAL NETWORK 

Commanding over 21% of the worldwide cruise market, Royal Caribbean is a huge operation comprising over 25 large vessels, and when it decided to enable a Cruise Ship Digital Experience with a Digital Network Platform it was always going to be a major undertaking.
X was able to help Royal Caribbean target Digital Network Architecture for two dry dock ship revitalizations in 2018, as well as preparing “Beyond”, a new ship launch with state of the art Digital Network in 2019.




-------------

Travel Portal at Deutsche Telekom

Deutsche Telekom wanted to create a portal (Tripdiscover) to offer potential customers personalized travel destination ideas and connect them with travel providers. The accelerated timeline and cost-effective delivery were possible due to X’s choice of Google’s App Engine, a Platform-as-a-Service (PaaS) ecosystem, and by leveraging X’s agile team structure and methodologies.


------------
HILTON DIGITAL ENTERPRISE STRATEGY

Hilton, the Hotel & Resort brand with over 570 hotels worldwide, wanted to become a data-driven organization that better understands the wants and needs of customers and anticipates opportunities. worked with Hilton to enact changes in the form of seven recommendations across media, loyalty program and customer satisfaction score, resulting in over $150m incremental revenue potential.

----------


Real time dynamic pricing for Leading Aviation company

Real time dynamic pricing allows to optimize price setting across large assortments by simulating the impact of new prices with live data (supply, demand etc.).
For a Leading Aviation company dynamic pricing was implemented via the application of a real time adjustment percentage to the fix fare. The price calculation system used different pricing processes in order to price the different product types.

-------------

Zoom Meeting Solutions

Skyscanner, the travel search site which allows users to find the best travel options for their trip, has over 900 employees across 10 offices globally. Connecting employees was a challenge, and the previous video solution had limitations on the number of attendees per call, poor video quality when more than 50 users joined and a bad Mac client experience. A solution from Zoom allowed for meetings with hundreds of video participants, consistently superior HD quality, and support for both Mac and Microsoft operating systems. Other key features included cloud recording which allowed for easy distribution onto the intranet, video breakout rooms which allowed for training sessions, seamless integration with Slack and fast ad-hoc calls with screen sharing for tech support.

----------------

Name Matching for Verified ID

Airbnb used Rosette Text Analytics software to connect online and offline worlds in order to provide users with even more tools to make informed decisions, using Verified ID which provides a connection between online and offline identities. Verified ID works by matching online identities (e.g. Airbnb reviews, LinkedIn, Facebook) to offline ID documentation (e.g. confirming personal information or scanning a photo ID). Name matching is a key component of this, and due to the international nature of the business, names can be in multiple languages, using different alphabets. Text Analytics software intelligently matches names based on algorithms that takes into account the origins and structures of names from different languages and cultures.


-----------------

Self-Service Reporting

Lufthansa Group, the global aviation group, used Tableau to gain 30% time savings in data preparation and analysis, increased autonomy through self-service reporting while gaining competitive advantages through real-time analysis. Before 2016, there was no uniform group reporting at Lufthansa and the individual effort for data preparation was very high so that comparatively little time remained for the real information analysis.

----------------

Route Optimization

Path analysis, a type of graph analytics, can be used to determine the shortest distance between two nodes in a graph. Route optimization can be used in logistics, supply and distribution chains, or traffic optimization for smart cities. It can help to ensure that orders are delivered on time, for example, save fuel, improve vehicle utilization and allow for better allocation of resources through automation of processes.


------------

Automatic Alert for Identifying Overdue Invoices

Apps target those challenges that are faced by multiple clients, 30+ apps today, across 8 flywheel offerings, targeting business outcomes for clients.  collaborated to create the apps Intelligent Collections, Payables Optimizer, Disputes Predictor and Provision Predictor for Finance and Accounting to optimize working capital. Intelligent Collections app provides descriptive analytics on accounts receivables and has the potential to increase in cash collection by 10 - 25%. Payables Optimizer app provides descriptive analytics on accounts payable and has the potential to reduce Travel and Expenses costs by 3-5%. Disputes Predictor uses binary logistic regression to analyze historical receivables and payments data and has the potential to reduce disputes by up to 10-20%. Provision Predictor app helps the CFO in giving better guidance on profit and loss numbers has the potential to improve provision prediction to 10-20%.

--------------------

Duplicate Payments Analytics

Finance & Accounting Analytics helps clients optimize working capital, improve cash collections & discount capture rates and reduce invoice dispute, across entire function. with the available apps like T&E Compliance Analytics utilized analytical models to automatically identify the non-compliance with travel and expense policies, infusing ethical behaviour and cost savings. And with the Payables Optimizer app prevented losses due to missed discounts and contractual penalties, identified and retrieved excess/duplicate payments from vendors, through right strategy. These eventually helped to reduce travel & expenses cost, improve compliance, improve discount capture by 10 - 20%, reduce duplicate payments by 15 -20%.

--------------

Pensa stocks shelves using drones

Pensa is a start-up that provides real-time inventory data based on a drone equipped with a AI vision camera that "learns once, recognizes everywhere." The drone flies inside the store scanning shelves for misplaced items and out-of-stock items as it compiles a complete audit of the store's inventory. Pensa's autonomous perception system uses proprietary AI-based deep learning to visually identify product items (SKUs) on the retail shelf and creates observable planograms or visual representations of products on the store shelf.

-----------------
Exoskeleton deliveries to reduce logistics injuries

A worker for food delivery service Eleme appeared on a Shanghai street on April 20 using robot developer ULS Robotics intelligent HEMS-GS lower extremity exoskeleton. He carried three take-out boxes on his back. The hashtag "robot exoskeleton for food delivery" was viewed more than 20 million times on Sina Weibo

----------------

Event Driven Enterprise Architecture at Nordstrom

Nordstrom remodeled it’s enterprise architecture to draw together existing models and data to manage risk and put together enterprise roadmaps. Their business and IT infrastructure included systems for Finance, Administration, CRM, Manufacturing, Merchandise Inventory, Sales and Supply Chain. They were able to document links between applications, servers, departments, capabilities and initiatives to carry out impact- and trade-off analysis.

------------
Digitalization of IT landscape at Reliance Industries 

Reliance Industries is India's largest private sector business enterprise and amongst the Fortune 500 companies. It’s businesses span 4G Digital Services, organized retail, and energy and petrochemical products. RIL has adopted a tailored TOGAF metamodel, and is using ABACUS (an enterprise architecture tool from Avolution) to co-ordinate its architecture and IT management.


-------------------



Zalando ramps up the next big thing in fashion: Microservices

Zalando’s microservice architecture leverages Amazon Web Services for provisioning, deploying with Docker and a self-built PaaS called STUPS
As a result of switching to microservices, Zalando created a flexible, scalable platform and a new working culture, which was attractive and motivating for developers.


-----------
Flipkart triples time on site

Flipkart decided to combine their web presence and native app into a PWA to allow their current and prospective customers to have instant access to their store. Compared to their previous mobile presence, Flipkart has tripled time-on-site with their Progressive Web App. Users spend an average of 3.5 minutes on Flipkart Lite versus 70 seconds on the predecessor. Flipkart has also generated a 40% higher re-engagement rate among first-time visitors and a 70% higher conversion rate due to the “Add to Homescreen” icon, because customers have instant access to Flipkart whenever they want.

--------------

AliExpress ships through PWA’s

AliExpress built a cross-browser Progressive Web App to serve its web users as well as mobile users. The statistics showed a huge improvement after their users started engaging through various features of PWA. 
The conversion rates from AliExpress to new users increased by 104%. This mobile web investment also resulted in conversion rates on Safari increasing by 82%. The new strategy also provided a much better experience. Users visit twice as many pages per session and the time spent per session has increased by an average of 74% on all browsers.

-------------
Walmart adopts cloud testing to enable CD

As part of an initiative to improve customer e-commerce experiences and developer productivity, the Walmart team adopted Continuous Testing best practices using Selenium and Test Armada.
Walmart also leveled up their testing by implementing a cloud-based solution for web and mobile app testing that provided access to hundreds of browser and operating system combinations, increasing test coverage and saving time.

-------------------

Application Portfolio at Home Depot

Home Depot has shifted from being a more traditional retail company to a truly digital organization. It has added more than 2,500 developers on Cloud Foundry, maintaining more than 1,600 apps and services in production. In any given month, that cloud handles more than 1.5 billion requests.


----------

FLEXIBLE IDENTITY MANAGEMENT

Whole Foods Market used Microsoft Azure Active Directory service to manage identity credentials in cloud environment. The company wanted to simplify identity management for employees for SaaS applications more effectively.

------------

NEUTRALIZE ACTIVE THREATS

A large retail organization used Attivo Networks’ ThreatDefend Deception Platform to determine active threats in networks acquired after M&A. The system detects confirmed interaction and engagement with decoys and traps deployed throughout the enterprise, and alerts on misconfigurations that may reflect risk exposure.





-----------

Mercari uses AutoML to improve their AI systems

Mercari is a popular shopping app in Japan that has been using AutoML Vision (Google’s AutoML solution) for classifying images. According to Mercari, they’ve been “developing their own ML model that suggests a brand name from 12 major brands in the photo uploading user interface.”
While their own model—trained on TensorFlow—achieved an accuracy of 75%, AutoML Vision in advanced mode with 50,000 training images achieved an accuracy of 91.3%, which is a whopping 15% increase. With such astounding results, Mercari has integrated AutoML into their systems.

----------------------



Tenth Street Hats using AR to enriching buying experience

Tenth Street Hats, a popular family-run hat store in the US, adopted an AR tool that allowed prospective customers to try out the hats from the comfort of their homes. It has been able to quickly scale up this feature from 1 hat to 32 hats. Since the implementation of this tool the retailer has increased its conversion rate by 52% and revenue per user has gone up by 41.8% for those who engage with the tool. Further, such customers were 2.2 times more likely to complete a purchase.

------------------

Smart Sports

Manufacturers like Sensoria produce a range of smart garments, such as upper garments that monitor heart rate or smart socks that detect cadence, foot landing or impact forces. These products connect directly to an app which displays the data alongside AI coaching. Ambiotex’s smart shirt contains integrated sensors that measure heart rate variability, anaerobic threshold, fitness/stress levels. The Athos Core collects and analyzes data using micro-sensors, enabling tracking of electrical activity produces by the wearers muscles.


-----------

Connected Clothing

E-textiles in clothing can provide increased convenience to the wearer, by making them more connected. Levi’s smart jacket, created in partnership with Google, has built-in sensors and tags to turn the clothing into the equivalent of the touchscreen on a smartphone. The interactive material is built into the jacket and wearers, particularly cyclists or similar, can benefit by accessing their mobile phone with out having to take it out of its pocket.


----------------
E-textiles in Travel and Safety

E-textiles have an important role to play in travel and safety industries. Lumos is a bicycle helmet with brake lights and turn signals, allowing cyclists to be more visible when on the road. The product uses 14 super-bright white LEDs at the front and 16 at the rear. The user also has access to a connected wearable with a built-in accelerometer to measure speed.


-----------------

Eco-friendly Fashion

Fashion brands such as Pauline van Dongen are producing clothing like the Wearable Solar T-Shirt. These products are not only eco-friendly as they are solar-powered, but the wearer can also charge electronic devices such as phones, using the garment. The smart fabrics used have tiny interconnected solar cells that recharge batteries embedded in the cloth – allowing the user to charge their devices ‘on the go.’

---------------------

Real-time recommendations for ASOS

ASOS uses Microsoft Azure Cosmos DB, a scalable, managed database that makes possible such innovations as real-time product recommendations and instant order updates for 15.4 million customers. 



---------------

FreshGo and Alibaba bring "New Retail" strategies to local HK fish markets

Alibaba Cloud provided FreshGo a refined solution to fulfill their need for a live streaming solution with scalable usage at a lower cost (-70%) which allowed them to give a better and more mature service to their customers

-------------------

Bitcoin payments

Online retailers and traditional retailers have begun to accept bitcoin payments. Interest has increased in the technology as it helps prevent identity theft, fraud and data tampering as well as helping to stop denial-of-service attacks. Retailers are also interested in the technology’s decentralized characteristics and its ability to minimize or eliminate exchange fees.

---------------------

Unmanned Aerial Vehicles

Amazon's drones will be able to pick up a package and track the location of the customer it is being delivered to by pulling data from their smartphone. They expect to deliver up to 400 million packages per year at a price of one dollar per delivery. 

----------------

Automation of pricing methods for American specialty retailer 

Analyze historical predictions and sales data to assess the effect of past promotions and dynamically adjust markdowns for future clearance periods
For an American specialty retailer a rule based pricing optimization process was  developed to help price management in the markdown period. The  markdown  solution identified poor performing Styles/ Color codes and recommends markdown percentages based on Weeks of Supply (WOS), elasticity, and user inputs.
A 4-step process was employed:

Identification of problematic items based on inventory on hand
Definition of discount % required to increase the sales with an objective of clearing off the remaining inventory
Application of business rules and constraints

This determined the final ticket price and action required based on the suggested ticket price and current ticket price.

----------------------

Customer segmentation based on offer optimization for Leading Brazilian retailer 

Offer Optimization allows to identify the most relevant offers / bundles that have the maximum likelihood of acceptance to the users
For a Leading Brazilian retail it was possible:

Predict price-volume relationship leveraging both internal data (e.g., stock level) and external data (e.g. competitors’ price) 
Use predicted price-volume curves as an input into a price optimization model (maximizing profit or revenue)
Facilitate effective change management & set up a trained COE
 
---------------------

Dynamic price markdown adjustments for Major Indian apparel retailer 

Analyzing historical predictions and sales data is possibile to assess the effect of past promotions and dynamically adjust markdowns for future clearance periods
A Major Indian apparel retailer built econometrics models to estimate price elasticity at segment level to estimate elasticity and offline simulation tool for price recommendation considering product hierarchy, elasticity and inventory on hand.

-----------------

On-demand Deliveries

Amazon Flex leverages drivers “on demand” to pick up packages at small warehouses and deliver them to customers.

---------------

Fraud Analytics

Huawei, the Chinese communications and technology solutions provider, leverages Big Data and analytics for live fraud analytics, analyzing financial transactions before they are authorized. The system, used in the case of retail transactions for example, is driven by Huawei’s FusionInsight platform and IMDBMS, and performs hundreds or thousands of queries per financial transaction, applying user-provided logic and rules to gigabytes of terrabytes of recent history. FusionInsight customers, the issuing retail banks in this case, are presented with an interface for inputing business rules, either manually generated, or derived from automated analysis and/or machine learning.

---------------

Social Network Analysis

Community analysis, or distance and density-based analysis, can be used to find groups of interacting people in a social network, for example, and perform analysis such as predicting network growth. Centrality analysis can be used to find the most influential people in a social network, or find the most highly accessed web pages, using the PageRank algorithm for example. Influencers can be used in retail to promote particular products, for example.


--------------

Marketing Campaign Optimization

Nielsen offers DaaS via the Nielsen Data Management Platform, providing clients with access to retail audience data across more than 60,000 segments. This allows organizations to customize audience targeting strategies for higher marketing impact, leveraging data such as demographics, psychographics, mobile, online, TV, spending, store visits, basket size and product purchases. The Nielsen Marketing cloud partner also gives clients access to a range of industry audience data – all segments can be activated across Nielsen Marketing Cloud’s 300 integrated media and marketing platforms.


---------------------



3D Printing

Custom Assembly At the Store: Retailers source and manufacture products centrally, selling via eCommerce channels and a network of physical stores. Retailers source centrally but final assembly is done locally to personalize products to customer preferences.
Personalized 3-D Printed Toys & Figurines: Consumers go to retail stores to buy toys and figurines available from a pre-selected assortment at the retailer’s stores. Consumers can get products made with customized features (such as names & images), with the aid of 3D facial scanner and/or  3D printers.

----------

Virtual Reality for retail 

Virtual Reality allows customers to view the fit of their apparel purchases virtually, thereby eliminating the hassle of returns. Today: Purchasing clothing via the eCommerce channel is often plagued by returns as customers are surprised by poor fitting clothing, resulting in high returns. Future w/ Virtual Reality: With Virtual Reality, shoppers can place clothing on a virtual-representation of themselves and evaluate proper fit, thereby reducing negative surprises and returns.

-------------

Virtual Reality Store

eBay and Australian retailer, Meyer, have created the world’s first virtual reality department store: a completely immersive shopping experience. Customers are empowered with convenience and experience.

--------------

DHL Pick & Pack

DHL uses Fetch Robotics ‘Fetch & Freight robots’: Two robots are required for large warehouse traversal, a mobile manipulator (Fetch) picks items from warehouse shelves, while Freight, a fast-mobile base, acts as an autonomous cargo delivery cart. The autonomous mobile robots have a loading capacity of 78 kilograms and can cover a distance of two meters per second.

-------------


 Carrefour Customer Service

Carrefour hypermarket uses Pepper Robot: The robot is programmed to chat and interact with customers, give direction and answer questions. The robot can also play music, light up, dance and take selfies with passers-by. Carrefour also has a photo booth service available, where customers can take a snap with Pepper and have it printed out.

-------------


Amazon Pick & Pack

Amazon Kiva: is a warehouse management robot in use since 2014. They are fast-moving robots that shuttle entire racks of inventory from a narrow aisle section of the fulfilment centre to a picking station. Amazon’s use of Kiva robots has reduced the “click to ship” time from 75mins to 15 minutes currently. It has expanded their use to upwards of 80,000 robots across 25 distribution centres.

----------


Delivery Robot

Domino’s Delivery Robot: Domino’s Robotic Unit (DRU) is testing its vehicles in Australia to deliver pizzas. The idea is to use GPS data Domino’s gathered over the years from its human drivers and combine that with an automated vehicle to make delivery rounds.

--------------


Cyptocurrency for retail
 
The Blockchain reduces transaction costs by eliminating intermediaries, saving millions of dollars in Retailer OpEx. Today: Retailers pay credit card companies up to 3% to facilitate and ensure customers can pay for their transactions. Tomorrow: Intermediary payment processors are no longer necessary as transactions are instantaneously captured and verified

--------------

Autonomous Vehicles for retail

Autonomous vehicles will automate the movement of goods between physical locations, including to the end consumer, throughout the supply chain. Autonomous vehicles will become mobile, on-demand transporters that deliver customers the products they need nearly instantaneously. Autonomous vehicles will automate the transport of goods across the supply chain network.


-----------------

Retail & Warehouse Robots

Robots performing operational tasks have a significant impact on reducing costs, while increasing efficiency. Robots scan and audit merchandise to maintain ideal product placement, and fill inventory. Robots analyze data collected from auditing to produce reports for misplaced and mispriced merchandise.

------------

DELIVERY

Pizza Hut and Toyota forged a global partnership to explore the autonomous pizza delivery vehicle - kiosk cruising the streets in the next few years.


-------

Delivery robots and Drones

Drone technology automates and reduces delivery costs and time. Retailers transport goods to local warehouses, and then leverage drones for last-mile delivery to the customer. Retailers use drones to move goods to local warehouse, and then leverage traditional transportation methods to move goods to the customers.

-------
PAAS - HOME LOAN APPLICATION AT UBANK

UBank launched new initiatives in an IBM Bluemix environment, including a virtual assistant that incorporates IBM Watson technology to support the bank’s online home loan application. The IBM Cloud solution offers the framework UBank needs to transform its operations for faster, more streamlined development and accelerated time to market for new products and features. 


----------------

A DevOps culture at Capital One 

Capital One shifted from waterfall to agile software development back in 2010. Now the company embraced also DevOps to cultivate an even more collaborative culture. Before, it used to be that developers' involvement with products mostly ended after delivery to operationsbut now that Capital One is using DevOps, developers feel even more ownership of these products and are empowered to get proactive about uptime, supportability, and monitoring. DevOps on the cloud is helping designers, developers, and engineers work together to make the customer experience better and better.


--------------

Smartlabs at Banco Santander 

Banco Santander Smartlabs are local deployment of design thinking + agile development teams structured in worklines of innovation. Santander Spain Smartlab is one of many new product development centres focused on rapid piecewise innovation. Too complex platforms are developed elsewhere where agile approach is adapted. The bank wants a factory of simple but valuable ideas come to reality. Agile team sizes are dynamic but often between 4 to 10 people depending on backlog complexity. Smartlabs main objective is to get value from APIs and transform their value into Open APIs use cases. In this regard, the Smartlab works closely with the Open Business Team at a local and global level.

--------------------

virtual assistant in the Bank Millennium mobile app

Bank Millennium chatbot is designed for all clients using the bank’s mobile app. This solution has big market potential as it meets demands posed by increasing number of mobile only customers that expect simple, instantly available and transparent services. Users find it easier to talk or write in natural language to the bot about an operation they would like to make than to fill the necessary form field by field. Chatbot Milla is unique on the scale of the Polish banking sector, which in addition to answering questions can accept and execute an order.


-------------------

BBVA API Market

BBVA API Market is the open and global API platform from BBVA. The place where everyone from startups to large companies can access a powerful set of financial tools and begin to innovate within their businesses, with the security and support of BBVA. Through the three lines of business: aggregate data, personal banking services and corporate banking services, with 8 APIs available, companies can enhance their business, developing new personalized commercial pathways and services that weren’t possible till now.
 
BBVA is already considered one of the leaders worldwide in open banking. A total of 2.400 registered users and over 100 users active (performing API calls on the last 30 days) and growing are beating all expectations of growth and activation.

-------------------

Intelligent Anomaly Detection Model 

The Central Bank of Egypt (industry regulator) assigned a new mandate. It stated that banks are the primary accountable entities for identifying suspicious customer transactional behavior indicating misuse of their personal accounts. In accordance, the Commercial International Bank had to come up with an innovative solution to address the aforementioned regulation. The intelligent anomaly detection model was developed using machine learning techniques making it the first AI-based model in the MENA region. Its development required a full collaboration of several teams on their lead; Consumer Banking, Data Science, and Compliance, merging industry expertise with intelligent analytical solutions. Based on the validation phase, the model showed an accuracy of around 80%, which is relatively high compared to best practice. 


------------------

Roboadvisors at CaixaBank

By using Roboadvisors, the objective of CaixaBank is to allow the contracting of managed portfolios by non-assessed customers via an online assessment service. Based on data on the profile, risk and objectives of the client, the bank can offer the automatic creation of investment portfolios at low cost.  So far, this kind of products where available just in the branch. Currently, there is no solution in the Spanish market such as Smartmoney, a RoboAdvisor for integrated use within online banking. A solution of this type is necessary to break barriers such as the definition of a 100% digital product, which is compliance. 

----------------------


Biometric Payment Card at RBS

RBS Group wanted to ensure their customers had the best payment journey. They selected Thales due to its advanced biometric payment card solution, proven by successful pilot programs in Italy, Cyprus and Lebanon. The RBS biometric payment card provided has fantastic green credentials compared with other solutions since it does not need a battery as it gets power directly from the payment terminal. 
 
These cards look like regular cards but have a sensor which can read the user’s fingerprint, which can be compared against a template stored on the card’s chip. If the fingerprint matches, the user is authenticated, and the transaction can proceed for authorization


--------------------------
Bank Polski is able to measure smile

Bank Polski is the largest physical branch network in Poland. The bank is always challenged with providing highest quality of service to their clients and a friendly work environment to consultants. Inspired with the use of the AI-backed emotion recognition solution in HoReCa sector the institution decided to implement it in banking. Thanks to the use of AI and emotional recognition they measure smile time of employees and their customers in real time, to improve customer service and employee satisfaction. So far, their Net Promoter Scores (NPS) has increased approx. by 20%.


-----------------

Rabobank Fundr

Rabobank Fundr® provides financing to SMEs in a fast, easy and convenient way by using new technologies and alternative data sources.
Its web front-end is enabled by an engine built up from the ground as a channel agnostic solution that can be easily embedded in 3rd party channels. The engine is designed as a suite of components and microservices. Features can therefore be changed without knock-on effects on the rest of the solution. This allows for fast and easy adaptions so that the bank can work in a continuous circle of improvement based on regular feedback from clients.


----------------


Citi’s cash management capabilities 

Citi provides integrated cash management and trade finance services to multinational corporations, financial institutions, and public sector organizations across the globe. With CitiConnect API, there is no need for additional providers or services. Global clients and partners can integrate Citi’s cash management capabilities directly into existing applications for a simple and intuitive experience for end users. This integration allows clients to include API calls directly into the application of choice: a Treasury Workstation, Enterprise Resource Planning (ERP) system, or multi-banking portal. Citi charges a monthly, lump sum fee for using its API gateway for production applications.


----------------------

Personalized Interaction at Intesa Sanpaolo

Intesa Sanpaolo is monetizing its transaction and demographic data by giving its affiliated business clients access to a customizable segmented audience (individual customers) reachable on bank’s channels, thus enabling an entire new personalized interaction between individual customers and business clients, as well as a whole new set of information available about customers such as spending behaviour and competitors market share.


-------------------
Blockchain to Bring Digital Payments to Rural Banks

Union Bank of the Philippines embarked on developing i2i, a blockchain-based platform that connects more than 90 rural banks and cooperatives to other financial institutions in the country. i2i stands for “island-to-island, institution-to-institution, and individual-to-individual” and is a core component of Send-i2i.Send-i2i enables rural banks serving far-flung communities cut off from international payment systems to receive cheaper, safer, and near–real time cross-border remittances from Singapore. 
 
Send-i2i makes this possible by leveraging blockchain-based technology and tokenised fiat. Send-i2i project is still in a relatively nascent stage. As it grows, it is expected to be a cost-effective way to increase the UnionBank’s market share of the remittance market, while delivering speed, security, and cost benefits of international money transfers to clients. Importantly, it enables rural banks in the Philippines to access the remittance network. 

--------------
AI for Customer Experience

A Turkish-based banking institution, IsBank, has partnered with Clinc, one of the leading firms in Conversational AI with a dedicated in- house Product Team, worked on developing Maxi’s unique capabilities throughout the year. Using the most advanced AI technologies, deep learning and NLP, Maxi has learned Turkish language from scratch. It was trained with more than 10 thousand Turkish sentences. Maxi lets customers communicate in their daily language and perform advanced PFM queries with enriched graphs and money transfer in one step. 


--------------------

Blue Prism's Digital Workforce for Mashreq

Mashreq selected Blue Prism's Digital Workforce to deliver greater operational efficiencies, higher accuracy and a massive reduction in processing time for new and existing customer services. The bank is using Blue Prism to integrate artificial intelligence and automate dozens of mission-critical processes across multiple business functions, including banking operations, compliance, customer care and their technology help desk.

-----------------

Automated document review

Using optical character recognition and robotics technology from IBM, HSBC’s Global Trade and Receivables Finance (GTRF) is automating the review of documents and sending them automatically to the bank’s transaction processing systems. HSBC is currently using the technology to analyze documents in English but it aims to adapt the system to read other languages, including Chinese, French and Spanish.


-------------------

ASSET TRADING

The Aidmatrix Foundation, a non-profit organisation decided to move its IT infrastructure to the cloud, to lower operating costs and increase agility. The joint X and Avanade team managed the design, development and migration of core applications to a Microsoft Azure platform. The transition achieved elastic scalability and accelerated time to market, enabled infrastructural cost savings and improved the client’s ability to store massive amounts of logistics data for analysis.


----------------------

Virtual Agents Sales

Using Rainbird’s AI software, MasterCard will extend the knowledge that was previously locked in a single team or among a few employees and make it accessible to the rest of its UK sales organization. This will help streamline sales processes and systems by applying standard best practices to manage leads and close deals with prospects. The virtual assistant will also possess the capacity to learn with time as more knowledge and data are added to the database.

------------------

Fraud Prevention

The organization implemented NICE Robotic Automation solution with desktop automation. The solution guides agents during their fraud investigations and interactions with credit card holders. Key gains included quality, predictability and speed. The company handles now 8,000 alerts per month with 99% accuracy in preventing fraudulent activity. The RPA solution reduced average handling time of the wrap-up phase by 82%. 

----------------------

Payments

The Monetary Authority of Singapore has developed Project Ubin, a PoC which focuses on the interbank payment system. The system allows banks to swap digital currencies or cryptocurrencies for cash, which is what the central bank aims to do in the future.


----------------------

Ethereum Trading

Recently, the firm is developing a private blockchain based on Ethereum. The project named Quorum plans to allow the bank to use a publicly available system for confidential transactions. Quorum is aiming to save costs and build faster and more stable systems.

----------------

Barclays – Agile Mobile App Factory

 implemented a Barclays Agile mobile App factory helping Barclays with Solution Design, iOS Development and Project Management. The commercial model is 100% capacity services to align with Agile approach, not fixed scope/price. The app factory was integrated with third-party suppliers for branding, design and legacy integration.


--------------------

Data Science

Capital One Financial Corp. is researching ways that machine-learning algorithms could explain the rationale behind their answers, which could have far-reaching impacts in guarding against potential ethical and regulatory breaches as the firm uses more artificial intelligence in banking.

----------------

Assessing applications with every sprint

Global Bank : Integrated Veracode Static Analysis and SCA into their software development lifecycle via build server and IDE integration, enabling them to go from assessing applications only twice a year with a legacy on premises SAST tool to assessing within each development sprint.


---------

Bank of America

Bank of America used IBM Trusteer’s online fraud protection software to offer its customers security and malware protection from financial malware and fraudulent websites by preventing them from stealing Online ID, passcode, sensitive information and tampering with transactions.

-----------------

BANKING & ACCOUNTING

Infosys has tied up with Axis Bank, ICICI Bank, IndusInd Bank, RBL Bank, Kotak Mahindra Bank, South Indian Bank and YES Bank to provide Blockchain with ERP solutions on Finacle. The range of functions include bill collection, Letters of Credit, opening accounts, customer-to-customer transactions, business-to-customer transactions, and invoice financing.

------------

Peer to Peer Payments

In 2016, Barclays partnered with the social payments app Circle. The start-up, which received a license from the FCA, allows users to send money to each other in messages, and supports bitcoin. Barclays provided Circle with an account to store sterling, as well as the payments network to transfer money. Further activity includes a signed agreement for a proof-of-concept with Safello, a Sweden-based bitcoin exchange.

-----------------

Car leasing with Bitcoin

The company built a new platform that allows customers to configure a car lease. Bitcoin transactions are used to create a digital fingerprint for each vehicle. Customers can interact with the platform inside a car. They choose car or insurance options and then pay for the lease. The secure ledger database registers the car’s identity and can be updated.


---------------



Utility Settlement Coin (USC)

Utility Settlement Coin (USC) project — led by some of the world’s largest banks (e.g. Banco Santander, BNY Mellon, Barclays, CIBC, Commerzbank, Credit Suisse) is intended to facilitate the issuance by central banks around the world of currencies using Blockchain technology. The USC platform will also make it easier for international banks to settle various transactions with each other, through the use of collateralized assets on a custom-built Blockchain. 
The collateralized digital tokens will be directly forwarded to the owner of the asset, rather than going through the traditional network of clearinghouses.


---------

R3 

R3 is an enterprise software firm focused on distributed database technology. It leads a consortium of 200+ members, such as financial institutions, banks, R3  developed Corda – an open-source distributed ledger platform, designed to work within finance . Corda has also garnered the interest of healthcare institutions, the insurance industry, and governments.
R3, often asked by the consortium members how blockchains can interoperate, partnered with Ethereum founder Vitalik Buterin to commission a paper on several current and proposed interoperability solutions. 
Additionally, R3 has developed a system that allows applications built on its platform to be able to interact seamlessly. While this resolves communication between distinct ecosystems, it focused on ecosystems built on R3’s platform.

-------------

Time saved at Deutsche Bank 

As leading multinational bank, Deutsche Bank turned to Red Hat to build a containerized, microservices-based application development platform. This improved efficiency and saved time and money, by giving on-demand compute to every application development team at the bank.

---------------

Automated Bill Payments

Bank of Montreal in cooperation with IBM have automated their customer’s bill payments by introducing machine learning-powered IBM Content Analyzer-based BMO Quick Pay tool – an AI-enabled automation solution which prepares the pre-authorized payment by processing e-mails or photos send by users.


------------


FINANCIAL CRIME DETECTION

APAC based banking group used advanced analytics driven fraud detection solution from BAE Systems to reduce both false positives and false negatives. The solution uses rule optimization and applied machine learning without interrupting existing live processes.

------------------

aPAAS at Barclays Bank

Barclays Bank: As part of Barclays’ cloud strategy, they are deploying Red Hat OpenShift Container Platform on Amazon Web Services. Barclays is believed to be one of the first banks to successfully implement an aPaaS and requisite operating model.

------------------

BBVA moves towards digital factories

The bank formed scrum teams in order to tackle new, high-impact business projects. It incorporated people from other departments (like the legal and compliance teams) into the growing number of scrum teams to create what BBVA calls its digital factories, groups dedicated to delivering projects that are high priority for the bank’s transformation .


----------------

SAFe at ANZ Bank

ANZ Bank has developed its own-brand of ‘scaled agile’ called New Ways of Working – distinct from the SAFe Scaled Agile Framework for large enterprises. The bank has 3000 agile trained resources currently and plans to train  13,000 employees by the end of the year. With new ways of working ANZ bank has gained the agility to release new features and functions monthly

-----------------

Automate at Fidelity

Fidelity Worldwide Investment adopted the DevOps approach using application deployment automation. Reduced release time from 2–3 days to 1–2 hours – all while improving quality. Leveraged IBM UrbanCode Deploy to orchestrate and automate the deployment of applications, middleware configurations and database changes into development, test and production environments.


---------------------
Collaboration at RCBC

The DevOps approach has helped shorten it’s development time from months to just days, across multiple platforms – mainframe to mobile. Through, ‘Collaborative development’ and ‘Continuous testing’ approach, RCBC enabled itself to deliver continuous software innovative by fostering collaboration between business, development and QA organizations. Also balanced quality and speed while reducing the cost of testing.


----------------

PORTFOLIO OPTIMIZATION AND RISK ANALYSIS

Quantum computing can be applied for many financial problems, such as portfolio optimization, finding arbitrage opportunities, and perform credit scoring.


----------------

PORTFOLIO OPTIMIZATION AT MORGAN STANLEY

Morgan Stanley articulated the bank’s hope of speeding up portfolio optimizations, such as Monte Carlo simulations, with the help of quantum computing.


----------------
PORTFOLIO OPTIMIZATION IN BANKING

First movers are the Royal Bank of Scotland, Goldman Sachs, and Citigroup which have funded quantum computing startups directly. Barclays and JPMorgan Chase have been experimenting with IBM’s quantum computing technology and joined the IBM Q Network. 

--------------

ASSET OPTIMIZATION AT NATWEST

NatWest is using Fujitsu quantum-inspired digital annealer technology to solve some of its most complex, challenging and time-consuming financial investment problems by optimizing its mix of high-quality liquid assets including bonds, cash and government securities.

----------------
BBVA pursues quantum advantage for investment portfolio optimization

BBVA is examining various lines of research to determine how quantum technologies could represent an advantage over traditional tools in different financial use cases such as dynamic portfolio optimization, credit scoring process optimization and currency arbitrage optimization. The bank launched a strategy that involves forging alliances, strengthening its internal capacity around this technology, evaluating the available tools, and developing proof of concepts in collaboration with research centers, traditional businesses and startups alike. 


---------------------

Fault Detection & Recovery At CITI 

Citibank is working with Pivotal for digital transformation on Cloud Foundry. Pivotal Cloud Foundry’s application health manager automatically detects and recovers failed application instances when the actual state of an app instance does not match the desired state. The system is also designed to detect, alert and auto recover processes running the platform components, should a failure occur. In the event that the VM itself has failed, the system will automatically “resurrect” a VM and restart failed cluster components.

--------------

MXDP help banks with AI-driven experiences across channels

Temenos Quantum, the leading multiexperience development platform (MXDP), helps banks and corporations capture market share and improve client retention by delivering seamless, hyperpersonalized, AI-driven experiences faster across web, native apps, wearables, conversational and immersive touchpoints. Clients include ABN AMRO, HSBC, Bank of Shanghai amongst others.

-------------------

DYNAMIC CONTAINER SECURITY

A Global 50 financial services business used  Illumio’s Adaptive Security Platform® (ASP) for visibility and uniform policies across containerized and non-containerized environments. The security team visualized connections between their containers environment and data center, then built policies to permit only allowed traffic. 


--------------------

COGNITIVE SECURITY

Cargills Bank is using cognitive security solution, IBM® QRadar® Advisor with Watson™ to proactively secure its customers. The solution uses IBM Watson® AI capabilities to facilitate the rapid investigation and classification of potential security incidents.


--------------

EARLY WARNING SYSTEM

US bank selected Secureworks to deploy an effective early warning system, consisting of deep and wide insights to emerging threats across the world, put into an operational context.


-------------------
BEHAVIORAL BIOMETRICS

Banco Santander deployed IBM Trusteer Pinpoint Detect to protect its digital channels against fraudulent transactions and account takeover in addition to detection of end user devices infected with high risk malware. Using behavioral biometrics, the Trusteer solution builds a customer profile based on normal account activity, including location, device, mouse gestures, clicks, access patterns and more. It continuously authenticates the customer’s identity by monitoring for behavior that doesn’t fit the profile.


--------------

FASTER EVENT CORRELATION

Busan Bank  deployed RSA NetWitness® Logs to centrally manage and monitor logs from on-premises and cloud-based systems. The solution also provides short- and longer-term retention capabilities and applies advanced analytics to speed event correlation and attack detection.


-------------------

AUTOMATE ACCESS CERTIFICATIONS

Orrstown Bank deployed IdentityNow from SailPoint to  automate access certifications and password management for most of their primary applications. Orrstown has reduced the time it takes to certify access, automate their processes and mitigate the risk of a data breach.


-----------------

INTERACTIVE LEARNING AT ANZ

The Australia and New Zealand Banking Group (ANZ) wanted to develop an interactive learning module that made training more fun, engaging and challenging for staff. To make learning engaging and motivating the learning design team built the module around a space exploration narrative structuring the training as a “space mission” to identify Operational Risks in the “ANZ Risk Universe”.


---------------

Insurance processing automation in Jforce

JForce adopted the IBM Digital Business Automation suite of products to help mitigate the concerns felt by its insurance and banking clients. To address the claims and credit processing woes of its clients, JForce deployed the IBM Operational Decision Manager, IBM Business Automation Workflow and the IBM Blueworks Live products, automating about 95% of its processes. As a result of the implementation, the insurance company reduced its insurance claims processing period from 30 minutes to 5 minutes along with having its labor costs drastically drop down to 80%.


-----------------

Location Services

Bluetooth 5 has the potential for adoption in many scenarios, particularly where location accuracy and navigation is important. This could include more efficient airport navigation experiences, asset tracking of inventory in manufacturing and smart city infrastructure that could help those with impairments be more mobile. 
Bluetooth 5 can also play a role in Beacon technology, which allows businesses, in retail for example, to beam messages to nearby potential customers with deal offers or advertisements. Beacon technology can also facilitate better indoor navigation, or be used in online payments or banking, among many other things. Current examples of beacon technology include Apple’s iBeacon or Google’s Eddystone, which both use Bluetooth Low Energy communication technology. 


------------------



Car leasing with Bitcoin

The company built a new platform that allows customers to configure a car lease. Bitcoin transactions are used to create a digital fingerprint for each vehicle. Customers can interact with the platform inside a car. They choose car or insurance options and then pay for the lease. The secure ledger database registers the car’s identity and can be updated.


-----------------
Utility Settlement Coin (USC)

Utility Settlement Coin (USC) project — led by some of the world’s largest banks (e.g. Banco Santander, BNY Mellon, Barclays, CIBC, Commerzbank, Credit Suisse) is intended to facilitate the issuance by central banks around the world of currencies using Blockchain technology. The USC platform will also make it easier for international banks to settle various transactions with each other, through the use of collateralized assets on a custom-built Blockchain. 
The collateralized digital tokens will be directly forwarded to the owner of the asset, rather than going through the traditional network of clearinghouses.



-------------

PORTFOLIO OPTIMIZATION AND RISK ANALYSIS

Quantum computing can be applied for many financial problems, such as portfolio optimization, finding arbitrage opportunities, and perform credit scoring.

--------------------
PORTFOLIO OPTIMIZATION IN BANKING

First movers are the Royal Bank of Scotland, Goldman Sachs, and Citigroup which have funded quantum computing startups directly. Barclays and JPMorgan Chase have been experimenting with IBM’s quantum computing technology and joined the IBM Q Network. 

-------------------

REDUCE POLICY FRAUD

UK-based insurer Hastings Direct deployed fraud detection solution NetReveal from BAE Systems to detect, investigate and prevent claim and policy fraud. The insurer can access automated fraud risk scores during the claims process to flag suspicious claims for investigation, support the investigation, and fast track genuine claims through the process.

----------------
RPA at AXA 

The UK arm of insurance giant AXA has deployed 13 software bots across its business since last summer, aiming to help employees with mundane, repetitive admin tasks like filing customer correspondence in the claims department, saving 18,000 people hours, which equates to roughly £140,000, in productivity gains since.

------------------

Leading insurance provider automated underwriting system with DMS

The insurance company chosen FICO’s DMS to be implemented in its auto and home insurance business. At the moment, once the applicant information is entered, the insurer’s underwriting system instantly applies the relevant decision criteria (i.e. Models, rules, strategies) and external data (i.e. Loss history, vehicle reports) and renders a final decision with suggested pricing included. As a result, the company’s ratio dropped, underwriting’s efficiency grew, and its losses were reduced.


----------------
Insurance processing automation in Jforce

JForce adopted the IBM Digital Business Automation suite of products to help mitigate the concerns felt by its insurance and banking clients. To address the claims and credit processing woes of its clients, JForce deployed the IBM Operational Decision Manager, IBM Business Automation Workflow and the IBM Blueworks Live products, automating about 95% of its processes. As a result of the implementation, the insurance company reduced its insurance claims processing period from 30 minutes to 5 minutes along with having its labor costs drastically drop down to 80%.


-------------

Service quality improvements and  big savings for EP Wealth Advisors

EP Wealth Advisors, an investment management, financial planning and wealth advisory firm, used RingCentral to streamline internal communications across the organizations for an enhanced flexibility and annual 20% savings. 


---------------

Tokenized Real Estate Assets

Stan Group, a Hong Kong firm, is reported to have a real estate portfolio estimated at over HK$50 billion ($6.38 billion). The Stan Group is  seeking a locally unprecedented approval from Hong Kong’s Securities and Futures Commission for the regulated trading of blockchain tokens that would have real estate as their underlying assets.
The group reportedly signed a MOU with local security token insurance platform Liquefy to jointly explore the possibilities of real estate tokenization, which can enable investors to own and trade blockchain tokens that confer ownership of a property — in whole or in part


---------------

SANTANDER TRADE RECOMMENDATION ENGINE

The objective was to create a new Trade Club solution, giving Corporate Banking Clients a way to identify and originate trade opportunities with industry counterparts, while improving the effectiveness and productivity of Santander’s Relationship Directors and expanding competitiveness, growing revenue and primacy through market-leading innovative analytics capability.
The building blocks of a new analytics engine were established, and confidence was also built in using GCP for the long-term solution. An excitement was fuelled around the re-imagining of the Trade Club and discussions are underway about how to move forward with a PoC/MVP.


--------------
---------------

4D Printing

Four dimensional printing (4DP) is a technique where the materials are encoded with a dynamic capability — either function, confirmation or properties — that can change via the application of chemicals, electronics, particulates or nanomaterials. The printing technology has extra functionality to sequence, mix and place specific materials that will have a calculated effect.


--------------------

3DP PaaS

3D PaaS relates to the set up of an online, cloud-based 3D printing service for use in consumer and corporate settings with a focus on collaboration and cooperation. A “subscription-type” model is an alternative to purchasing one (or many) 3D digital printers. 


-------------------

IP Protection in 3D Printing

Intellectual property (IP) protection for 3D printing refers to practices and supporting technologies that protect creations from anyone attempting to illegally use or distribute that innovation through 3D printing. Such creations include inventions, literary and artistic works, designs and symbols, and names and images used in commerce.


-----------------

Macro & Nano 3DP

Macro 3D printing refers to systems using additive manufacturing to "print" physically large objects where the largest dimension is approximately two meters or more in magnitude. Nanoscale 3D printing is the use of specialized 3DP technology and related processes to create minute, intricate structures and/or features with high aspect ratios!

-----------------

3DP Software

3D print creation software refers to software that either creates geometry, which can be directly 3D printed, or converts other 3D content to 3D printed objects. In addition, today more sophisticated software like 3D printing workflow software enables organizations to manage the estimating, pricing, scheduling, capacity planning, costing, order entry and billing of 3D printed items. 


------------------

3DP Process Advancements

The numerous available 3D printing processes tend to be additive in nature with a few key differences in the technologies and the materials used. Some advanced methods melt or soften the material to produce the layers, while others cure liquid materials using different sophisticated technologies.


-------------------

3DP Material Advancements

Today, 3D printing is a technology used in architecture, medicine, aerospace, and other industries, and most of the parts used in those industries cannot be made just from ABS or PLA plastics. The development of advanced 3D printing material such as photopolymers, thermoplastic powders, thermoplastic filaments, metal powders, welding wire, plaster, sand, binder and bio materials, are key to keep up with the market demands.



------------------

3DP Scanners

A 3D scanner device captures the shape and appearance of real-world objects spanning products, body parts, buildings and other structures as "clouds of points" to create 3D models of them for use in activities that include design and 3D printing




------------

Open Banking

Open Banking is the practice of sharing financial information electronically, securely, and only under conditions that customers approve of. Open APIs allow third parties to access financial information efficiently, which promotes the development of new apps and services for the digital ecosystem.﻿ Ideally, open banking should result in a better experience for consumers.

-----------------

AI Robo-advisors are collaborative online financial tools that use AI algorithms to perform basic and more advanced investment management and financial planning functions. They can accept input from both the client and the advisor. Client access and features are set by the advisor, and the tools can be integrated with the advisor’s or wealth manager firm’s applications to provide a holistic view of individual clients.

------------

Biometric Authentication Methods use unique biological or behavioral traits to corroborate a person’s claim to an identity previously established to enable access to an electronic or digital asset. A biometric authentication method is typically used in one-to-one comparison mode (biometric verification), to support an identity claim. Biometric mobile banking authentication is the use of biometric technology to authenticate customers accessing mobile banking or payment applications on smartphones and tablets.


---------------

Payment data monetization refers to the ability to extract value from payment data (either raw payment data, composite data that also include customer details and preferences or transformed data making use of big data analytics and artificial intelligence) to obtain insights to improve customer services, decisions as well as product and business development.

-----------------

Waterfall is a mature software development methodology that tends to be among the less iterative and flexible. In a waterfall model, progress flows in largely one direction ("downwards" like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, deployment and maintenance.

-------------------
DEVOPS


DevOps encompasses activities that result in continuous delivery of value. Collaborative culture, product-centric mindset, agile practices and automation underpin the value stream.

Devops provides benefits such as faster delivery time, high collaboration between teams, early defect detection and continuous release and deployment. It is something that teams really need to get their hands around if they need to remain competitive. They need to scale up or skill up in order to make it because we’re certainly facing a faster-paced world as we move forward. 



With digitalization, there is a critical need to support businesses that must operate at higher speeds and with greater agility. This has resulted in agile and DevOps growing fast and becoming key to many organizations in their pursuit of competitive advantage. This shift to new methods of developing software is occurring rapidly, and many service providers have created offerings, but few are truly differentiating. Service providers able to create effective differentiators will be the winners in this space

According to the 2019 Gartner CIO Survey, top-performing organizations were more likely to employ agile infrastructure strategies including adoption of DevOps (54%), while 52% of respondents indicate that improving system reliability remains a key objective driving DevOps demand

DevOps continues its trajectory of awareness and adoption as firms focus on achieving product differentiation through the speed and quality promised by DevOps methods. However, the road to adoption has been neither steady nor effortless. Entrenched resistance will provoke more pronounced and far-reaching responses from organizations seeking DevOps benefits.

In 2020, as DevOps teams face an inflection point in reduced release speeds, firms that want to overcome this speed bump will need to adopt new technologies and a renewed customer-centric approach. 

--------------------

AGILE (XP,BDD)


Agile development spans beyond the software domain and into a more end-to-end business model and process development realm by employing principles such as adaptive planning or evolutionary development, heavily influenced by design and usability analysis, early delivery, and continuous improvement.

Companies are becoming increasingly comfortable with Agile practices. Adoption remains strong, and teams are getting better at knowing when and where to use Agile practices, but the techniques and understanding remain spotty at the enterprise level.

In Gartner Enterprise Agility 2019 survey 87% of respondents said they used agile for at least some of their application development. Scaled Agile Framework (SAFe) remains the most adopted framework by survey participants. 

85% of organizations have adopted, or plan to adopt, a product-centric application delivery model. Although full adoption is rare, overall, survey respondents use the product-centric model for 40% of their work. Gartner predicts that this figure will reach 80% by 2022.


---------------------

MASA- MESH APP & SERVICES ARCHITECTURES (INCL. MICROSERVICES)


MASA helps move enterprises away from monolithic architectures. It supports fundamental digital business requirements, such as rapid delivery of new features and capabilities; multichannel interfaces, and optimized continuous experiences for better user engagement; development of ecosystems and support for API economy interactions; IoT integration; and improved automated decisions by leveraging pertinent contextual information.

Many organizations have been evolving toward MASA and a more service-oriented model without realizing it in order to address multichannel requirements. If an application supports multiple clients from a single back end, it's on a path to MASA. As organizations get more comfortable with the model, they typically start decomposing the back end into coarse-grained services (macroservices and miniservices).

The growth of MASA is dependent on the evolution of its constituent parts, namely multiexperience, miniservices, mediated APIs and full life cycle API management. The further IT moves towards decentralized systems, the more the need for MASA. 

Today, applications need a different architectural approach to support digital business ecosystems. That approach is mesh app and service architecture (MASA). MASA is an architectural model that has emerged over the last five years as organizations have experimented and established new principles for building and delivering cloud-native, web-scale, modular and adaptive solutions that can be continuously and rapidly modified and refactored to support the dynamic and changing needs of a digital business.

Adoption of MASA encourages event driven IT, which is at the heart of modern innovation. This design model helps organizational systems to be more responsive and agile, scale independently, support real-time business decisions, and deliver digital business and IoT solutions.

To succeed in digital business, organizations are challenged to master event-driven thinking in design, architecture, technology, organization and culture. As per Gartner, by 2020, achieving broad competence in event-driven IT will be a top-three priority for the majority of global enterprise CIOs.

 
 

--------------------------

CLOUD NATIVE APPLICATIONS


Cloud-native applications are optimized to take advantage of the elasticity, self-service and software-defined nature of public or private cloud environments. They provide substantial benefits over traditional apps such as auto-scaling to handle continuous business needs and enable pay as you use billing, They are inherently resilient to failures and enable automatic provision of resources. In short, they facilitate on-demand, self-service, programmatic provisioning, and releasing of resources used for compute & storage services, thereby reducing costs significantly. 

New research indicates a significant change in enterprise cloud adoption through the increased use of cloud native applications - applications and services built to perform optimally in the cloud, leveraging Platform as a Service (PaaS).

Driven by recognition that cloud-native applications can enable IT to better contribute to business agility and innovation, Gartner predicts that by 2022, more than 75% of global organizations will be running containerized applications in production, which is a significant increase from fewer than 30% today. While, more than 50% of containerized workloads will span across hybrid environments, up from less than 20% today.

 
-----------------------

POLYGLOT PLATFORMS


Polyglot platforms provide a universal, language-agnostic interface across applications as switching costs on scaling, management, and monitoring infrastructure that accompanies a change of language for an app has been too high historically.

Polyglot platform flexibility has its advantages: it allows developers to combine the strengths of different languages, selecting the best of each to interact and work together in a single project. The trend of polyglot programming first rose in popularity around a decade ago when two popular languages, Java and .NET, first allowed for language interoperability

The number of languages, platforms and technologies has exploded in the last decade, which is set to rise multifold with the advent of containers and microservices. Platforms such as Oracle GraalVM have started consolidating languages such as Java, JS, Python etc. into a single VM core to simplify enterprise environment complexity.

Through 2021, Dev tools based on polyglot environments and multiplatform support will lead to an increase in language agnosticism and gravitation to a core set of three to five best-of-breed languages.

A polyglot Platform-as-a-Service (PaaS) development environment allows developers to write application modules in multiple languages with all modules interacting with each other. Each language leverages the skill sets developers as well as the strengths of the language itself. Cloud Foundry is a leading example of a highly customizable open source, polyglot, cloud PaaS. It allows developers to code in multiple languages and frameworks. 


--------------------
SELF ORCHESTRATING/ HEALING APPLICATIONS

Companies are tapping artificial intelligence to automate the care of their operations and information-technology infrastructure, since AI can identify and fix problems more quickly than humans.Self-healing platforms are capable of identifying and fixing operational issues automatically, before they cause full-scale systems failures. The technology monitors how systems normally function and flags any deviations from those patterns as potential errors. If the system identifies a certain problematic pattern, it automatically makes adjustments to restore normal operations.



Self Learning powered enterprise software will anticipate rote or foreseeable workplace tasks that need doing and simply auto-complete them or move several steps to finishing them by the user.

IT systems will evolve into self-organizing, self-healing and self-improving systems capable of capitalizing fully on the opportunities provided by big data and advanced analytics based on AI. Systems of intelligence will continually monitor, process and organize system behavior from pools of data, enabling optimized outcomes based on human-defined goals

As organizations move beyond AI augmented Devops, they will require always on AI to monitor all edge failure cases and remediate for swift MTTR with least or no user downtime and no human intervention, organizations seeking customer trust will need to invest into such systems to stand out for exceptional reliability and resilience. 

AI and machine learning constitute the next wave of IT business value by augmenting the human workforce to improve operations, simplify processes, and create capacity. IT leaders are using AI and ML capabilities to improve uptime and productivity. With self-healing platforms, we have reached an inflection point where machines can heal themselves, and humans can focus on higher level tasks.

In a IDC survey, about 50% IT workers reported that the most important thing an AI-enabled system would bring is improving the availability and performance of applications, which would help improve user experience.

--------------------

CLOUD TESTING PLATFORMS

Cloud-based testing can reduce the cost of management, as well as hardware, software and power. At the same time, it can be the crucial element that is needed to reach the goal of Continuous Delivery (CD). Hosted tools increase the ability to run more tests more frequently, which reduces production errors and system failures. Cloud-based software provides more flexible billing and capacity, which can be balanced against usage profiles.



Rapidly maturing cloud technologies are a key driver for cloud based testing platforms.
The demand created by mobile-first and bring-your-own-device programs, as well as omnichannel delivery initiatives, has created a strong demand for web, mobile web and mobile device testing options in the cloud as a form of platform.

By 2023, the automated cloud software testing market is expected to reach spending of US$1,276B from US$740B in 2019. An increase in complex sourcing and growth of complex development and deployment for cloud environments to drive some demand for automated cloud software quality solutions.

-------------------

AI AUGMENTED DEVOPS

The promise of AIOps platforms to provide rapid insight into large volumes of highly volatile data has not been fully achieved yet. The architecture and platform have improved, but the technology is still emerging and requires time and effort to get quality outcomes. Yet, Gartner anticipates that, over the next five years, wide-scope domain-agnostic AIOps platforms and narrow-scope domain-centric AIOps tools such as ITIM, APM or ITSM suites will become the main stays for delivering Devops functionality



AI can enhance Devops in several ways: It can suggest a better strategy, automate test designs, predict the quality of outcomes based on type and test coverage, increase coverage while reducing test design and execution, and automate visual UI testing to improve problem detection through better object recognition. Last but not least, AI can also reduce the time to fix bugs in production by supporting root-cause analysis.

AI will enable Developers to build new types of applications in a new way, Bots and intelligent agents will quickly take center stage in development; developers won't be programming them but rather teaching them.

By 2023, 30% of large enterprises will be using artificial intelligence for IT operations (AIOps) platforms and digital experience monitoring (DEM) technology exclusively to monitor the nonlegacy segments of their IT estates, up from 2% in 2018. Meanwhile 40% of DevOps teams will augment application and infrastructure monitoring tools with artificial intelligence for IT operations (AIOps) platform capabilities.

Gartner estimates the size of the AIOps platform market at between $300 million and $500 million per year. Artificial intelligence technologies such as machine learning have influenced the evolution of IT operations management intermittently over the past two decades, and AIOps platforms are the most recent example of that influence.

------------------------

AUGMENTED PROGRAMMING

Due to the forecasted shortage of skilled developers and desired reduction in application development timelines, developers will have to resort to augmented programming methodologies beyond basic RPA and code completion. Future developers will shift focus towards design and function while leaving the heavy lifting to AI workers to free precious human resource.

Augmented programming is the use of AI-enabled technology and services to assist in the construction and analysis of software solutions. It is a much-hyped nascent market with potential risks for early adopters and fast followers, yet provides significant gains if done right. Today, several augmented development innovations are emerging that show strong potential to disrupt modern application development practices. 

With the integration of ‘developer literate AI’ systems and the permanence of augmented interfaces across AR/VR etc. developers will be able to focus on design and logic while leaving the repetitive test and build phases to machines.

ML-augmented coding is likely to gain adoption as a rapid application development tool that supplements and extends enterprise investments in low-and RPA tools. Rather than reinvent the wheel with handcrafted code or repurposed code modules, future developers may simply check off program requirements in a high-level graphical user interface, and then, with a single click, autogenerate the predictively best-fit code-build into the target runtime environment.

Gartner predicts that by 2022, design-to-code technology will reduce the need for presentation layer developers by 50% and by 2024, low-code application development will be responsible for more than 65% of application development activity. These will be augmented by AI based integration since by 2023, more than 50% of new integration technologies evaluated will include AI among key criteria for simplifying integration.

Finally, it is expected that by 2023, AI will combine with human programmers to create “centaurs” performing 50% of traditional programmer workloads — doubling the throughput of stand-alone programmers.

------------------------

MULTI-EXPERIENCE DEVELOPMENT PLATFORMS

A multiexperience development platform (MXDP) offers development teams an integrated set of front-end development tools and backend services. It can accelerate app delivery by increasing developer productivity using high-productivity tooling, shared components and services, and common development languages and patterns.

Multiexperience is part of a long-term shift from computers as individual devices we use to a multidevice, multisensory and multilocation environment we experience. In near future, user experience will undergo a significant shift in how users experience the digital world and software providers will require MXDP’s to create a unified digital experience. 

As the underlying technologies for PWAs, conversational and immersive apps mature over the next two to five years. Custom mobile app development will remain the main driver of adoption of MXDPs over the next two years. The ability to create PWAs will also become an important criterion as MXDPs increasingly support a broader mobile user experience. In terms of immersive app development, AR use cases on iOS and Android to be the primary focus for MXDPs

MXDPs are not “build once, run everywhere” or “omnichannel” solutions. The core value of an MXDP lies in its ability to coalesce software development life cycle activities across a range of apps to address the digital user journey. The need for this ability will only increase as the number of apps, devices and modes of interaction increases.

Gartner predicts that by 2021, at least one-third of enterprises will have deployed a multiexperience development platform (MXDP) to support mobile, web, conversational and augmented reality development. And by 2023, more than 25% of mobile apps, progressive web apps and conversational apps will be built and/or run through a multi-experience development platform.

------------------

PROGRESSIVE WEB APPS & CROSS-PLATFORM FRAMEWORKS

Progressive web apps (PWAs) provide a seamless end to end user experience even with unreliable networks that improves the number of successful conversions. Moreover it helps engage users that have friction towards downloading enterprise apps for each use case,  leading to a significant increase in interaction for enterprise apps. PWA’s reduce the steps between discovery of an app and getting it on the home screen thereby eliminating friction of getting an app installed. 

Moreover, they provide near native functionality without the restrictions of native app stores and revenue share licensing overheads. 



PWA capabilities enabled by service workers and app manifests depend on browser support and user approval. This currently limits the full benefits of PWAs to browsers that support advanced capabilities and users who are willing to allow features like notifications and home screen icons

Requires additional skills: Implementing PWAs effectively requires an additional set of skills on top of the traditional web application development skills. E.g. skills such as service workers, app-controlled cache/synchronization strategies
Limited support in some javascript frameworks, MXDPs and WCM tools: Not all javascript frameworks, MXDPs and WCM tools support all PWA capabilities

PWAs bridge the gap between native and mobile web apps, and offer functionality that can support the digital commerce customer life cycle in terms of acquisition, conversion, engagement and retention while providing for CI/CD and other web app benefits. PWAs can leverage assets built for commerce websites and don’t have to conform to the various rules of iOS or Android app stores. Since they support a “develop once and deploy across platforms” approach, the development cost can be one order of magnitude less than that of native apps. 

Gartner predicts that by 2023, leading digital commerce platforms will offer PWA toolkits. Also, PWAs will be the standard manifest for mobile web apps as 90% of enterprises will use a combination of web, native and hybrid mobile architectures rather than a one-size-fits-all approach. 


-------------------------
CITIZEN DEVELOPERS

Business users increasingly want to generate their own IT solutions outside the IT organization. This provides an unprecedented opportunity for IT to partner with the business. To do so, IT must enable a comprehensive ecosystem that supports the citizen community. The ecosystem must support not only the technology to facilitate the creation of citizen solutions, but also the process and organization that support the fluid movement of solutions through the development process into production.

Today's rapidly changing business climate demands greater application agility. IT's timelines are often too long and costlier to meet business needs. Although agile development methodologies can help IT respond to business needs more rapidly, a lack of resources often prevents a prompt IT response. In this scenario it is faster, less expensive and better for end users to build the applications they need, with the help of citizen developers rather than engage the IT application development group.

The citizen developers help organizations to bring business and technology closer. As a result, organizations can focus more on core business and less on developing.

To create a healthy and secure organizational application development culture IT department must engage with end-user developers more actively to enable them to become "good citizen developers.“Gartner surveys show that on average, a total of 13 apps are built by citizen developers in an organization within a year. The most common type is web apps, reported by 79% of respondents. By 2023, the number of active citizen developers at large enterprises will be at least four times the number of professional developers

-----------------
------------------------------

E-TEXTILES

The e-textiles market has boomed in recent years. Research by Juniper Research suggests that more than 7 million ‘smart-clothing’ units will be shipped to consumers by 2020, increasing to 30 million by 2022 – a CAGR of over 100%. Value is predicted to reach $1 billion during the next two years.

There are many potential applications across a range of industries: Fashion and sports are front-runners, but as the technology improves (battery and electronics technology etc.), opportunities will emerge in healthcare, defense and engineering, among many others.

Use of renewable technology, such as solar, to power e-textiles products provide opportunities that are eco-friendly.
Incorporation of technologies such as GPS (for tracking) and NFC (for interaction with other devices) into smart fabrics is creating the potential for further market growth.

E-textiles technology continues to advance with developments in sensor miniaturization, battery technology and ways of integration with fabric. Today, the most common use cases can be found in fashion and retail, and in sports and fitness, but the list of industries will grow quickly to include healthcare and wellness, automotive, manufacturing, military and engineering among others.

As this improving technology approaches maturity, E-textiles brands and manufacturers should form collaborations to improve key aspects of smart fabrics, such as look, comfort, durability or battery recharging speed/convenience. Aesthetic improvements will clearly be important for growth in industries such as fashion – smart fabrics that can change color or appearance will also be attractive to consumers. Tangential applications such as solar-powered sailboat sails which increase efficiency, or GPS tracking technology which is directly embedded into clothing to track people or sensitive materials, are also ones to watch.

The e-textiles market is clearly an exciting and rapidly developing one, but most products remain in their infancy – in testing or early generations. A host of complex challenges – from supply chain and manufacturing difficulties to lack of components or high prices - are providing hurdles for companies to overcome.

----------------------

FLEXIBLE CIRCUITS

Flexible circuits, also known as flex electronics, is a technology for assembling electronic circuits by mounting electronic devices on flexible plastic substrates. The ability of flexible circuits to bend and contort into different shapes allows them to fit into places where rigid printed circuit boards cannot. Thinner and lighter than printed circuit boards (PCBs), flexible circuits are favored in applications where size and weight are important considerations.



According to market research published by Credence Research, the global market for flexible printed circuits was valued at US$16.6bn in 2018 and is expected to reach US$41bn by 2027, growing at a CAGR of 10.6% between 2019 and 2027.

The technology is already mature, having come into commercial use within the last few decades of the last century. In the last 10 to 15 years, the increase in demand for miniaturized electronic and industrial products favored the growth of flexible circuits, since they offer great flexibility to engineers while designing low-cost, high-quality and lightweight miniaturized products. They are also preferred in heat-sensitive electronics and products as they are better able to counteract thermal stress compared to rigid printed circuit boards (PCBs). Multiple benefits have led to the adoption of flexible circuits in a wide range of fields, such as communications, high tech, aerospace, defense and automotive. In addition, increasing penetration of connected and smart devices is expected to further boost demand for flexible circuits in the coming years. A 451 Research analysis of the IoT market indicates that total connected devices will number 13bn by 2024.

---------------------


3D PRINTED METALS

Metal 3D printing is suitable for complex, bespoke parts that are difficult or very costly to manufacture with traditional methods. Metal 3D printed parts have excellent mechanical properties and can be manufactured from a wide range of engineering materials, including metal superalloys.



Metal 3D printing (3DP) processes can be used to manufacture complex, bespoke parts with geometries that traditional manufacturing methods are unable to produce.
Metal 3DP parts can be topologically optimized to maximize their performance while minimizing their weight and the total number of components in an assembly.
Metal 3DP parts have excellent physical properties and the available material range includes materials that are difficult to process, such as metal superalloys.

The 3D printed (3DP) metals market is likely to mature slowly and, through 2020, will remain a niche technology relevant for one-off custom components and objects that cannot be created by conventional manufacturing techniques. Industries such as aerospace, construction and utilities that have such requirements should monitor the technology diligently.

Significant cost and time savings can be made, particularly with high strength materials, such as nickel or cobalt-chrome superalloys, that are very difficult to process with traditional manufacturing methods.

From 2020, as the number of providers increases and 3D metal printers’ throughput, quality and material range improves, industries such as event and exhibition producers, print service providers, retail and consumer goods should actively examine the technology’s use cases.

-----------------------

3D PRINTED ELECTRONICS

3D printed electronics may still seem far away in time, however the technology is poised to be a game-changer to the industry. While the technology is still primarily used as a prototyping tool, the benefits of 3D printing – from faster time-to-market, greater freedom of design and customization – can be leveraged by the electronics industry and have a significant impact on their supply chain.

In-house prototyping: In an industry as competitive as electronics, the demand for smaller, thinner devices electronics circuit with improved functionality is inevitable.
Faster time-to-market

Design flexibility: For example, multilayer circuits can now be 3D printed on non-flat, flexible surfaces, which would not be possible with traditional manufacturing techniques

Expands the capabilities of manufacturing customized electronics
Simplified supply chain

Design software will need to be developed that can define how electronic components can be printed within the part itself. Currently, such design software is still in its infancy, although this is expected to advance over the next few years.

Development of the materials suitable for 3D printing at nanoscale, as many electronic components are nanometer-sized.

Prototyping is still the most commonly used application of 3D printing within the electronics industry. Looking ahead, however, 3D printing could become a viable technique for producing wearable or embedded sensors for real-time health monitoring. Additionally, 3D printed sensors could be built into lenses to enable augmented reality applications, whilst 3D printed embedded electronics could be used in objects from smartphones to cars, adding functionality whilst making them lighter.

But before we see these applications gaining traction, there are several technical challenges that need to be tackled. Materials and design software will need to catch up to enable electronic manufacturers to 3D print parts with higher complexity and functionality. In spite of this, 3D printed electronics has all the chances to follow the same path as early adopters of 3D printing technology like automotive and aerospace. As the technology matures and new players enter the market, it is expected that 3D printed electronics could eventually shift from being solely a prototyping tool to direct, end production.

At present, 3D printed electronics remains at the ‘’improving” stage. It has established itself as a useful prototyping technology but has a long way to go before entering the mainstream.

-------------------




4D PRINTED MATERIALS

Rapid innovation has taken place in the field of printing technology, expanding in recent years to include smart materials. Pilots for 4D printing, although in its infancy, suggest many compelling case studies. It is estimated that the 4D printing market was worth almost $65m in 2019 according to GlobalData, and is expected to grow at a CAGR of 33.2% of the next 6 years, to $360m in 2025.

4D Printing refers to 3D printed materials that can change shape or properties in a predicable way over time based on external stimuli such as  exposure to water, air, heat, or an electric current. The concept of ‘shape-shifting’ was developed through collaborative efforts between academia and technology firms – this technology has many potential applications in industries such as automotive, aerospace & defense and medical. Dynamic and self-assembling materials are disrupting the way engineers think.

Characteristics
Ability to change shape/properties over time, based on external stimuli
Commonly uses programmable carbon fiber, wood grain or textiles (programmable carbon fiber has high stiffness, low weight, and tensile strength, so is beneficial for many industrial applications)

The growth of new technology at the intersection of smart materials and 3D printing is an exciting development. The 4D printing market is primed to grow thanks to scientific advancements (biology, chemistry, electronics etc.) and new printing techniques. Some key developments in recent years have caught the headlines: The Lawrence Livermore National Laboratory produced research on “field responsive mechanical metamaterials” (ferromagnetic particles injected into 3D lattice structures to demonstrate stiffening properties when exposed to magnetic fields); the City University of Hong Kong has developed technology for introducing shapeshifting capabilities to ceramic materials; researchers at Rutgers University have created “tuneable metamaterials” using shape-shifting polymers and 3D printing; DARPA’s Engineered Living Materials project is using 4D printing to develop “living biomaterials that combine the structural properties of traditional building materials with attributes of living systems”.

4D printed materials is an improving technology segment, but looking forwards, Defence will likely be a key growth driver from an industry perspective. Healthcare will also be home to many 4D printing use cases – it could be used for tissue engineering, self-assembly human-scale biomaterials, or nanorobots for chemotherapy. It is clear that these initiatives are in their infancy and not ready for widespread use, but the potential for this technology in the medical field is significant. 

Leaders within business, R&D, IT or with remits for product innovation should be exploring 4D printing opportunities today, and promoting learning opportunities internally. There are significant challenges when it comes to building 4D printing capabilities. R&D partnerships will help in the development of proofs of concept, while strategic partnerships will help when it comes to navigating a complex regulatory environment, to develop business cases and improve product specifications.



-----------------

E-TEXTILES FOR BIOMETRICS

The emergence of e-textiles that can track and analyse biometric information opens up a world of possibility for a whole host of new applications, particularly in sports and recreation, healthcare and in military-related functions.

Some examples of applications that are emerging in the military include E-textiles that could supply specific muscle compression to soldiers in response to specific biometric conditions, to ease pain or help to increase speed.

Products in sports and fitness industries could give athletes performance enhancing muscle vibration technology or even release topical painkillers or moisturizers.
In healthcare, E-textiles could release medication to patients in response to biometric conditions and play a role in detecting early warning signs of disease or medical emergency.

In fashion/recreation, we could see clothing that changes color or luminesces based on biometric data, or even releases perfume/cologne or other pheromones if bodily biometrics signal romantic attraction.

Opportunities aren’t limited to those that are clothing-related – in transportation, smart fabrics could be incorporated with car seats to monitor driver attention, health or other useful information.

Innovation in e-textiles and other technologies such as the Internet-of-Things is creating a flurry of opportunities for companies in the smart fabrics and biometrics space. Biometrics are a plentiful data source – physical stress levels, breathing patterns, calorie/energy burn rate and many more variables can be tracked and analysed. Psycho-emotional data, which can indicate emotions or predispositions, can also be measured.

There will be pressure on regulators to outline the scale of responsibilities for organizations in this area, and what the limits of the technology should be. Data security and privacy issues will be prevalent, particularly if companies opt to sell user data to marketing, insurance of healthcare providers. The measurement of emotions is an exciting new frontier for this market – be it providing the ability to share mood-related status on social media, or give customized retail recommendations based on predispositions when shopping – but a potentially controversial and risky one for companies to navigate. For these reasons, e-textiles for biometrics is an emerging technology segment.

There have been limited numbers of commercial successes to-date, due to the immaturity of the technology and a lack of confidence in the accuracy of measurements, particularly in industries such as healthcare where the consequences for incorrect results are large. Markets like sports and fitness have seen more activity, as there is less risk. E-textiles and biometrics companies will benefit from partnerships, with companies in fashion or retail for example, in order to achieve scale. If they can successfully gain consumer trust and demonstrate consistently accurate results, these companies will succeed in the new markets.

---------------

FLEXIBLE BATTERIES

Flexible batteries have the potential to achieve significant volume uplifts and weight savings compared to traditional batteries. They have longer operating lifetimes and quicker charging times, opening the door for wider adoption in products such as mobile devices, PCs, and the Internet-Of-Things.

The batteries market has been dynamic in recent years. New form-factors have emerged – flexible, ultra-thin and stretchable devices – while manufacturers are competing to offer new solutions, such as for electric vehicles or energy grid applications. There is a growing focus on flexible batteries due to the proliferation of Internet-of-Things devices, wearables, environmental sensors and other technologies which require features and designs that traditional batteries cannot provide – such as size and power. This continues to push global battery suppliers to innovate. That said, currently the technology is nascent and todays implementations are only achieving modest performance improvements.

Leaders in relevant industries should therefore be evaluating flexible battery technology and making system design changes in preparation where necessary. Cost is expected to be a key growth driver or inhibitor; flexible batteries will become more commercially competitive and productions volumes increase if competitive cost targets are met. The pace of innovation will of course be key for this emerging technology segment. Companies like Panasonic, LG Chem and 3M are working to develop silicon anode technology. Graphene could also improve the performance of such batteries; research at Lawrence Berkeley National Laboratory has shown potential gains in energy density using lithium sulphur graphene chemistry, for example.

5-10 years down the road, flexible batteries are set to play a key role in building a more green and more efficient energy supply, in addition to supporting jobs and industry growth in battery manufacturing. Balancing high performance with the minimizing of environmental impact will be the key to success.

-------------------





SWARM ROBOTICS

Swarm robotics, the concept that takes inspiration from bees and other social insects represents the next stage of robotic deployments and promises to revolutionize the applications of robots in unstructured environments through collective intelligence and sensing.

Multiple drones or mobile robots acting in concert are opening new applications for automation, covering larger areas, delivering more useful sensor data, and interacting with their environments in new ways. Unlike other planning algorithms that are centralized or rely on a single computer to handle delegating tasks, swarms are decentralized, i.e., each robot makes its own decision making the system resilient to failure.

For businesses, swarm robots are a building block for enabling Industrial Internet of Things, predictive analysis, and ultimately, more efficient operations.

Relatively simple individual rules can produce a large set of complex swarm’s behaviours. A key-component is the communication between the members of the group that build a system of constant feedback. The swarm behaviour involves constant change of individuals in cooperation with others, as well as the behaviour of the whole group.
 
The idea of using a set of devices — nanobots working inside the human body, microbots working inside machinery or standard-size robots working in mines or farms — appeals to the entertainment industry, the military, aerial applications and so forth.

The bionic aero vehicles inspired from swarm intelligence technology will become applicable in a few years. It can be foreseen that machine bees or cockroaches with reconnaissance equipment and bombs will possibly show up in military arsenal.

Much of the research done on swarm robotics is still in the laboratories. In 10 years from now analysts expect more and more swarm systems having an actual application. The swarm intelligence market is expected to be valued at USD 447.2 Million by 2030, growing at a CAGR of 40.47% from 2020 to 2030 driven by the increasing usage of swarm intelligence for solving big data problems, the rising adoption of swarm-based drones in the military, and need for swarm intelligence in the transportation business.

------------

HUMANOIDS

Humanoid robots, also known as legged robots, typically possess physical appearance like that of a human. For applications such as personal assistants/ elderly care, a human mimicking form or humanoid is preferable. Such a robot would replicate physical as well as emotional characteristics as emulated by a human worker under the same environment and stimuli to appear almost ‘human like’.



Humanoids find higher adoption and likeability for applications such as customer service and store counters in countries like US and Japan where technology adoption is favorable

Humanoid form factor increases the trust users have in their machine and may lead to higher adoption in sensitive applications such as healthcare/elderly care providers and personal home assistants

With increased Artificial General Intelligence (AGI) these systems will be highly autonomous and better at blending in with human workers as they learn human-robot interaction skills

There is an increasing demand for humanoid robots in the retail industry as personal assistants and for customer service. In healthcare, humanoids find applications in precision applications like patient care, surgery, medical training etc. In addition to this, the rapidly aging population as well as increasing number of rehabilitation centres drives the demand for humanoids to provide a helping hand to the elderly. Other drivers include rising spending power, high speed innovations and rising trend of autonomous rescue operations.

Apart from these, they will have significant military implementations -  combat soldiers will be replaced by robots to save human lives in combat and other high-risk operations such as firefighting, disaster rescue etc.

Key drivers for adoption of humanoids will be increasing mobile processing capabilities with quantum processors and improving artificial intelligence algorithms for humanoid robots to boost their performance.

Significant capital investments in research and development and high cost of humanoid robots are some of the factors expected to hinder the growth of the humanoid robot market in the short term

The market for humanoid robots is expected to reach US$5.5 billion by 2024 according to Global Market Insights


--------------------

AUTONOMOUS VEHICLES

A significant amount of investment has been made into the development of autonomous vehicle perception systems, with companies racing to develop a system that is deemed safe enough for human intervention free commercial deployment. Self-driving cars and trucks clearly have the potential to reduce climate-change emissions as well as make roads safer for occupants, Machine to Machine communication will enable better traffic optimization as well as overall better route planning and travel experience. The potential for autonomous vehicles opens commute as a time for productivity which people lose to traffic presently.

Autonomous systems of the future would provide for an engaging travel experience like what we get in railways presently without the last mile challenges. Companies such as Tesla which are collecting data through real world experimental vehicles may lead future passenger transport industry when autonomous vehicles may become the only transit option.

The main implications of self-driving vehicles will be in economic, business and societal dimensions. The interest of non-automotive companies highlights the opportunity to turn self-driving cars into mobile computing systems that offer an ideal platform for the consumption and creation of digital content, including location-based services, vehicle-centric information and communications technologies.

Continued advancements in sensing, positioning, imaging, guidance, mapping and communications technologies, combined with artificial intelligence (AI) algorithms and high-performance computing capabilities has accelerated interest in autonomous vehicles.

The technology is applied to wide use cases, including autonomous livery and autonomous trucks in transportation, unmanned vehicles in federal/central government and autonomous vehicles in the resource industry.

The adoption of autonomous vehicle technology will develop in three distinct phases — automated driver assistance, semi-autonomous and fully driverless vehicles. Each phase will require increasing levels of technical sophistication and reliability that rely less on human driving intervention.

Within Gartner’s  IoT Forecast Database, Level 3-and-above-capable autonomous driving net additions will rise from 137,000 in 2018 to 2.5 million in 2028 (a CAGR of 34%). Consumer vehicles will account for a majority of net additions, but commercial autonomous net additions will grow quickly from 2% in 2018 up to 13% by 2028.

---------------

SOCIAL & EMOTION RESPONSE ROBOTS

The recognition and understanding of human emotions is crucial for robotic systems to behave in appropriate ways according to the situation and smoothly integrate with all the different aspects of human life. In the new normal post the COVID-19 pandemic, robot deployment across service and assistance touch points will accelerate leading to renewed interest in making these systems as “human enough” as a normal human worker.

The idea of using emotion in the design of robots is not new, and the last three years have shown a great increase in the acceptability and usability of the concept of emotion, especially in designing software agents. The concept of Artificial Emotion is principally used in making robots respond emotionally to situations experienced in the world or to enhance interactions with humans

According to the WEF 's 2019 report on emerging technologies, the field of social robotics has reached a tipping point. For instance, there are about 15,000 Peppers (by Softbank Robotics) deployed worldwide that are used to perform services such as hotel check-ins, shopping assistance, and airport customer care. Moreover, social robots are gaining traction among in toys, as vendors continue to incorporate social behaviors into them. For instance, Sony sold around 11,000 units of its new AIBO robotic dog equipped with a sophisticated set of features that enable it to develop new behaviors based on past interactions

Gartner predicts that by 2022, personal devices will know more about an individual’s emotional state than their own family. While 5% of big-box retail stores will adopt camera-based or biosensor-based emotional recognition to understand moods of visitors, enhancing the retail experience and optimizing in-store digital display ads or real-time location-triggered ads

The Social Robots market is estimated to grow at a CAGR of about 14% over the forecast period 2020 to 2025, expanding from US 5.6 billion in 2018. Revenues are expected to grow to US$19 billion by the end of 2025, with over 65 million robots sold every year

---------------

EXOSKELETONS

Exoskeletons are best known for improving mobility among paraplegics. However, they are becoming increasingly useful for applications such as military, construction and industrial where supplementing human effort reduces time and cost.

It opens a new avenue of human + machine as next generations of exoskeletons can be integrated with smart Cobots for optimum efficiency. Advances in material science provides for use of flexible materials that can make exoskeletons easier to fit, and improve maneuverability leading towards true wearable like augmentation of human capability.

Over the past years, Exoskeletons have primarily been in demand for military, healthcare (both surgery and rehabilitation) and industrial workloads. As we progress in adoption of the technology the big bulky mechanized suits will reduce in size as well as complexity. Leading to higher adoption and faster setup.

In the next decade, exoskeletons will be able to identify wearers through implants or other forms of identification and download configuration and preference/operational data to optimize the experience. Future versions may be able to interact with implants for real-time assessment of the wearer, as well as with implanted/embedded fixtures for better alignment or connectivity to the wearer’s body.

Emerging work to interface prostheses directly with the neural network of the human wearing it will likely appear with exoskeletons as well but will require some advances in computing interfaces. Current actuator sensing based exoskeletons are not adept at understanding movement intent and often confuse the movement. Advances in brain computer interfaces such as Neuralink will be able to provide better intent information.

According to BIS research, Advanced prosthetics and exoskeletons market was valued at US$2.11 billion in 2018, and is expected to reach US$3.82 billion by 2024, registering a CAGR of 10.42% during the forecast period of 2019 to 2024.

-------------------

DRONES & UNMANNED AERIAL VEHICLES

Drone technology has been used by defense organizations and tech-savvy consumers for quite some time. However, the benefits of this technology extends well beyond just these sectors. With the rising accessibility of drones, enterprises are beginning to understand the true potential for drones, particularly UAV’s. Increasing work efficiency and productivity, decreasing workload and production costs, improving accuracy, refining service and customer relations, and resolving security issues on a vast scale are a few of the top uses drones offer industries globally.

Drones are especially attractive for autonomous inspections, used in sparsely populated environments where valuable assets are physically distributed and are costly or dangerous to inspect. This includes maintenance of solar and wind-power generation equipment, offshore rigs, oil and gas pipelines, power distribution lines, process manufacturing plants and agricultural and construction work sites.

Drones may replace human surveyors, inspectors, drivers and cameramen who previously had to perform costly jobs in unsafe conditions — they offer productivity improvements by reducing and/or redeploying head count, while improving real-time data capture and worker safety. The greater diagnostic capability of drones coupled with the increased availability/reliability of surveillance resources can reduce operating costs, missed opportunity costs and the risk of catastrophic events, while improving project management and resource allocation.

The market for drones and UAV’s is expected to expand at a CAGR of 13.8% to US$42.8 billion by 2025 from US$22.5 billion in 2020. The Energy sector is the largest industry on the commercial drone market in 2020 and will continue to be so in 2025, it is followed by Agriculture and Construction. However, the Transportation & Warehousing industry will be the fastest growing due to the changing consumer demands fueled by the global pandemic.


---------------------

SENSOR FUSION

Sensor fusion is a process that aggregates and "fuses" many disparate sensor inputs to increase sensor data accuracy and/or sensing coverage for the system to develop insights and decisions. A sensor fusion solution usually includes a set of sensors, a hardware sensor hub, a fusion engine and a software sensor fusion stack. It provides the ability to bring together inputs from multiple radars, lidars and cameras to form a single model or image of the environment around the robot.

The navigation system plays an important role and challenging competence for autonomous robots. In navigation application, a mobile robot must interpret its sensors data to extract environment information, with which the robot can determine its position. Since autonomous robots operate in an uncertain and constantly changing environment, a steady stream of rich and reliable information on the environment is needed for navigation and path planning.

To construct a reliable navigation system, sensor fusion is considered to improve the position estimation generated by differential encoder system.During the past few years, sensor fusion has evolved to include lidar, radar and visual sensing for autonomous cars, SLAM (simultaneous localization and mapping) for drones and robots, and six degrees of freedom (6DoF) visual and 3D audio immersion for (head mounted displays) HMDs. These advancements have enabled robots to better navigate unstructured environments.

Sensor Fusion has high adoption in the automotive and consumer electronics industry. It is used in a variety of automotive applications which includes advanced driver assistance systems, camera systems, measuring and controlling the speed of vehicles and safety systems for autonomous vehicles

As miniaturization of technology continues sensor fusion is required to reduce cost, system complexity and the number of components involved to increase accuracy and confidence of sensing

Sensor Fusion System market is expected to grow at a CAGR of 19.4% over the next five years to reach US$7.6 billion in 2024, from US$2.6 billion in 2019. Increase in demand for sensor fusion in automotive applications for safety features and consumer electronics and growing trend of miniaturization in electronics are expected to drive the growth of the sensor fusion market.

---------------
COMPUTER VISION

Data is viewed potentially as one of the most important and unique strategic business assets that organizations control. Computer vision has the potential to generate value from around 80% of dark data assets — including uneventful surveillance video, video meetings and unsearchable text and graphics. Key use cases today include the use of advanced analytics for video surveillance automation, health and safety compliance (PPE detection, COVID-19 mitigation, etc.), visual search, shopper and shelf analysis, automotive applications, OCR and quality assurance/production line automation in manufacturing. Increasingly, in the future, organizations that can value and leverage their computer vision assets strategically will gain strategic advantages to peers. 



Computer vision technology enables a robot to use a camera or scanner to transform multidimensional inputs into data it can process, “perceiving” its surroundings and mimicking sight. Computer vision coupled with machine learning and/or artificial intelligence gives the robot increased technical abilities and the opportunity to perform more complex tasks. This data can be communicated rapidly via 5G technologies to enable fast reactions to the surroundings and humans working close by


The world is marching towards an era where, instead of replacing workers, robots will assist by guiding workers through their workflow and completing repetitive tasks, taking away mundane travel time to give back to the worker for high-value work. Cooperative orchestration between automation systems and human workers through computer vision technologies will be critical to achieving the highest levels of productivity improvement in the future


The global machine vision market was valued at US$9.9 billion in 2019 and is projected to reach US$14.7 billion by 2025, growing at a CAGR of 6.5% between 2020 and 2025.
As computer vision matures , obots will be equipped with multiple tooling sets and have the capability for more general-purpose applications based on their environment.

--------------

MARINE ROBOTICS

Underwater robots are applied for continuous oceanic surveillance as they are designed to work in challenging environments where the safety and accessibility of divers is compromised. The major purpose for the use of underwater robotics is the necessity for continuous surveillance at submerged ocean areas.

Increasing undersea mineral exploration activity, applications of underwater robots identified in military application, increasing use of advanced robotics technology in oil & gas industry and growing investments in defense sectors across different countries are the factors fueling the marine robotics market

Marine robots are mostly deployed in extreme conditions and can be further categorized according to specific purposes. Their size, shape and payload depends on activity, for e.g. high-resolution cameras for ocean mapping and oil rig surveillance, heavy toolkit for undersea drilling and exploration etc.

Marine Robotics has grown from nascent navigation and control algorithms for underwater and surface vehicles, to powered autonomous underwater vehicles routinely able to dive beyond 6000 meters. We have seen underwater gliders cross the Atlantic Ocean and unmanned surface platforms (Wave Gliders) cross the Pacific as a proof of their capability
AUVs are providing many new lucrative opportunities. E.g. a new way of maintaining offshore power cables. Researchers are using ocean robots to make underwater cable surveys faster and cheaper. Other commercial uses include deep sea mining, asset location, debris assessment, site clearance and exploration

The global underwater robotics market is expected to grow from USD 2.30 Billion in 2017 to US$ 7.08 Billion by 2025 at a CAGR of 15.1% during the forecast period 2018-2025.

--------------------

COBOTS

Autonomous things cannot match the human brain’s breadth of intelligence and dynamic general-purpose learning. While there are successful use cases of Cobots working alongside humans, the more complex use cases that involve virtual machines will change the overall nature of human-machine collaboration. In turn, new human skills are needed to maintain, train and manage these autonomous things. The future will require human machine collaboration-based workspaces and Cobots are necessary to enable this. 

The development of Internet of Things (IoT) and technological enhancements have fuelled the adoption of automated technologies in industries. The high demand and use of smart devices, wireless, and cloud technology will influence the future of industrial robots like collaborative robots in terms of range of utility and mode of operation. With rising competition, vendors are offering attractive options such as services and control modules through smart devices and Wi-Fi technology for collaborative robots from remote locations. These devices are flexible and convenient and allow a single human operator to control multiple Cobots simultaneously.

Collaborative Robot (Cobot) Market is expected to grow from US$981 million in 2020 to USD 7.8 billion by 2026, at a CAGR of 41.8%. Collaborative robots are increasingly being adopted by various industries due to advantages such as increased productivity and effective employee utilization, as well as improved workplace safety. Smaller sized businesses are expected to quickly adopt collaborative robots, as many of these SMEs are yet to adopt robotic automation in their production processes. Cobots are being adopted by large manufacturing companies as well for increased cost savings and improving the ergonomic work conditions for workers.

Facilitation of automation, reduction of barriers in manufacturing, an introduction of advanced robotic technologies and attributes such as user friendliness and safety are some of the factors driving the global Cobots market.


-----------------

ASSEMBLY ROBOTS & ROBOTIC ARMS



The industry is focused on improving the business value for industrial automation. Robotic arms with integrated sensing and vision augmented by artificial intelligence and deep learning empower industrial robots to improve themselves and keep pace with the human workers around them. Future robots will even figure out their own environment and programming required through trial and error methods.

We’re already seeing companies like Fanuc working on robots that can teach themselves, so this aspect of the future is already becoming a reality. The need for increased automation will also be fueled by workforce dynamic changes accelerated by the Covid-19 outbreak. 

For applications with short product life where hard automation is costly to reconfigure, such as car components, robotic assembly lowers costs while boosting quality and capacity. Unlike dedicated automation equipment, robots are flexible, off-the-shelf machines that can be reconfigured or redeployed as needed. Perhaps of greatest importance, robots are a mature technology, making them a low-risk, high-return investment.

Industry 4.0 will play an increasingly important role in global manufacturing. As obstacles like system complexities and data incompatibility are overcome, manufacturers will integrate robots into factory-wide networks of machines and systems. Robot groups will operate on real-time data collected by sensors. This will fuel demand for cloud robotics in which robots will be connected to optimize work flow. The cloud network would allow these connected robots to perform the same activities while optimizing parameters of the robot’s movement such as speed, angle or force.

Global industrial robot market is expected to grow from US$ 48.7 billion in 2019 to US$ 75.6 billion by 2024, at a CAGR of 9.2%. Technological advancements and decreasing costs are making industrial robots more affordable to SMEs and are enabling seamless integration and programming. Shortage of labor and increasing manufacturing requirements is driving the need and acceptance for automation

----------------

-------------------

EDGE COMPUTING

Edge Computing has become important because of the increasing requirement that data-intensive processing work be done as close to the location of need as possible, without latency (waiting time).  This may require full-fledged processing and data storage capacity be available at the edge of an enterprise network, rather than at the core of the network or remotely (e.g., in the cloud).

With data-driven digital transformation occurring in all sectors, the need for Edge Computing capabilities is only expected to grow over the next several years. 

Edge computing has been touted as one of the lucrative, new markets made feasible by 5G networks. 5G requires a vast, new network of (ironically) wired, fiber optic connections to supply transmitters and base stations with instantaneous access to digital data (the backhaul).  As a result, an opportunity arises for a new class of computing
service providers to deploy multiple µDCs adjacent to radio access network (RAN) towers, perhaps next to, or sharing the same building with, telco base stations. 


These data centers could collectively offer cloud computing services to select customers at rates competitive with and features comparable to, hyperscale cloud providers such as Amazon, Microsoft Azure, and Google Cloud Platform.

Think of Edge in terms of opportunities at multiple edges/endpoints: Edge Computing is evolving in tiers with opportunities at hardware to software, data hubs, and applications such as smart cities, autonomous transportation, and video orchestration.

Focus on tooling to deliver and drive businesses of future: Software and hardware developers need new tools, frameworks, and support to build next generation edge apps, services, and deliver hyper-personalized experiences.

Edge will continue to be an ecosystem play: Participate in building standards along with global standards bodies and industry specific consortiums globally for development of technology and advocacy/advisory.

Partner with Silicon vendors  as they are championing Edge as next big opportunity after smartphones: Vendors like ARM are developing new architectures, solutions by embedding AL/ML  and are eyeing a bigger role in distributed computing and data centers.

Aim to build and/ or source requisite skills and talent: Patents, developers, technologists, architects and UX specialists. Follow multi-pronged strategy like internal training & development acquisition, hackathon & crowdsourcing and collaboration with academia and industry, to fulfil skills/ talent needs.

-------------

QUANTUM-INSPIRED COMPUTING

While fully-quantum solutions are still in the distant future, current use cases are offering a new and complementary option that leads to more unique solutions. Today, businesses should be preparing for quantum computing, while getting the most out of present classical computers and quantum-inspired algorithms.

Quantum-inspired computing accomplishes positive results, but is still limited in the type and size of problems it can address due to memory and processing speed limitations inherent in classical computing architectures.


Quantum-inspired computing has emerged trying to fill-up an existing gap between the theoretical advances in quantum computation and real quantum computers. Quantum-inspired algorithms use classical computers to simulate some physical phenomena such as superposition and entanglement to perform quantum computations. 

Characteristics
Quantum-inspired methods use parallelization and problem-reformulation in algorithms and hardware
Not fully-quantum; classical computing is augmented using quantum techniques.
Used to find the best option out of a range of possible solutions, with many use cases across industries.

Quantum-inspired algorithms will enable the quantum approach to computation to advance faster than pure quantum hardware and pure quantum algorithms are currently advancing
Quantum-inspired algorithms could bring innovative solutions and approaches to product development. It could also reduce time to market and optimize customer delivery.  
Solving optimization problems faster


The applicability of quantum algorithms to classical systems are limited by the short coherence time of noisy quantum hardware in the so-called Noisy Intermediate-Scale Quantum (NISQ) era and the difficulty in executing the input and output of classical data.

Other roadblocks toward practical implementation include limited number of qubits, limited connectivity between qubits, and large error correction overhead.
Experimental phase- Only a few use cases in proof of concept stage

CIOs should view quantum computing as a competitive advantage, as new quantum-inspired algorithms could bring innovative solutions and approaches to product development. It could also reduce time to market and optimize customer delivery.  

Additionally, waiting or ignoring quantum computing might place intellectual property (IP) and patent portfolios at risk. Early organizations will have the competitive advantage by patenting quantum inspired innovations within their specific domains. For example, a rival company could develop a quantum inspired innovation that improves Monte Carlo simulations by 1,000% or a pharmaceutical company could shorten the time to market for new drugs.

Gartner predicts that by 2023, 20% of organizations will be budgeting for quantum computing projects, compared to less than 1% today. CIOs should look for potential opportunities from quantum computing and be ready to help the business leverage them.

--------------

QUANTUM COMPUTING

Quantum computing uses quantum-mechanical phenomena to execute operations on data. A theoretical concept born in the early 1980s, quantum computing's first technological implementations were demonstrated 15 years later. Since then, the Quantum computer has not yet reached the mainstream, though tremendous advances have been emerging on both the hardware and application sides.



Large-scale Quantum computers are not expected before five to ten years. In the meantime, industry efforts are focused towards mid-scale Quantum processors, or accelerators, with sufficient computing power and reliability to provide a Quantum advantage on some difficult computing problems.

While science and industry have had their doubts about building Quantum processors for decades, the last three years have seen a giant leap forward thanks to impressive advances and huge investments from public actors as well as private IT giants like IBM, Google and Microsoft. 

Logistically, quantum computers are difficult to maintain and require specialized cryogenic environments. They also need calibration several times per day. For most organizations, this is neither feasible nor affordable - quantum hardware alone comes in at at least $10 million.

Enterprises interested in quantum computing should leverage quantum computing as a service (QCaaS) to minimize risk and contain costs. According to Gartner, by 2023, 95% of organizations researching quantum computing strategies will utilize QCaaS.

CIOs should start experimenting and looking for potential opportunities from quantum computing now, even though large-scale quantum computers are not expected in the short-term. They should also start looking for quantum experts. These opportunities will need to be fully integrated with traditional IT, and will require new cross-collaboration from research scientists, computational data scientists and quantum data scientists. 



-------------------

NEUROMORPHIC COMPUTING

Neuromorphic computing is not just a way to keep devices on the same increasing performance trajectory after Moore's Law expires but the brain-inspired architecture will also enable machines to support dynamic learning in the context of complex and unstructured data that silicon chips can't and, in addition, they will have much lower power consumption than conventional computation.

Neuromorphic Computing refers to computational paradigms inspired by the way human brain processes information and hence it is intended to mimic neuro-biological architectures similar to those of nervous systems of living beings to provide new ways of information representation, adaptation to change, fault-tolerance, and to incorporate learning and self-development capabilities to computation. At the architecture level, there are many flavors of neuromorphic system design. Their implementation varies in a large spectrum with the following two ends:

Customization of general-purpose CPU with data paths optimized for the execution of mathematical approximation of neural networks.

Direct digital or analog implementation of networks of relatively simple neural models.

Neuromorphic Computing Systems are dedicated to the analysis of large-scale data in search of complex patterns and/or in recognizing known/novel features.
Neuromorphic can address a large spectrum of applications including climate analysis, exploitation of physics experiments, search for novel features in genomes, protein structure prediction in pharma, etc. 


Machine Learning software is already implemented on classical hardware to tackle problems with complex and noisy datasets that cannot be solved with conventional 'non learning' algorithms. Considerable progress has been made recently in this area using parallel processors potentially enhanced with acceleration technologies. This implementation with its limitations, however, represents only an intermediate step towards neuromorphic computing.

For High-Performance Computing (HPC), the American Department of Energy (DoE) has set 2021 as the target date for first exascale systems with one designed on a novel architecture, and neuromorphic computing is a strong candidate to provide that new technology. Based on this objective, we can expect that by 2025 neuromorphic chips will integrate cyber-physical systems to play an important role in a large spectrum of industrial applications (transportation, medicine, manufacturing, space exploration, etc.) and will help to drive the growth of the Internet of Things (IoT).

--------------------

DNA STORAGE

The creation of digital information is rapidly outpacing conventional storage technologies. DNA may provide a timely technological leap due to its high storage density, longevity, and energy efficiency. In a generic DNA-based information storage system, digital information is encoded into a series of DNA sequences, synthesized as a pool of DNA strands, read by DNA sequencing, and decoded back into an electronically compatible form.

Recently, a growing number of universities and research institutions have focused on implementing and improving each of these four steps.

DNA data storage is defined as the process of encoding and decoding binary data to and from synthesized DNA strands. To store a binary digital file as DNA, the individual bits are converted from 1 and 0 to the letters A, C, G, and T - the four main compounds in DNA. To recover the data, the sequence A, C, G, and T representing the DNA molecule is decoded back into the original sequence of bits 1 and 0.

 
 
 Ability to store massive quantities of data in media having small physical volume. Using DNA Storage, all the digital information currently existing in the world could reside in four grams of synthesized DNA.
 
Longevity: Because DNA molecules can survive for thousands of years, a digital archive encoded in this form could be recovered by people for many generations to come. This longevity might resolve the troubling prospect of our digital age being lost to history because of the relative impermanence of optical, magnetic, and electronic media.

Scientists have long touted DNA’s potential as an ideal storage medium. If successful, DNA storage could be the answer to a uniquely 21st-century problem: information overload.

The world has produced 4.4 zettabytes of data in 2015; that's set to explode to 160 zettabytes each year by 2025. Current infrastructure can handle only a fraction of the coming data deluge, which is expected to consume all the world's microchip-grade silicon by 2040. DNA has an information-storage density several orders of magnitude higher than any other known storage technology. For instance, formatting every movie ever made into DNA; could be smaller than the size of a sugar cube - and it could last for 10,000 years.

Cost, however, is still a limitation. Sequencing—or reading—DNA has gotten far less expensive in the last few years, but the economics of writing DNA remain problematic if it’s going to become a standard archiving technology. DNA-synthesis companies like Twist Bioscience charge between 7 and 9 cents per base. 

In March 2018, University of Washington and Microsoft published results demonstrating storage and retrieval of approximately 200MB of data. The research also proposed and evaluated a method for random access of data items stored in DNA. In March 2019, the same team announced they have demonstrated a fully automated system to encode and decode data in DNA.

------------
DNA (MOLECULAR) COMPUTING

DNA Computing will offer an alternative to silicon-based systems that will potentially be faster, smaller and more energy efficient for some specialized problems, potentially offering massive parallelism, massive storage and high levels of Artificial Intelligence alongside low-waste and low-energy usage. DNA computing may also provide a whole new field of innovation in healthcare and life sciences, such as for detecting cancerous activity within a cell and releasing an anti-cancer drug upon diagnosis.

DNA Computing is one of the ways currently explored to solve combinatorial problems. It consists in coding a problem instance with DNA and use classical molecular biology to simulate the calculus.

The slow processing speed of a DNA computer is compensated by its potential to make a high amount of multiple parallel computations. This allows the system to take a similar amount of time for a complex calculation as for a simple one. 

DNA computing may be used for executing calculations that require extreme parallelism, which can be achieved through the billions of molecules interacting simultaneously with each other, and solving problems that cannot be deterministically solved in polynomial time.

This computing system may provide advanced bio-templates for bacterial and viral analysis, sparking other biomechanical technologies since they are currently the only self-replicating computing technology known.


The computing system using DNA relies on data encoding mechanisms that are fundamentally different than conventional computing ones. While electrical charges are manipulated by classical computer in binary form, the information is encoded in DNA chemical basis in quaternary form (A, T, C and G bases).

The extreme slowness of this process (response times are counted in minutes, hours or even days) is compensated by the massively parallel capabilities: several billions of molecules are interacting with each other. 

These alternative computers have a number of potential use cases, primarily performing living processes based on complex biomolecular interactions involving biomolecules coded by our DNA. 

With DNA computing on course to impact Health and Life Sciences first, especially when combined with other areas of genomics (such as gene editing), it would be recommended that organizations in these industries ensure they keep abreast of latest advances; other industries, meanwhile, should understand biocomputers at a high level at this stage. DNA computers are at their initial phases of R&D, so business impact is still far off.  





------------------------

SWARM COMPUTING

Swarm computing directly addresses new business models around digital markets by creating novel opportunities at the intersection between people, businesses and things. It forces existing offerings that have emerged as part of a centralization paradigm to evolve into a hybrid decentralized and autonomically managed environment.

While Edge computing is the initial step towards the decentralization of computing, swarm computing will consolidate this trend by exploiting IoT devices' increasing rich computing and storage capacities. Combining complex multi-Cloud architectures with Edge computing will enable swarm computing scenarios to develop. 

Swarm computing is the emergent collective intelligence of groups of simple agents having characteristics of self-healing, self-configuration and self-optimization. It is a group of similar elements (the swarm) behaving through mutual interaction to achieve an outcome as a collectivity. Learning from nature in the man-made world, swarm computing combines network and Cloud capabilities to create on-demand, autonomic and decentralized computing. 

Swarm computing is envisaged as an evolution of current IoT, Edge and Cloud computing environments, taking advantage of the computing, network and storage capacities that more complex mobile IoT devices will bring. Some identified scenarios such as smart robotics and connected vehicles are developed at the technological level, while others such as smart homes, smart grids and intelligent transportation are set at the application level.

Cloud computing is mainstream today, and there is a significant technology push toward the adoption of IoT and Edge computing. Swarm computing will be the next step after these are consolidated in the market, relying on existing best practices for Edge and Cloud computing to enhance the mobility, temporary control and data sharing of connected devices.

Businesses should be encouraged to investigate swarm computing today to fully exploit it tomorrow.

-----------------



MANAGED IOT CONNECTIVITY SERVICES

Managed IoT connectivity services are a small component of an IoT solution but have a critical role due to the complexity of endpoints and connectivity types to manage.

According to Gartner, by 2023, 10% of manged IoT worldwide connectivity will be provided through hyperscale cloud providers, up from less than 1% in 2019.



Managed IoT connectivity services, also known as managed machine-to-machine (M2M) services, encompass connectivity hardware, software, and network and IT services that are generally bundled and managed by a third-party provider. These services enable enterprises to connect, monitor and control business assets and processes over a fixed or wireless connection. These services are key to informing and integrating purpose-built and stand-alone telematics systems, IoT platforms or legacy back-end IT (e.g., ERP, CRM) and OT systems (e.g., SIS, DCS).

Key Features
Sourcing and logistics management
Connectivity management portal
Service management and support
SIM and subscription management
Device management
Application management
Security management
Multisourcing service integration (MSI)

5G and edge computing, which are likely to accelerate growth in this market
Connected vehicles; by 2023, over 60% of all new connected vehicles produced will feature an embedded SIM (eSIM) for cellular connectivity, up from less than 5% in 2019, according to Gartner.
Broader opportunities for enterprises looking to offload the management of IoT solutions include cost reduction and increased time to value, access to new skills, improved user experience and achievement of more effective monitoring/management to guarantee a certain level of uptime and security.


Global 3GPP LPWAN deployments require roaming and interoperability which still needs at least two years to mature
Difficulty in choosing from the wide range of service activities supported, and picking the right expertise from a growing and fragmented provider landscape
Pricing can be unclear and ill-defined
Lack of use cases that link successful managed IoT deployments with ROI.

The market for cellular-based-managed IoT connectivity services is mature for traditional 2G, 3G and 4G LTE networks, field-area networks (FANs) and satellite. LPWAN (low-power wide-area network) will be a key technology for managed IoT connectivity services providers in the near-future. It is expected that by 2022, 40% of global managed IoT connectivity vendors will offer worldwide 3GPP (3rd Generation Partnership Project) LPWA network (including NB-IoT (NarrowBand IoT) and LTE-M (Long Term Evolution for machine-type communications)) roaming coverage, up from 0% in 2019. The market for 3GPP LPWAN is likely to accelerate due to national deployments in large countries like China, specifically NB-IoT, or the US. Partnerships will also be key – by 2023, 80% of manufacturers that embed 3GPP connectivity services will use a partner revenue share model, up from 20% in 2019.

Access to managed connectivity services will continue to be important when rolling out IoT solutions at scale – technology leaders will be required to look beyond network coverage to the quality and capability of vendor portals, security and broader deployment support services. Managed IoT connectivity services have the potential to transform the IoT into a measurable, recurring operational expense as opposed to a nonrecurring capital expense that would be managed internally.

--------------

IOT EDGE ARCHITECTURE

There is a growing awareness of the technical challenges involved for both end- users and vendors when bringing data at the edge of IoT, alongside an understanding that edge processing and storage will be key elements of many IoT deployments.

Edge capabilities are increasingly being used in end user product and technology evaluation,  and most major vendors involved in IoT enablement are looking to address the edge in one form or another.

With the growth of analytics being applied to IoT data, many end users are looking to put more analytics capabilities closer to the point of data creation, giving vendors opportunities to offer more products with compute and storage capabilities  and IoT-specific edge networking solutions (e.g. embedding security, data and device management in IoT-specific edge networking solutions).

IDC predicts that, by 2024, over 75% of infrastructure in edge locations will be consumed and operated via an as-a-service model, giving service provides the opportunity to focus on enabling infrastructure (e.g. collocated data-centers and matching IT resources to network access points).

Although core data-centers will continue to make up the lion’s share of spending on compute/storage infrastructure in support of IoT workloads, spending on storage deployments at the edge is set to grow rapidly and more data will need to be stored for processing at the edge. A broad set of potential IoT configurations will fuel the adoption of IoT infrastructure at the edge. A well-defined and carefully designed IoT edge architecture is therefore a critical building block for the majority of IoT initiatives. The best examples balance technical elements, such as data analytics, security and scalability, against overall business requirements and integration complexity.

Due to this - IoT deployments becoming more complex and businesses more data-driven - decision-makers must look to combine scalability with manageability. This becomes more and more challenging when dealing large volumes of data generated by connected devices, and with network limitations of bandwidth, latency and connectivity. Analytics needs to be brought to the data, rather than vice versa, and pushing analytics to the edge of IoT requires a flexible, knowledge-driven framework.

Due to the challenges around deploying an advanced IoT edge architecture, the business case is not always there even if the use case for doing so is. Hence, IoT edge architecture is classified as an improving technology with a maturity horizon of 3-5 years. It is important to note that these dynamics can vary notably by industry vertical – investing in edge architecture in consumer IoT is less feasible than in industrial IoT, for example, where applications tend to require robust edge capabilities by necessity.

IDC expects the IoT edge infrastructure market to grow at 20.3% between 2019 and 2024, from $2.5bn to $6.4bn.

-----------------

IOT PLATFORMS

Organizations around the world are adopting IoT strategies to gain operational efficiencies, as well as to create better customer experiences and new product and service offerings. IoT platforms play a central role in such deployments, offering a standardized approach to build, deploy, manage, and secure IoT applications and devices.



The IoT platform market is yet to consolidate but is growing in complexity – vertical IoT platforms are emerging alongside horizontal ones, including multifunctional software platforms which connect and manage IoT devices as well as analyze IoT data. These IoT platforms are playing a key role in the deployment of IoT strategies, offering a standardized approach to build and deploy managed and secure IoT applications and devices.

Enterprises are leveraging IoT platforms that drive specific business solutions such as predictive maintenance, remote asset tracking and consumption efficiency, and suppliers should focus on providing examples of specific use cases to customers rather than purely demonstrating technology.

An effective IoT platform can handle the complexity of IoT solutions and provide customer value, if providers can address the top issues which inhibit IoT developments; security, privacy, initial and ongoing costs, lack of skills and proof of ROI. Openness will be also key for IoT platform success – collaborative models with open and fully documented APIs/open source tools that use industry standards will have the strongest chance to thrive. Building an ecosystem of partners around an IoT platform will also be key for suppliers, as partners can provide technology functionality, vertical industry expertise, service capabilities and go-to-market opportunities.

According to IDC,  the global IoT software platform market is set to grow at a compound annual growth rate (CAGR) of 28.6% between 2018 and 2023, from $1.5 billion to $5.3 billion.

--------------------

IOT SECURITY

The IoT security market is maturing rapidly to meet the challenge of new cybersecurity threats in a multitude of industries, brought about by the increasing proliferation of connected devices.

Amidst the proliferation of IoT devices, IoT security is too often an afterthought when developing hardware or software that may contain vulnerabilities. Concerns persist regarding the integration of IoT with traditional systems and practices, often deepened by the vertical and use-case-specific nature of IoT security deployments. By 2021, according to Gartner, IoT risk and security needs will increase total IoT project costs by 10%, up from 3% in 2018. The market for security technology for IoT is expanding with providers offering hardware and software through the IoT stack as well as different service delivery formats.

IDC expects worldwide IoT security market to grow from $12.9 billion in 2018 to $21.3 billion in 2023, at a CAGR of 10.5%. Physical IoT security and safety is the largest segment, representing approximately three quarters of the total.

Although IoT security spending is increasing, it is playing catch-up to emerging threat vectors. Successful IoT security deployments require collaboration among multiple organizational roles, from infrastructure and operations to application deployment and delivery, to business insights, to security. And with an evolving landscape, enterprises must be prepared for unexpected threats – by conducting risk assessments and simulating IoT-specific breaches. IoT security is about securing the IoT network and IoT devices, so multiple control layers and products will also be required. Analytics is also key for IoT security - more sophisticated analytics can deliver automation that will help security professionals.

In the short-term, IoT security will remain the main barrier to entry to the IoT. In the longer-term, however, emerging security technologies will in fact enable the IoT.

------------------

DIGITAL TWIN

The digital twin is building momentum continually as the idea of modelling people, physical assets and processes gain traction. According to Gartner, over 40% of enterprises expect to deploy digital twins within three years – digital twins could become the dominant design pattern for digital solutions within the next decade.



A digital twin is a virtual representation of an entity such as an asset, person or process and is developed to support business objectives. The three types of digital twins are discrete, composite and organizational. Digital twin class elements include the model, rules, relations and data properties. Digital twin instance elements include the model, data, unique one-to-one association, and monitorability.

Key Features
Digital twins for people: Digital twins are the evolution of trends including customer 360-degrees, patient electronic health records, and fitness monitors. Their near-term uses include health monitoring and employee safety, particularly in response to the pandemic.

Digital twins for physical assets: Digital twins adoption aligns to Internet of Things (IoT) trends. For owner/operators, near-term use includes lowering maintenance costs and increasing asset uptime for equipment users in factories, hospitals, utilities, etc. For product original equipment manufacturers (OEMs), near-term uses include product differentiation, business model differentiation through new product service models, and obtaining customer data.

Digital twins for processes: Digital twins are being developed to model IT organizations, financial exchanges, and processes such as purchase orders.

Digital twin capabilities are being positioned as competitive differentiators by several industrial companies, (including ABB, GE, Hitachi and Siemens) as well as systems integrators/consultancies and manufacturers. By 2021, according to Gartner, half of large industrial companies will use digital twins, resulting in those organizations gaining a 10% improvement in effectiveness.

 

There are many upsides to digital twins but application is complex – these complexities vary based on the use case, the vertical industry and the business objective. Some digital twins may be simple and based on clearly defined functional or technical parameters, while others may be far more complicated requiring new capabilities across the enterprise and with business partners. If correctly designed, digital twins could lower maintenance costs and increase asset uptime in industry, or help manufacturers manage warranty costs, support channel partners and better understand customer experience. 

That said, the market is very much in its infancy. Gartner’s CIO Survey 2020 shows that 6% of enterprises have implemented digital twins, although less than 1% of assets have digital twins. According to Gartner, the digital twin has moved past the Peak of Inflated Expectations, because of enterprise confusion driven by conflicting vendor marketing and implementation challenges.

--------------------

5G

5G, the next-generation standard after 4G, is envisioned to offer ultra high-speed internet, low latency and high device connection density as its key features. High-density connections offer long battery life for IoT applications (up to 1,000,000 devices per square km).



5G, the next-generation standard after 4G, is envisioned to offer ultra high-speed internet, low latency and high device connection density as its key features. High-density connections offer long battery life for IoT applications (up to 1,000,000 devices per square km).

5G is the next-generation cellular standard after 4G (Long Term Evolution [LTE], LTE Advanced and LTE-A Pro). It is currently being defined across several global standards bodies — the International Telecommunication Union (ITU), the Third Generation Partnership Project (3GPP) and the European Telecommunications Standards Institute (ETSI). The official ITU specification, International Mobile Telecommunications-2020 (IMT-2020), targets maximum downlink and uplink throughputs of 20 Gbps and 1 Gbps respectively, latency below 5 (milliseconds) and massive scalability.


Standardized 5G networks promise to combine the benefits of 2G, 3G, 4G/LTE, wired technology, and Wi-Fi into a single platform that offers density, high speed, and reliable connections at low power. Many IoT deployments require real-time communication with very low latency and need to accommodate large volumes of data traffic, for which 5G is highly suitable. 5G networks are expected to serve as a boost for IoT cellular connections, helping cellular providers maintain a competitive advantage over LPWAN alternatives.

However, commercial rollouts have been limited. The state of 5G in 2020 varies considerably by country, but it continues to be immature overall. The current focus is heavily on high-speed data, coverage won’t be ubiquitous for several years and real-world performance will be variable. Gartner expects that in 2020, 3% of mobile communications service providers (CSPs) will launch 5G networks commercially - innovation opportunities might not appear until 2023. 5G deployments are not straightforward – the use of higher frequencies for spectrum, as well as massive capacity, will require dense deployments with higher frequency reuse. 5G technologies are being developed and deployed in parallel with many other technologies, such as autonomous vehicles (AV). AV systems generate huge amounts of data, requiring careful management to extract optimal value without excessive cost, while safety performances are attracting scrutiny from regulators, creating opportunities for 5G technology to expand and enhance native AV safety systems.

Technology leaders should therefore look to include 5G in their innovation roadmaps and pilot applications, while planning how to mange the risks of immaturity and studying the early experiences from rollouts across the globe.

----------------

IOT SERVICES

Research continues to demonstrate that enterprises’ lack internal resources skilled in IoT technologies and knowledge of how to apply and how to operationalize the integration of IT, OT and IoT. Provision of IoT services therefore plays a vital role in the market.

The continuing proliferation of connected devices means that the market for IoT services will carry on growing at a fast pace, as providers leverage IoT to improve customer experiences. By 2021, over 65% of midsize to large enterprises will use external IoT services for at least half of the IoT solution build effort, up from 35% today, according to Gartner.



IoT services encompass support, maintenance and professional services to provide a range of business and technical expertise in support of IoT plan, build and run services. Various frameworks, methodologies, and assets are within scope for IoT services. IoT services must be viewed within the broader remit of “digital services.” The core outcomes of IoT services lie in the enablement of data acquisition and data contribution to broader digital business strategies.

Key Features
There is a broad mix of providers, industrial equipment OEMs, traditional IT ISVs, IT and OT system integrators, niche IoT providers (hardware and software), offering a catalog of IoT services that spans:

Advisory and consulting services that address business and technology issues.
IoT-specific development and integration of legacy IT and OT, or ensuring that legacy enterprise applications benefit from IoT data acquisition.
Installation and product support services aimed at the Microsoft Azure IoT Edge.

As the IoT market matures, professional services companies are becoming better positioned to enable IoT implementations, as focus shifts from the technologies developed (eg. sensors, connectivity, analytics) to the services (eg. consulting, SI) provided. These players can position themselves as a controlling part of the IoT ecosystem as focus shifts further from consulting and SI towards managed IoT services.

From a buyer perspective, the proliferation of technology and connected devices is putting the onus on enterprises to develop or acquire in-house IoT skills and analytics expertise – companies should continue to make significant IoT investments in the next 1-3 years as adoption of IoT-enabled solutions continues at pace.

Service providers should address key customer concerns in order to help enterprises capitalize on the IoT services opportunity:

Resistance to change within client organizations.
Reliance on legacy systems that are incompatible with new requirements.
Lack of understanding of ROI.
IDC predicts that spending on IoT services will reach $30.8 billion by 2021, from $12.1 billion in 2016, at a CAGR of 20.7%.

-----------------------


HAPTICS

XR-based haptics technology allows users to interact with virtual objects in a completely new way. They can essentially touch these virtual objects by wearing specialized gloves and enjoy a greater first hand experience. Haptics is yet to fully evolve, and may take several years before it reaches mainstream adoption.

Just as AR and VR appeal to our visual and audio senses, Haptics technology appeals to our sense of touch. The technology makes use of sensor-based wearable devices such as gloves and suits that allow users to touch any virtual object and get a perception of it being real. The perception includes attributes like texture, temperature, and pressure. Haptics technology provides a more holistic user-experience when used in conjunction with AR or VR.

With the advancements in XR haptics companies are aiming to use the sense of ‘touch’ to make virtual things feel more real – to replicate natural interactions. Most existing touch-based technologies have been devised only to make things noticeable – e.g. phone vibrations.

The rise of new haptics technology will add a new dimension to XR and make the user experience more holistic. According to IDTech, the XR haptics market will reach $4.8 billion by 2030. In addition to being more engaging, the technology will enhance the overall utility of XR by enabling new use cases. For example, with haptics gloves users can now replicate their exact hand movements on a robot’s mechanical arm which is thousands of miles away. The multiplicity in form factors such as suit, gloves, and even ‘mid-air’ allows for greater flexibility and more enriching experience.

It’s important to note that this technology is quite nascent and existing applications are few. Hence, it may still take about 5-10 years before getting into the mainstream adoption. Further, the high cost of these devices is also acting as a barrier to adoption.

-------------

MIXED REALITY

Mixed Reality (MR) combines the best of both augmented and virtual reality. The resulting experience is highly engaging and allows the users to interact with virtual objects in the physical world.

Mixed Reality (MR) is a technology similar to Augmented Reality (AR) which overlays computer-generated digital objects onto the real world. However, one main difference is that MR allows users to interact with both physical and virtual objects while AR allows only viewing. For example, both AR and MR have the capability to display a virtual 3D box on an actual table. But MR allows users to pick up the box and open it. MR allows an intermingling of the physical and virtual worlds quite well.

The global MR market opportunity is expected to reach $3.7 billion by 2025, growing at a CAGR of more than 40%.

Research indicates 82% of enterprises expect to use MR within the next 3 years. Major players such as Airbus, Renault Trucks, ThyssenKrupp, and BAE systems have either adopted or planning to adopt MR solutions.

Mixed Reality gives enterprises an opportunity to solve some of their core business issues – such as improving operational efficiency and providing quality training.
Externally, MR can serve as a powerful technology for marketing and customer engagement. Consumers can define their own rules using MR and see a world where the virtual and real elements interact in a pre-defined way.

Technological – It is tough to create a perfect virtual world that exactly aligns with the real world such that users find it deceptively real.
Most MR devices are still at an infancy level – including a poor battery life, small field of view, and are often uncomfortable when used for long.

Most existing MR devices support interaction either using remotes or speech. Mid-air hand gestures often cause fatigue on repeated use. The ways to interact with the environment need to evolve in case of MR.
Cybersecurity concerns for connected MR devices.

A blend of AR and VR, Mixed Reality (MR) is fast becoming the favorite of enterprises as it enables them to see and do things in ways that were not possible in a purely real world. According to a recent survey, 87% of executives are currently exploring, piloting, or deploying MR in their company workflows. Another 68% believe that MR will be vital to achieving their companies’ strategic goals over the next 18 months. Further, the global market is expected to reach $5.3 billion by 2024, growing at a CAGR of 72% between 2018-2024.

MR is more complex than other XR technologies including AR and VR, and this leads to technological barriers it may have to overcome to reach mainstream adoption. This includes a more accurate alignment of real and virtual worlds, longer battery life, high-speed connectivity such as 5G to support mission-critical remote applications, and faster processing for standalone operations.

Having said that, MR can prove to be the most useful of all XR technologies given its ability to interact with the virtual and real objects and manipulate them. This allows enterprises and consulting firms to bring to life a range of new applications that can help generate new business and make the existing businesses operate more efficiently.

-----------------

XR PLATFORMS & CONTENT

Extended Reality (XR) software, platforms and content are essentially the key ingredients of any XR application that make them engaging and relevant for the end user. Technological advancements in these components are as critical as advancements in devices such as HMDs. Investments in these areas can fuel rapid adoption of the XR technologies.

Extended Reality (XR) software, platforms and content are essentially the key ingredients of any XR application that make them engaging and relevant for the end user. Technological advancements in these components are as critical as advancements in devices such as HMDs. Investments in these areas can fuel rapid adoption of the XR technologies.

The availability of SDKs (Software Development Kits) such as ARCore and ARKit has opened up the market to a large number of developers who can now build a variety of XR apps with even little technical expertise.

Traditional content companies have an opportunity to develop unique content for the new AR and VR platforms. One company, for example, recreated the entire Apollo Moon Landing experience in the form of VR content, and is monetizing the same.

New opportunities exist across several sectors. For example – companies in Education sector can use these platforms to redesign their annual curriculum to make them more engaging and student friendly.

The user interface of certain SDKs can be more user-friendly, since they require developers to have greater coding skills. This raises the entry barrier for people with limited technical background.

At the moment, platforms like ARCore and ARKit support only a limited number of devices.

Software and platforms form the foundation of any XR application and advancements in these are as crucial as advancements in devices. The presence of ARCore and ARKit as two major software platforms has worked well for the AR community since these common set of frameworks has allowed development of apps that work across many devices and are interoperable within their ecosystems. Also, the open-source nature of ARCore allows a large community of developers to contribute to the codes and enhance their functionalities and security. According to IDC, the global AR software market is expected to grow from $700 million to $25 billion during 2019-2023. While the VR software market will grow from $3 billion to $19 billion during the same period.

There are, however, some challenges impeding the growth in platforms. For example, ARCore needs more coding expertise than ARKit, given its UI. There is even a dearth of good web-based platforms such as Amazon Sumerian. Also, there are few high-quality content for VR applications – most of which are available mainly for educational and training purposes and remain inaccessible to masses. Further, developing such content is costly and time-consuming.

Issues like these can be resolved if more companies jump into the bandwagon, creating more high-quality and affordable content – fueling the demand. From a maturity standpoint, it may take them another 3-5 years before these platforms and content become pervasive.

---------

AUGMENTED REALITY

Augmented reality (AR) is the real-time use of information in the form of text, graphics, audio, and other virtual enhancements integrated with real-world objects and presented using a Head-Mounted Display (HMD). AR acts as a digital extension of the user’s senses, serves as an interface between humans and the physical world and provides a digital filter to enhance the user's surroundings with relevant or actionable information.

Augmented reality (AR) is a type of interactive, reality-based display environment that takes the capabilities of computer-generated display, sound, text and effects to enhance the user's real-world experience. AR combines real and computer-based scenes and images to deliver a unified yet enhanced view of the world.

AR is broadly applicable across many markets, including gaming, industrial design, digital commerce, marketing, mining, engineering, construction, energy and utility, automotive, logistics, manufacturing, healthcare, education, customer support, and field service.

Augmented reality (AR) provides a digital filter to enhance the user's surroundings with relevant, interesting and/or actionable information.
Augmented reality (AR) can provide value by enhancing training, maintenance and collaboration efforts.
AR offers brands, retailers and marketers the ability to seamlessly combine physical campaigns with their digital assets.

Hardware limitations such as the slow development of transparent displays, battery life constraints and processing power for Head-Mounted Display (HMD)
Speed of network connectivity, especially for mission-critical applications, which is likely to get resolved when 5G becomes more pervasive.
The high cost of developing and deploying AR solutions as the adoption rate is still quite low.
Security and privacy issues as AR solutions may involve use of personal data and location sharing.

Although Augmented Reality (AR) solutions look promising in terms of their offerings, the actual progress is limited. The use cases for businesses and consumers need to be defined more clearly for industries. The existing applications are few and cater to mostly the enterprise segments. Consumer applications are generally limited to social media/gaming industries. Having said that, solution providers are working towards creating compelling applications and devices to exploit this opportunity. And IDC believes that the worldwide spending on AR will go up from $3.5 billion in 2019 to $102 billion in 2023.

While advancements in devices such as HMDs are vital to the success of AR, the good news is that there are numerous smartphone/tablet apps that support complex AR applications. For example, civic bodies in the US are testing a smartphone application to view assets below the street just like a virtual X-ray. On the software/framework front, companies like Google and Apple have already developed their XR platforms – ARCore and ARKit, to drive interoperability.

As more vendors flock to AR, a variety of use cases can be brought to life. Similarly, a drop in the device cost will also fuel the adoption by enterprises and consumers. It’s time that companies should start looking at AR as a revenue generating opportunity rather than just a cost saving / efficiency-driving technology.

--------------

VIRTUAL REALITY

Virtual reality (VR) provides a computer-generated 3D environment that surrounds a user and responds to an individual's actions in a natural way, usually through immersive Head-Mounted Displays (HMDs). Technologies such as cloud graphics processing and mobile video games, as well as the proliferation of broadband access, will allow application developers to integrate VR more easily into their products.

Virtual reality (VR) is an immersive technology that creates a fully virtual environment that users can experience through devices like a head-mounted display (HMDs). Users can experience the virtual environment through movements and gestures, and the environment responds to these actions such that users perceive it as a real world.

Progress in VR opens up opportunities for the entire XR ecosystem – including device makers, chip-makers, developers, UX/design companies, and content providers.
A lower cost of device along with high content quality can fuel greater adoption.

As businesses integrate VR applications with their existing products and services, it can give rise to new revenue streams. VR can be used to gather customer insights and unlock business value. For e.g., Kellogg uses a VR solution to track customer eye movement as they enter a retail store, helping the company to plan its product placement.

Success of VR systems depends on the quality of the UX or app. It needs more high quality content such as games, movies, music, and software that must be envisioned and built proactively. The current approach to content creation is still experimental.

Battery life for untethered devices is currently inadequate, as is processing power for graphically intensive applications.
Lack of high-speed connectivity such as 5G for low-latency applications.

The inconsistency between technology and human biology leading to issues like motion sickness even though there is no real motion. Further, these devices carry health warnings for pregnant, elderly, and heart patients.

In its decade-long existence VR has already managed to find a place higher up on the maturity curve. The growing number of use cases and applications will likely enable the technology to approach maturity/main-scale adoption in next 3-5 years. According to IDC, the worldwide spending on VR is expected to go up from $13 billion in 2019 to $58 billion in 2023. There is a full-blown ecosystem to supply various components of VR – including devices, content, and software/platform. However, these players are at different maturity levels slowing down the overall growth.

There are a few bottlenecks in VR which companies will need to overcome though. First, most sophisticated VR headsets today need to be tethered to a PC to run high-end graphics and these headsets serve as mere displays. This could limit the use of VR for remote applications requiring greater movement. Battery life and high-speed connectivity like 5G are some of the other challenges that need to be overcome for a standalone VR.

In terms of use cases, training continues to remain as one of the most popular enterprise applications. VR-led trainings appeal to a variety of learning styles, offer experiences that promote retention, eliminate risk and safety concerns, and save significant time and budget. For consumers, VR will be most appealing in gaming and education, as it continues to provide graphics and sensory-rich experience to users at a relatively affordable cost.

--------------



AUGMENTED MEMORY AND COGNITION

Augmented cognition is a form of human-systems interaction in which a tight coupling between user and computer is achieved via physiological and neurophysiological sensing of a user's cognitive state. This interactive paradigm seeks to revolutionize the manner in which humans engage with computers by leveraging this knowledge of cognitive state to precisely adapt user-system interaction in real time.

Augmented cognition could potentially be very useful if applied to situations in which people are required to make quick decisions within a demanding or stressful work environment such as aerospace, homeland security and military operations.

Augmented memory shows high potential in healthcare, in particular to ease the symptoms of brain disease and neurological disorders (e.g. amnesia).

Augmented cognition is based on theoretical models of cognitive processes, but it is difficult to imagine how scientists could enhance processes that are still not fully understood (e.g. Working Memory).

Skeptics say that augmented cognition is science fiction while researchers working in this field, on the contrary, affirm augmented cognition will be accomplished in a 5 to 10 years timeframe.
Exposure to potential hackers of the human body.

Human augmentation theory is grounded in the view that human information-processing capabilities are the weak link in the symbiotic relationship between humans and computers. Here there are several “bottlenecks” including sensory memory, Working Memory, attention, and executive function. As computational prowess continues to increase, human and computer  capabilities are ever more reliant on each other to achieve maximal performance. Demanding or stressful conditions call for expertise not from a specific human or computer system, but from a linked human–machine dyad. A dyad that is functionally a human and their computational system, which through shared experience and insight into how they both function, will jointly deliver solutions at a previously unimagined rate.

Augmented cognition thus seeks to 1) enhance sensory perception by exploiting multiple sensory channels for increased input capacity and 2) support simultaneous processing of competing tasks by allocating data streams strategically to various multimodal sensory systems while maintaining multimodal information demands within working memory capacity.
As wearable device sensors have become ubiquitous, there has been a rapid progression along the lines of detecting physical, emotional and cognitive states of humans. While invasive techniques provide better results, it is expected that the noninvasive technologies will grow at a quicker rate.

--------------

AUGMENTED HEALTH, AUGMENTED IMMUNITY

Augmented Health refers to the use of all relevant physical, cyber, and social data obtained from wearables, AR/VR, Internet of Things (IoT), mobile applications, Electronic Medical Records (EMRs) to offer a more targeted and personalized medicine. The exploitation of all relevant data, relevant medical knowledge, AR/VR and AI techniques will extend and enhance human health and well-being.

Augmented Immunity refers to advances in human genetics and immunotherapy  such as disease interception, gut microbiome, cell therapy and immuno-oncology.

Repair/immunize biosystems from disease
Extend or improve the limits of human perception with implant hardware ranging from microchips to magnets
Transform big data into actionable smart data. Use low-cost sensors to continuously collect multimodal data about the patients physiological and psychological condition, activities (including meals, exercise and sleep), and about the surrounding environment

Recent studies indicated that the volume of healthcare data has reached 150 exabytes in 2017. However, there are several challenges to be overcome in making sense of this large amounts of data. For instance: ensuring the reliability and quality of the data as well as proper interpretation of data that is diverse and multimodal.
Ethical and legal issues will come hand in glove with Augmented Humanity. Technologies that so profoundly impact human health and performance will create a lot of controversy and debate.

Augmented Health is an entirely new approach to human health compared to the current episodic system of periodic care primarily centered around healthcare establishments. It involves continuous monitoring, engagement, and health management, where rather than treating a patient for a disease, the focus shifts to involving the patient in preventing disease, predicting possible adverse outcomes and preventing them through proactive measures, and keeping them healthy and fit with lifestyle changes. Rather than chronic disease management, it takes a holistic approach to improving the overall quality of life.

Innovative technologies to mimic the cellular environment and quantitative methods to study single-molecule to multicellular events are essential tools for the biomedical community.The revolution of cellular medicine has the ability to augment our immunity and longevity by amplifying the body's ability to fight disease, heal, and regenerate itself. Most augmented immunity therapies such as disease interception, gut microbiome, cell therapy and immuno-onocology are still embryonic and are expected to be transformational in the next five to ten years.

---------------------

AUGMENTED BIO-STRUCTURE, BIOPRINTING

Augmented biostructure refers to the repair or replacement of human tissue (e.g., skin, cartilage, bones and blood vessels) as well as organs. The advances in 3D bioprinting is a major contributor to this biotech area. 3D bioprinting involves the utilization of 3D printing–like techniques to combine cells, growth factors, and biomaterials to fabricate biomedical parts that maximally imitate natural tissue and organs features. Bioprinting covers a broad range of biomaterials including synthetic and biological ones.

Main opportunities are in life science R&D and human transplants. The latter is one of the ultimate transformative breakthroughs of healthcare's digital business era.
The ability to precisely personalized human tissues substantially augments the array of therapeutic options for a wide range of patient injuries and diseases. Bioprinted organs using the patient's own cells are hoped to have the benefit of avoiding the rejection of implanted/transplanted tissue, and the costs of a lifetime of anti-rejection drugs.

Creating transplantable organs has all the complexities of human tissue bioprinting and many others, such as functional vascular systems. Challenges include how to test how well the organs will integrate in the body (avoiding rejection issues) and how to test and prove the long-term viability and effect of the organs.

Very complex scientific, technical, regulatory and adoption issues are also in play. Very profound potential business, funding, ethical and personal health impacts also arise when all of the other challenges are finally conquered.

Bioprinting is still an embryonic market but has the potential to be disruptive. The real challenge isn't the bioprinting technology itself, but rather the scientific assumptions and analysis behind it. In order to improve the outcomes from 3D bioprinting, informatics groups must close the loop between the physical testing and informatics-based simulations.
There is a current and currently bigger market for production of 3D printed tissues for life science research and drug testing. This business is providing some of the needed early revenue for startups, targeting human tissue and organs 3D bioprinting.

Nevertheless, the impact on medicine is expected to be transformational in five to ten years. Bioprinted organs using the patient's own cells are hoped to have the benefit of avoiding the rejection of implanted/transplanted tissue, and the costs of a lifetime of anti-rejection drugs.

-------------------

HYPER-CONNECTIVITY

Hyper-connectivity refers to the state of being constantly connected to people and systems through smart devices and (sometimes) software. New interaction designs — from gesture and expression to voice —  are transforming digital usage patterns and creating entirely new usage contexts. The continued hyper-connectivity of people, services and smart things will only increase as the smart device ecosystem grows, augmenting human potential.

Conversational platforms operating across the smart ecosystem — The shift to voice interaction with services and knowledge bases is underway. By 2020, VPAs will facilitate 40% of mobile interactions, and the postapp era will begin to dominate.

Moving computing power closer to edge devices. The architecture to support this is delivered by what is called "the fog" (a low-lying "cloud" closer to the ground/edge).
Biometric authentication and payment methods via wearables. Authentication techniques, such as iris and gait recognition from wearables and vein analysis, are in the rapid prototype stage.

The combination with tactile interfaces, such as touch and haptics, can bring well established interactive gestures, such as pinch, swipe and expand, to articulate a new human-machine interface blend between gestures and augmented reality.

Load on the network — Hi-Fi sensor and rich-media streaming to the cloud from smartphones, wearables, connected homes and automobiles does not scale.

Security and privacy — Not all consumers wish to store sensitive personal data about health and family matters in the cloud. Many will prefer to store personal information locally within a home or work environment for reasons related to data ownership, privacy, data theft and data loss.

For the last 15 years, consumers have seen a transformation from the communication technologies that resulted in most people being ‘always-on’. Today we are heading towards more extreme forms of hyper-connectivity through increasingly more diverse sets of communication channels and human-machine interaction. The World Economic Forum in 2014 stated that Hyperconnectivity will be to the 21st Century what the internal combustion engine was to the 20th Century.

A richer array of new devices is creating entirely new digital experiences that would otherwise have been pointless or impossible on smartphones. We see new experiences being woven into kitchen appliances, healthcare and other areas of consumers' lives. Car tech has unlocked entirely new insurance models for companies leveraging IoT, while connected-home products provide new levels of safety and security using cameras, facial recognition and smartwatch alerts. Thanks to haptic technology, for instance, users can have a physical experience, making the technology more interactive. This will revolutionize the gaming experience but also be useful in medicine and every-day life.

Areas that will become relevant in the next 5 to 10 years include Connected Personal Hearing Devices, Spatial Computing, Electrovibration or Ultrahaptics and Speech-to-Speech Translation.

--------------------------

AUGMENTED MOBILITY (EXOSKELETONS)

An exoskeleton is a wearable device powered by actuators that is worn on the entire body, or on just the upper or lower extremities. It uses sensors and mechanical or hydraulic systems to assist, enable or enhance human movement such as walking, lifting and repetitive work tasks.

Rapid advancements in robotics and battery technology will drive growth in exoskeletons, especially for manufacturing, military, construction, healthcare and warehousing environments.

Increased focus on worker safety will drive exoskeleton use in construction, manufacturing and warehousing environments.

Augmentation of human strength and endurance will play significant role in adoption. Making hard tasks easy, saving time, and improving quality and motivation will reduce turnover and improve economic returns and outcomes.

Cost and battery life are two limiting factors to broader adoption. Advances in 3D printing promise to reduce unit costs on devices that act as human prosthetics. Low-cost customization is key.

Exoskeletons are highly customizable to specific use cases and operating environments. Without increased interoperability to drive multiuse applications for single devices, mass commercial or military adoption by target markets is still 10 years away.

Because exoskeletons are worn, matching the size of the device and wearer matters for most applications, and often requires at least a semicustom fit. This limits scalability, keeping costs high. Adjustable or fabric-based systems that provide a one-size-fits-most, mass market approach remain years away from being available.

Logistics and warehouse managers should review exoskeleton use cases for greater worker safety or for job functions involving extreme repetitive motions that can create health issues. Exoskeletons in industrial solutions will augment worker strength and endurance. Aerial arms, vests or full-suit-type exoskeletons can be used for tasks such as heavy or overhead loads, or high-frequency and long-duration lifting..

Exoskeletons could help chief safety officers manage safety in better and more efficient ways, especially for rehabilitation use cases and for workers who repeatedly lift heavy objects. The ROI will often be a reduction in worker injuries, which in turn saves on medical expenses.

Product usability and market success vary based on application. Military device adoption estimates are hard to verify, but exoskeleton use will likely follow robotics and drone adoption as all three are related to trends in higher-tech battlefields, medical evacuation and military logistics operating environments.

Exoskeletons used in medical applications range from prosthetics that mitigate permanent mobility loss to wearable training devices for temporary injury rehabilitation. More modern prosthetics using electromyography (EMG) for control mechanisms and built on a 3D printer may be the biggest near term business impact areas. Prosthetics of this type can reduce costs from $20,000 to $200.

------------------

SMART FABRICS

Smart fabric refers to a range of technologies that transform textiles used in clothing, accessories, upholstery and more into devices that can be deployed as sensors, switches, connectors, batteries or displays. The components and electronics may be embedded in the fabric or, in some cases, within the fibers themselves.

Smart fabric exhibits characteristics such as lightness, breathability, waterproofing and heat resistance for use in energy generation and storage, electronic controllers, sports monitoring, and heating wraps.

Smart fabric with sweat-sensing capabilities can be used to derive information such as adrenaline, engagement levels and health parameters.
Smart  fabric with electromyography (recording electrical activity of the muscle) sensors could be used to measure muscle activity for professional athletes.

Still on beta phase or first generation, not ready for wider market adoption
A lack of standards, a complex manufacturing process, insufficient availability of miniaturized electronic components, low yield and high costs continue to be the main challenges preventing mass adoption of smart fabric.

Other factors that impact smart fabric adoption include the high price of smart apparel, low longevity, achieving high-quality end products, cleaning without impacting electronic components, privacy concerns and exclusive marketing budgets to raise awareness.

With the development of smart materials – materials that can either sense, react or adapt to their environmental conditions or to other stimuli – a new field  of  smart  fabrics  promises  to  cause  further  radical  changes  across  a  broad range of industries.

Use cases are expected to extend beyond sports, professional athletes, healthcare and  personal care to wider industries, including automobiles, manufacturing, military, emergency services and engineering. Advancements in battery technology, sensors and electronic components will allow easy integration with the fabric without causing discomfort to end users.

Sensor units, controllers, actuators, and other electronic components are usually incorporated into smart fabrics for proper functioning. The reducing costs of manufacturing such components and fabrics is predicted to allow growth in the global smart fabrics market.

Global smart fabrics market is envisaged to be valued at a US$5.5 bn by the end of 2022, progressing from a US$2.2 bn attained in 2017, at a 19.6% CAGR, according to Transparency Market Research (TMR).

-------------

GENE EDITING, REGENERATIVE MEDICINE

Gene editing (also called genome editing) is a group of technologies that give scientists the ability to change an organism's DNA. These technologies allow genetic material to be added, removed, or altered at particular locations in the genome. Several approaches to genome editing have been developed.

Most uses of genome editing have so far been in scientific research – for example to investigate models of human disease. However, the potential applications of these techniques are much wider than just research. Given that genome editing has the potential to alter any DNA sequence, whether in a bacterium, plant, animal or human being, it has an almost limitless range of possible applications in living things.

Areas of research and possible applications include crops and livestock (e.g. increasing yield, introducing resistance to disease and pests, tolerance of different environmental conditions), industrial biotechnology (e.g. developing ‘third generation’ biofuels and producing chemicals, materials and pharmaceuticals), biomedicine (e.g. pharmaceutical development, xenotransplantation, gene and cell-based therapies, control of insect-borne diseases) and reproduction (e.g. preventing the inheritance of a disease trait).

Some scientists have expressed concern that human germline editing has not only crossed the ethical redline; it is also fraught with many challenges. The recent research by Chinese scientists using CRISPR-Cas9 to edit the embryo genome was not completely successful and it had to be abandoned at its preliminary stage. There were off-target mutations in the genome, which can be deleterious as they can cause cell death and transformation.

Embryo germline editing could be exploited in non-therapeutic research. For instance, it can be used to produce designer babies by eliminating undesired qualities and replacing them with desired ones. Due to the challenges and ethical concerns some scientists call for temporary moratorium should be called on the technology.

Genome editing is of great interest in the prevention and treatment of human diseases. Currently, most research on genome editing is done to understand diseases using cells and animal models. Scientists are still working to determine whether this approach is safe and effective for use in people. It is being explored in research on a wide variety of diseases, including single-gene disorders such as cystic fibrosis, hemophilia, and sickle cell disease. It also holds promise for the treatment and prevention of more complex diseases, such as cancer, heart disease, mental illness, and human immunodeficiency virus (HIV) infection.

Genome editing techniques make use of certain proteins that can cut DNA in a precise, targeted location. Although this family of proteins was discovered in the 1960s, it is only since around 2005 that the ability of some of them to make precisely targeted cuts at almost any position in the genome has been recognized and utilized by scientists.
Among the recent genome editing technologies, CRISPR-based methods are particularly promising owing to their relative efficiency, low cost; and ease of use, and the prospect of making edits at multiple sites in the genome in a single procedure.

--------------------------

BRAIN COMPUTER INTERFACE (DECISIONS AT SPEED OF IOT)

Brain Computer Interface (BCI) is a type of user interface whereby signals in the brain are interpreted, or written to, by a computer. Typically, data is either passively observed for research or used as commands to control an application or device. There are three approaches:

Invasive, where electrodes directly connect to the brain.
Partially invasive, where the skull is penetrated, but the brain is not.
Noninvasive, where commercially available caps or headbands are worn to interact with signals from outside the skull.


While invasive techniques provide better results, it is expected that the noninvasive BCIs will grow at a quicker rate as the method has no issues with infection and discomfort, and can be more easily accommodated by institutions, patients and consumers

Marketers, customer experience professionals and interaction designers can use these devices now to add more quantitative signals on the state of mind to better understand how consumers use products and view messaging.

Outside the medical domain, speech recognition, gaze tracking or muscle computer interfaces offer faster and more flexible interaction than brain-computer interfaces. The need to wear a headband or cap to recognize the signals is also a serious limitation in most consumer or business contexts.

Latency is a major problem for BCI. Novel solutions look to improve the human-computer interface (HCI) with a mutual learning approach, where users alter their own wave patterns as well as machine learning optimizing device learning provides marked performance improvements.

Currently, the best BCIs are used for limb prosthetics, and use 100 channels to distill the neural signals of the brain. The Defense Advanced Research Projects Agency (DARPA) looks to improve this to a million channels with their Neural Engineering System Design (NESD) and in 4Q17 DARPA awarded five research organizations funding to move this project forward. This project also writes to the brain.

Noninvasive methods cannot use higher-frequency signals as the skull blocks and disperses electromagnetic waves meaning less fidelity of signal. A 2017 Stanford study showed that paralyzed patients could type via brain control with invasive methods but not with noninvasive approaches. While control today is not very smooth or continuous, it is possible to control virtual objects in multiple dimensions, play interactive games and control hardware.

Brain-computer interfaces have moved to an emerging level of maturity in recognition of the gains made in large commercial scale deployments in China, VR headsets natively embedding the technology, a richer vendor landscape and broader applications in conjunction with deep neural network technologies.

Grand View Research projects that the total BCI market will be worth $1.73 billion by 2022 and expected to grow at a rate of over 10.0% over the forecast period (2016 to 2024).

------------

AUGMENTED SENSING (SENSOR FUSION BASED)

Augmented Sensing aka reverse electrovibration, also known as virtual touch, is an augmented reality (AR) technology that facilitates electronic transmission of the human tactile sense, allowing end users to perceive the textures and contours of remote objects. The field of study involving virtual touch is known as haptics. There are three major categories of haptic systems: graspable, wearable, and touchable.

Many tech-giants are investing in the future of haptic feedback. More and more patents are being filed each year in this technology. Immersion Corporation holds maximum number of patents in this field followed by other big players like Samsung, Nokia, Sony, LG, Apple, Senseg, etc. Engineers keep experimenting with electrotactile stimulation for commercial purposes. Disney, Senseg, Nokia or Apple, are pushing haptic feedback with various levels of success by their own iterations.

Most current  solutions are glove-free. Also, no specialized back-pressure-sensor-like devices are necessary. The technology delivers such a small amount of current into the user's body that it poses no health risk, which could open up fast adoption of technology and corresponding “killer apps”. 

Key challenges for Augmented Sensing will be reducing the size of haptic actuators while maintaining bandwidth, force output, range of motion, and important degrees of freedom; powering haptic actuators in mobile and wearable devices; and enabling wireless communication with and control of haptic devices.

Kinesthetic haptic devices have yet to find their “killer app,” since the high-degree-of-freedom devices that are most compelling are expensive and not amenable to commercialization.

Availability and expressiveness of hardware. Commercially available actuators designed exclusively for haptic output are limited, which leads many researchers to develop their own haptic devices using off-the-shelf components.

Popular Augmented Sensing applications for researchers have been medical simulation, rehabilitation, and computer-aided design. Attempts have also been made at developing force-feedback joysticks for gaming applications, but the use of kinesthetic feedback in these fields has not yet achieved widespread commercial success. A common application of kinesthetic feedback in a consumer product is in the dashboard control/navigation system in some high-end cars.

Teleoperation of robots that are remote in distance or scale can enhance human performance or keep humans safe when performing tasks in dangerous environments. Applications include situations in which remote operation of a robot is desirable owing to challenges in access (especially distance and danger) and large differences in scale relative to typical human manipulation.

Disney's system, called REVEL, imparts a low-level signal, creating an oscillating, weak electric field around the user’s skin. Signal variations correspond to texture variations in the distant object. The signals are generated in such a way that the resulting sensation in the fingers mimics the sensation of sliding the fingers over the object. No gloves or specialized back-pressure-sensor-like devices are necessary. 

-------------------

MIXED REALITY(HA)

Mixed Reality (MR) is the merging of real and virtual worlds, where physical and graphical objects appear to interact and integrate naturally. MR includes an underlying group of technologies encompassing the spectrum of immersive displays and interactive systems that spans from the digitization of real environments, to augmented reality (AR) and virtual reality (VR). MR is an overarching technology that includes all immersive displays (mainly, head-mounted displays [HMDs]) and combines functionality that spans from the displaying of fully immersive virtual worlds, to ones that are augmented with matching graphics and overlays. 



Potential use cases are training on use of equipment, entertainment experiences in which the user directly interacts with the content, maintenance of equipment using visually integrated instructions, or in-field service environments.

Improvements in device technical constraints regarding size, power consumption, and heat generation, especially for standalone devices (i.e. HoloLens)
5G will bring radical changes in MR as higher speed and lower latency will enhance user’s experience and the efficiency of MR applications.

Complex rendering used in MR demands more processing power than Augmented Reality, this is a key reason why Mixed Reality applications and devices are still in the proof-of-concept phase and far from consumer availability.

Integration with existing infrastructure and security paradigms can kill a potential MR project before it gets off the ground without proper planning and integration specialists, especially in industries such as manufacturing

The value of real-time data collection and analysis will create pressure to deploy user-efficient systems and make the data available for MR use.

MR will become a widely used human-machine interface technology in the coming years. Just as keyboard and mouse gave way to touch, future user experience technologies will integrate much deeper with how humans naturally interact with the real world. In similar situations in which AR or VR could be used, MR technologies will likely take a front seat as the preferred technology because it better integrates real-world objects and their virtual counterparts.

During the next 10 years, MR and the user experiences that it enables will undergo a fundamental change above and beyond the capabilities of AR and VR. Today, MR capabilities focus on optimizing "hands-busy" work environments, and over time, MR will expand to include many types of experiences that can visually enhance everyday objects.

New business models will emerge that change how customers buy products using MR or how they conduct operations by visually connecting the user's view of the real world with their virtual world counterparts. Once MR technology matures, its adoption will accelerate due to the transformative nature of its user experiences.

The mixed reality market was estimated to be $46.8 million in 2017 and is estimated to reach $3.68 billion by 2025 at a CAGR of 72% from 2018 to 2025. The primary driving factors for the mixed reality market is its widespread adoption by different industrial verticals, especially the manufacturing industry. [ BIS Research ]

-------------------

VIRTUAL AGENTS(HA)

Virtual Agents (VA) are software that are capable of complex reasoning and direct interaction to support humans and act on their behalf, enabling them also to augment time spent on more creative and productive activities. VA can advise users about the state of business processes, make recommendations and provide answers, and take action based on perceived user need. 

Virtual agents allow for efficient service interaction for users that is faster and less expensive than traditional methods.
By integrating with APIs, virtual agents can leverage dynamic data and knowledge management solutions to provide personalized solutions to users.

Today, virtual agents can be accessed via mobile/voice interfaces, messaging apps, kiosks, IoT devices, physical robots, social networks and traditional web channels. Over time, more natural interfaces for end-users will drive growth in the market.

Virtual agents can deliver analytics that show the positive and negative outcomes and factors of each individual customer engagement.

Virtual agents require continuous modifications and updates. Virtual agents require consistent end-user feedback in order to be effective.
Virtual agents are useless if  they are leveraging a weak or incomplete knowledge base.

Customer pain points must be accurately identified to ensure the efficacy of the virtual agent.

Security is critical and will largely depend on trusted data and identities to deliver secure interactions for customers.

Virtual agents have witnessed an increase in growth/adoption due to advancements in the areas of NLP, NLG and machine learning. Most virtual agent applications are external and focus on the functions of customer service and sales. According to Forrester Research, 33% of US online adults who use smartphones, tablets, and computers say they use virtual agents. Growth will steadily continue. IDC has  predicted that, by 2020, 40% of commerce transactions will be enabled by cognitive/artificial intelligence personal shoppers and conversational commerce.

As research advances in NLP, NLG and machine learning so will the growth, adoption and capabilities of virtual agents- specifically for internal enterprise use cases. Internal use cases for virtual agents, if implemented correctly will offer improved employee productivity- therefore improving customer satisfaction and overall revenue.

In coming years, enterprises will more widely adopt and integrate virtual assistants with back-end systems such as CRM or ERP, and virtual assistants will find their place assisting employees with inward and outward functions. In some cases, the human maybe completely removed from the loop as virtual agents handle complex B2B transactions and negotiations.

-----------------
 

ASSET TOKENIZATION

Tokenization, the process of creating a token, is not blockchain dependent. Tokens and tokenization have been around for decades. However, the ability to apply the concept in a shared data/DLT/blockchain construct is unlocking tremendous new value. The most obvious manifestation is with ‘currency.’ But across businesses and organizations, this opens the door up to new digital assets or units deemed valuable by enterprises and the industries in which they operate. For example, the ability to create digital twins of objects (i.e. creating tokenized versions of real-world assets) is massively valuable.  This can enable more efficient, smart transactions across ecosystems, saving both time and money, while also creating the foundation for new products and services.

A token is a representation of value, assets, identity or information, as well as an output of the contractual agreements defined by the underlying company/institution (unilateral), industry (multilateral) or community protocol. Tokenization is the process of turning the tokens into digital assets. This digital representation can be placed on a blockchain to support more efficient transactions such as asset tracking or trading and selling with the use of smart contracts.

Enables easier liquidation and splitting of assets, delivering greater efficiency.
Unleashes new investment models, creating new ways for greater participation in the creation of products and services.

Tokenization combined smart contracts can reduce costs and transaction time by eliminating the need for unnecessary intermediaries (or augment the use of intermediaries for high-value efforts.

The ability to create digital twins of objects through tokenization will enable new products and services, while improving the consumer experience.

Regulatory and legal uncertainties of tokenization in certain industries will be a barrier to implementation
Technical challenges with ensuring standardization/consistence of on-chain tokens and corresponding off-chain assets (for asset-backed tokens).

-------------

ETHEREUM

Ethereum enables the creation of completely automated (if not autonomous) digital applications and businesses, with significant advances that allow the technology to be leveraged by public users or private enterprise.

The Ethereum code base has been adapted by enterprise clients, allowing for the creation of permissioned blockchains with smart contracts (more information available in the enterprise blockchains profile).
Ethereum is an open-source, public, blockchain-based distributed computing platform featuring smart contract (scripting) functionality. This added smart contract functionality allows users to easily build applications, organizations or even cryptocurrencies on top of the Ethereum network. It is also the core differentiator between Ethereum and Bitcoin, or other public blockchain networks.

Characteristics
 
Decentralized platform: the applications run exactly as programmed without any possibility of downtime, censorship, fraud or third-party interference.

Smart contracts: aim to provide greater security than traditional contracts and bring down the associated costs.

Custom built blockchain: an enormously powerful shared global infrastructure that can move value around and represent the ownership of property.

Enterprise Clients: Ethereum code base has also been adapted by enterprise clients, allowing for the creation of permissioned blockchains with smart contracts.

Enables the creation of completely automated (if not autonomous) digital applications and businesses, the potential next wave of disruptors
Development of cryptocurrencies and enabling ICOs (initial coin offerings), effectively the origin of the crypto investing boom
Significant advancements allow for the technology to be leveraged by public users or private enterprises

Yet to demonstrate successful production level applications
The DAO attack in 2016 highlighted the risk of development without guardrails
Inability to rollback transactions
Demonstrating the potential risk and value of a hard fork to solve issues
Difficult to scale and gain widespread adoption.

Although Bitcoin is the most adopted and proven blockchain platform in production, the Ethereum platform has recently gained credibility and attention. Ethereum is seeing adoption of both its technology and crypto asset and is seeing increased engagement on open sourced and enterprise projects.

Accenture is helping clients develop on enterprise grade instances of the Ethereum network, and helping to set the standards for development as a founding member of the Ethereum Enterprise Alliance.

The rise of Ethereum has helped accelerate an immense amount of research from academia and international corporations, bringing the blockchain opportunity to a multitude of industries beyond just payment transactions. Companies are adapting and advancing the core technology to suit their needs. While some are exploring digital currency and the open-source, free-for-all ecosystem of public blockchains (of which Bitcoin and Ethereum are prime examples), far more are concentrating on how the technology underpinning those systems can add value to their businesses. Many are concocting “permissioned” or “private” blockchains, designed for a more centralized architecture where only authorized operator can join.

--------------

BITCOIN


Bitcoin is an open-source, peer-to-peer, digital decentralized cryptocurrency that can increase the speed of transactions and decrease overall cost compared to traditional payment methods. Bitcoin remains the most mature and wide-scaled application of blockchain technology. Bitcoin is an open network and has a high degree of resiliency and security.

Bitcoin, built on blockchain technology, is a cryptocurrency and worldwide payment system. In 2017, Bitcoin has gained publicity and media attention as a cryptoasset, able to be traded on cryptocurrency exchanges. However, the technological value remains as the introduction of blockchain technology in 2009.

Characteristics
 

First cryptocurrency: created by the legendary Satoshi Nakamoto to prove the Blockchain technology
Decentralized digital currency: the system works without a central bank or single administrator
Peer-to-peer network: transactions take place between users directly, without an intermediary
Immutable transactions: verified by network nodes through the use of cryptography
Publicly available transactions: recorded in a public distributed ledger called a blockchain
Solves for double spend

Open network with minimal barriers to entry, allowing both individuals and corporations to easily participate
Solves the trust issue between two parties transacting due to its peer-to-peer validation system.
High degree of resiliency and security, as its decentralized and redundant nature prevents single point of failure attacks and the cryptography provides a high degree of protection for the data and transactions
Decrease in overall cost compared to traditional payment methods
Recent advancements in the lightning network,  a payment protocol layer on top of Bitcoin, increases  transaction volumes and begins to address bitcoin scalability issues.

Lack of understanding of the key business implications as an asset or coin
Limited mainstream growth
Scaling transaction speed (latency) and storage
Highly intensive compute process (called mining) to validate transactions
Difficult for the casual user
Uncertain regulations contribute to stifled mainstream growth
Accessible to everyone with zero transaction privacy and does not align with enterprise requirements. 

Bitcoin remains the most mature and wide-scaled application of blockchain technology, having been in production for eight years with a near-flawless operational track record with regard to the foundational protocols.

Bitcoin has achieved considerable adoption, with an approximately 420 million transactions since July 2018 and spanning an average of 467,000 unique addresses. Bitcoin’s popularity is also helping to drive user growth across blockchain wallet, now  reaching an  estimated 39 million users globally.

Mainstream companies like PayPal, Dell, Overstock, Stripe, Square and Expedia are providing mechanisms to enable bitcoin transactions.

Other financial (JPMorgan and Fidelity) and tech companies (Facebook) have recently entered the crypto space, including AT&T which has started to accept Bitcoin as a bill payment option. These initiatives could create more demand and help encourage greater adoption of bitcoin as well as other cryptocurrencies and digital assets.

However, bitcoin transaction volumes are still a small portion of the global economy and have slowed down as the value of the currency has increased. As speculation on Bitcoin as an asset increases, it casts doubt on the application of Bitcoin as a payment protocol.

For enterprises, the transparency and speed (albeit improving) of public blockchains such as Bitcoin, currently do not meet enterprise requirements. Private enterprises require confidential transactions and system accountability. Therefore, enterprises are adopting  private blockchains or distributed ledgers, to enable permissioned user access and ensure roles and responsibilities are clear for issue resolution (review DLT/Blockchain slide for more information).



-------------------

BLOCKCHAIN PAAS

For existing and new business ecosystems the table stakes to experiment with and test new DLT applications are reduced by Blockchain Platform as a Service (bPaaS). bPaas is a set of Blockchain software services  offered to subscribers on the cloud by a vendor. It currently offers the easiest go-to market blockchain strategy for enterprises, leveraging current cloud providers’ tools and neatly incorporating into existing architecture.

Blockchain platform as a service (bPaaS) is a set of blockchain software platform services that includes some or all of the distributed ledger, node or consensus mechanisms, and other ancillary services to manage a network of distributed ledgers on the vendor's cloud infrastructure.

Characteristics
 
Set of blockchain software platform services: offered on the cloud by a vendor for subscribers.
Services include: distributed ledger, node or consensus mechanisms, and other ancillary services to manage a network of distributed ledgers on the vendor's cloud infrastructure.

bPaaS offers the easiest go-to market blockchain strategy for enterprises, leveraging the security, infrastructure, and advanced tools offered
Enterprises can leverage current cloud providers tools, easily incorporating into existing architecture
bPaaS platforms will make it easier for companies to both test and deploy distributed ledgers, allowing enterprises to blockchain networks in days instead of months
Advantage of using bPaaS systems is that the table stakes to experiment with and test new DLT applications for existing and new business ecosystems are reduced.
bPaaS eases the management of decentralized networks for its users

Interoperability for distributed ledger across competing platforms is at early stage of evolution.
Enclosed to infrastructure services only at present
Uncertain market direction because of its vague operation and DNA consensus protocols.

Blockchain PaaS offers the easiest method for enterprises to experiment and develop with blockchain technology. They can leverage existing relationships and develop in familiar environments with minimal risk. While blockchain PaaS offerings are in the early stages, we expect rapid development as cloud providers use blockchain offerings as a way to attract and retain enterprise customers.

Due to the rapidly evolving landscape, platforms are likely to undergo major changes over the next two to five years, creating concern for any enterprise planning to lock into one offering at this stage. Enterprises should capitalize on the easy toolset and development opportunities to increase their blockchain capabilities. They should also run evaluations of multiple platforms and test each according to their requirements.

BPaaS eases the management of decentralized networks. Recent cloud-service offering from enterprise cloud vendors (e.g. IBM Bluemix and Microsoft Azure) — and small technology startups — such as BlockApps, BlockCypher and WanCloud.

Each bPaaS vendor attempts to add unique elements around security, interoperability, analytics and performance to differentiate their offerings.

The underlying DNA of DLT is that Blockchain will continue to evolve and enterprises should stay abreast of these developments and advancements. Rapid market use and adoption of a specific application suite and consensus proof protocol could result in market success for one or more of the bPaaS services.



-----------

INTEROPERABILITY

Most blockchains are technologically siloed or not interconnected. This is due to lack of standards regarding the structure of distributed ledgers and platform components. However, DLT  solutions cannot exist in a vacuum. This brings about the desire for interoperability, or the ability for blockchains to interact and connect to each other. The ability to link and interact in an end-to-end, seamless solution is required if companies hope to reap the full value of DLT.

The blockchain platform market is starting to stabilize, but interoperability and integration standards are still not yet established and current platforms are still undergoing major changes.

To enable a blockchain ecosystem, standards are required on public encryption, digital signature, hash algorithms and consensus protocols.

Interoperability is the ability of two or more components or systems to exchange information and to use the information that has been exchanged.

Characteristics
Syntactic Interoperability: Where two or more systems are able to communicate and exchange data. It allows different software components to cooperate, even if the interface and the programming language are different.
Semantic Interoperability: Where the data exchanged between two or more systems is understandable to each system. The information exchanged should be meaningful, since semantic interoperability requires useful results defined by the users of the systems involved in the exchange.


Organizations are exploring the use of APIs in a number of domains for interoperability and modularity in complex systems.
Second generation open source interoperability solutions (including Accenture-owned capabilities) are currently underway for permissioned systems to engage with two or more DLT platforms.
Blockchain Platform-as-a-Service (bPaaS) efforts can potentially enable an interoperable blockchain ecosystem as they aim to be protocol agnostic.
Potential solution is sidechains. The sidechain concept has evolved into developments on payment and state channels.


Lack of potential solutions since there are very few blockchain instances running in production.
Short cut solutions demands introducing standardized APIs to enable interoperability on the blockchain, running contrary to the principles of shared ledger features (eliminating data reconciliation).
The competitive dynamics of DLT platform providers and the accelerated pace to scale, hinders effective collaboration across DLT solutions
Blockchain interoperability benefits can be challenging to demo to business stakeholders.

Although third parties are working to implement interoperability mechanisms, efforts at establishing blockchain interoperability are still at a very early stage. The International Organization of Standardization and the International Securities Association for Institutional Trade Communication have noted this as a high importance area and established working groups to address these challenges.

Many consortia have also acknowledged this future issue, hoping to design standards in advance of the majority of enterprise development. Interoperability between different consensus mechanisms, tools, frameworks and smart contract capabilities remain limited but a key challenge to address prior to mainstream development.

While some forms of interaction and communication are possible today, no best practice has yet emerged that achieves ‘interoperability’ and eliminates the use of messaging (and data reconciliation) such that we can maintain the efficiencies and simplicity that drives blockchain value.

We expect the consortia to lead the way with standard setting, and major blockchain PaaS providers to be heavily engaged in the discussion. Lines are expected to blur as features and functionality become more interchangeable, and interoperability and protocol/data standards become more prevalent. There will likely be a convergence in private and public architectural deployment styles, with all distributed ledgers having similar functional characteristics. According to Gartner, "market differentiation will lie in the inherent capabilities of consensus mechanisms (for example, based on the efficiency of the distributed ledger), interoperability and user experience.”


----------------------

ENTERPRISE BLOCKCHAINS / DLT

The emergence of Distributed Ledger Technology (DLT) / Blockchain solutions offers a new approach to data that allows multiple stakeholders to confidently and securely share access to the same information via a shared ledger. This offers enterprises and governments and the ecosystems in which they interact promising, innovative applications to enable business transformation.

Distributed ledger technology (DLT) is a digital system for recording the transaction of assets in which the transactions and their details are recorded in multiple places at the same time. Unlike traditional databases, distributed ledgers have no central data store or administration functionality. In addition to the distributed underpinning of the technology, it requires consensus mechanisms and protocols to make changes to the data. DLT/Blockchain features also enable an audit of transactions (provenance), tamper evidence for data integrity, and smart contracts.


Deployment Patterns

Central Infrastructure Leader. This model is where an existing operator of an ecosystem governs a platform on behalf of a set of participants

Consortium. This type of blockchain is managed by multiple trusted organizations and to get access, there needs to be consensus by multiple participants in the consortium.

Shared ERP. This early model is where existing large platform providers such as ERP systems that have traditionally managed their business as an install base, with separate implementations at each client, are realizing that they can add a shared data capability and convert their install bases into networks.

Opportunity to create and sell new products and services to an ecosystem.
Augment intermediaries role to offer new high value services (vs simply passing data)
Early developments are being realized across many different industries, ranging from financial services, supply chain, energy, food, and identity etc.
Vertical Solutions and Consortia have formed to help understand, develop and implement use cases dependent upon cross-industry and competitor partnerships.

Unable to leverage the network effect and digital currencies of public blockchain networks
Difficulty connecting between two different blockchains or connecting blockchains to existing shared systems.
Development and management costs must be covered by the participating institutions
 
 
 The ability for enterprises to leverage the benefits of blockchain technology without the risks and downsides of public, permissionless blockchains has led to the rise of enterprise applications with Distributed Ledger Technology. While Financial Services led the way, nearly every industry is testing and/or adopting the technology with supply chain and identity cases having the most momentum.

Most attractive to enterprise is the ability to permission networks, providing a means for controlled standards development. They also provide a mechanism to edit or modify data in accord with evolving regulatory standards, and correct errors as necessary. Many new features not existing in the original blockchain are now possible/available: confidentiality, performance, simplified consensus, permissioned/private etc.

The key benefits around enterprise applications stem from the evolution from public blockchains to permissioned DLT. Early developments are being realized across many different industries, ranging from financial services, supply chain, energy, food, etc. already solid enough for enterprise adoption.

The picture will continue to evolve in the coming years, with new solutions entering the market, consolidations and failures etc. Nevertheless, solutions are already solid enough for enterprise adoption. Careful consideration must be used to evaluate potential use cases to ensure proper fits. Similarly, selecting the right solution for the test case is pivotal to a successful implementation.

-------------

CRYPTOCURRENCIES (AND ICOS)

Cryptocurrencies intend to make it easier to transfer funds directly between two parties in a transaction, without the need for a trusted third party (e.g.  bank or credit card company).

A cryptocurrency is a digital asset designed to work as a medium of exchange that uses cryptography to secure its transactions, control the creation of additional units and to verify the transfer of assets. Bitcoin was the first cryptocurrency, though there are now more than 2200 in existence. To distribute cryptocurrencies, companies often hold Initial Coin Offerings (ICOs) as a way of raising capital.

A cryptocurrency is a digital currency in which encryption techniques are used to regulate the generation of units of currency and verify the transfer of funds, operating independently of a central bank.


Cryptocurrencies can be divided into three groups: 

Digital currencies have no intrinsic value, but exist as a store of ‘accepted’ value (e.g., Bitcoin)
Utility tokens or app coins provide users with future access to a product or service (e.g., Filecoin)
Tokenized securities provide owners with a share of a company or business (e.g. tZERO)

Enables crowdfunding of projects with immediate ownership or value provided to the investor
Creating a new asset class and store of value (from land to metals to paper currency to electronic currency)
Creates wide-range of decentralized networks and products.

Consider legal or regulatory requirements that protect against financial crimes
Regulate the stability of accounting and financial markets liquidity of the products and services exchanged
Establish viable cryptocurrency exchanges
Set up transactional mechanisms for clearing and settlement
 
 
While cryptocurrencies and specifically the investment opportunity enabled have drawn the most attention in the blockchain ecosystem, cryptocurrencies have minimal impact on Accenture or our clients. As most cryptocurrencies exist on public blockchains, early enterprise use cases will have little alignment (see Enterprise Blockchains or Blockchain as a Service for more details).

As the cryptocurrencies market develops, it will be interesting to watch progress over the next few years, especially with introduction of Facebook Libra (see use case example on next slide). The bull case for cryptocurrency would include highly adopted, decentralized applications that consumers and corporations would be leveraging. One example is Storj as an alternative to AWS. While this is not an immediate consideration, our customers should be cognizant of this potential outcome.

Of note, Gartner has made 3  predictions around cryptocurrency adoption:

By 2023, two countries will have seen their national election campaigns won by a nonmainstream party funded through cryptocurrency.
By 2025, public blockchain will provide a core interoperable foundation for global, decentralized identity management.
By 2023, at least one country with hyperinflation will use cryptocurrencies over its local currency.

--------------

DISTRIBUTED EXECUTION / SMART CONTRACTS

Distributed execution, commonly referred to as “smart contracts”, is a new way to improve complex, multi-stakeholder transactions where changes must be verified by numerous parties across siloed systems. It lowers cost by the elimination of third parties. While smart contracts gained prominence through the Ethereum blockchain, alternative distributed ledger technology is starting to incorporate the technology.  It is an event-driven program which runs on a distributed ledger and can execute code in conjunction with an action upon a digital asset. Distributed execution can be seen as embedded business logic within the blockchain. Distributed execution are Logic-driven, simplified contracts to provide a clear statement of events, actions and triggers.

Distributed execution is composed of a programmable contract defined within computer code which can facilitate, verify, execute and enforce the negotiation Distributed execution can be seen as embedded business logic within the blockchain. Easily written in standard programs and coding languages, Distributed Execution increases auditability of contracts by legal and compliance and prevents contract tampering or change. While smart contracts gained prominence through the Ethereum blockchain, alternative distributed ledger technology is starting to incorporate the technology.

Embedded business logic: within the blockchain.
Composed of a programmable contract: defined with computer code which can facilitate, verify, execute and enforce the negotiation or performance of contract terms automatically on a distributed ledger.

Logic-driven, simplified contracts to provide a clear statement of events, actions and triggers
Lower cost contracts by the elimination of third parties
Increased security preventing contract tampering or change
Increased auditability of contracts by legal and compliance
Automated action triggering (such as payments) based on contractual clauses
Often easily written in standard programs and coding languages


Inability of the owner of a smart contract to revoke the program
Premature phase of Technology makes it not available on all blockchain platforms
If not secured and receptive, programming languages will lack ability to implement smart contracts.
Deploying Smart Contracts is complex, costly and risky without exhaustive tests.

Distributed execution is typically coupled with "smart assets" enabled by AI and IoT. Smart assets can be viewed as digital property that has embedded within it a concept of ownership as well as dynamic behaviour.

Distributed execution will have an impact on all industries and public-sector organizations, as it changes how decision rights are provided and how decisions are enacted. This will help drive the programmable economy in which distributed execution facilitates, verifies or executes the terms of contracts – therefore enabling algorithmic businesses and decentralized autonomous organizations. Distributed execution gives organizations the potential to eliminate complex legal documents and automate business processes. According to Gartner, by 2021, less than 2% of global organizations will have adopted complex (bundled) smart contracts, yet 20% of organizations will be subject to them. Moreover, by 2022, ratified unbundled (that is, defined impact) smart contracts will be in use by more than 25% of global organizations.

Today, distributed execution programming is an immature subsystem of the blockchain ecosystem. The industry is still looking for a scripting language that is "Turing complete," as well as easy to create and provably correct. Organizations are looking at proven methods of using distributed execution to automate complex agreements in a trust less runtime environment.

-----------
------------------

VIRTUAL AGENTS


Virtual Agents are software capable of direct interaction to support humans and act on their behalf, with increasingly complex reasoning. They bundle advanced analytics, machine learning and Natural-Language Processing (NLP) to support customers, partners or employees. Growth of Virtual Agents is due to advances in the areas of AI, NLP, Natural Language Generation (NLG) and Natural Language Processing (NLP), and machine learning.



A virtual agent is a contact-centre-type capability that uses a virtual character, created through natural language understanding and generation, animation & artificial intelligence, acting as a virtual  service agent.


Ability to deploy Virtual Agent across contact channels (web, mobile, social, contact centre, SMS, instant messaging platforms, IVR, physical robots)
Backed by powerful dialog and knowledge management systems

Real-time, actionable reporting that combines voice of the customer feedback with conversational and predictive analytics & supervised machine learning
Flexible options to integrate with the systems and software important to your business.

Efficient service interaction that is faster and less expensive
By integrating with APIs, Virtual Agents can leverage dynamic data and knowledge management solutions to deliver personalized solutions
Enable consistent service levels and shared customer context across traditional web channels, mobile/voice interfaces, messaging apps, kiosks, IoT devices, physical robots and social networks

Utilize analytics that show the positive and negative outcomes of each customer engagement
Virtual agents are increasingly becoming part of larger offerings. ERP, CRM Vendors and customer service providers (CSPs) increasingly incorporate both middleware and core conversational platform technology into their offerings.

To ensure efficacy, customer pain points must be accurately identified and targeted – today, Virtual Agents are only effective within limited scopes
Out-of-the-box support for specific industries is still limited, especially if you also need to support other languages than English 
Requires continuous modifications and improvement, which requires consistent end-user feedback

Useless if leveraging a weak or incomplete knowledge base
Highly dependent on customer trust, thus critical to use only trusted data and identities

Virtual agents are not a new technology and in fact have been around for more than 50 years. Today, virtual agents are primarily single-purpose and the scope of their function is limited, while adoption increases thanks to advances in NLP, NLG and machine learning. 

Also, virtual agents are increasingly becoming part of larger offerings, i.e. ERPs and CRM, hence in coming years, enterprises will more widely adopt and integrate virtual assistants with their back-end systems and virtual assistants will find their place assisting employees with inward and outward functions. In some cases, humans may be completely removed from the loop as virtual agents handle complex B2B transactions and negotiations. 

RPA platforms are also being directly embedded with ML and AI-driven chatbots. Vendors like NICE and Pegasystems have chatbots that apply text analytics to agent-assisted interactions to help judge the customer’s true intent.  Proprietary virtual agents are also sold by some of the service providers as an element included in their broader customer experience offerings (i.e. Teleperformance, Genesys).   

Most virtual agent applications are external and focus on the functions of customer service and sales. According to Forrester Research, 33% of US online adults who use smartphones, tablets and computers say they use personal virtual agents, driving adoption in business contexts as well. IDC has predicted that, by 2020, 40% of commerce transactions will be enabled by cognitive/artificial intelligence personal shoppers and conversational commerce.                                                                           

As research advances in NLP, NLG and machine learning, so will the growth, adoption and capabilities of virtual agents- specifically for internal enterprise use cases. If implemented correctly, these cases will offer improved employee productivity- therefore improving customer satisfaction and overall revenue. 

Straightforward implementation will be more common as the technology matures. For example, in December 2019 Microsoft has announced Power Virtual Agents – an offering that enables anyone to create chatbots using a guided, no-code graphical interface, without the need for data scientists or developers. 



----------------

AI-ENABLED AUTOMATION TOOLS

With Robotic Process Automation (RPA) as the foundation for automating routine processes, enterprises can orchestrate the automation of complex tasks that require little or no human oversight by applying AI advancements such as machine learning, natural language processing, speech recognition and image recognition. By integrating a continuum of technologies, enterprises  manage and optimize processes.

Today, robotic process automation (RPA) is the first step for most organizations in implementing process automation solutions.

The integration of analytics and machine learning to RPA will drive the growth of AI-enabled process automation, able to optimize the process and to make reasonable decisions in ambiguous situations.
 

Dynamically manage and optimize processes through learning from feedback
Handle more complex processing autonomously
Make processing decisions contextually
Processing decisions will be made contextually and deliver more accurate predictions
Perform tasks that previously required humans, possibly exceeding human performance levels

By applying machine learning to RPA, bots will be able to handle complex processing autonomously
Enterprises will be able to automate complex processes that require a deeper level of analytics involving both structured and unstructured data, leveraging multiple sets of data and knowledge
Bots will be able to learn from historical data and predict and prevent anomalies, eliminating time, effort and cost needed to handle them
Advances in NLP will help systems automatically convert human inputs in the form of voice or text into machine-readable instructions
Software will be taught rather than programmed, moving to unsupervised learning in the future

Realizing business benefits is dependent on finding use cases where "mostly correct" processing is sufficient
AI-enabled automation tools are not a clearly defined product category, making implementations mostly experimental.

Robotic Process Automation (RPA) is “low hanging fruit” for firms looking for early wins in automating time-intensive manual tasks.

The integration of analytics and machine learning to RPA is what is driving and will continue to drive the growth of AI-enabled process automations. The fusion of analytics and machine learning with traditional automation solutions will improve the pace and scale of complex problem solving, risk analysis and business decision-making at an enterprise level, while enabling employees to achieve significant productivity gains—as much as 30-40%—even in functions that are already automated. AI automation spending is estimated to surpass RPA by 2021, at $2.7 billion vs. RPA at $1.2 billion.

According to IDC, the major RPA vendors have introduced marketplaces that extend capabilities through partnerships that include AI and are integrated with these AI-based solutions. They are also building their own AI capabilities and packaging them together with RPA to automate discrete process roles.     

Advances in machine learning will be critical for solution maturation. According to Gartner, by 2021, AI-related services will enter mainstream adoption, with 30% adoption by large companies. Most service providers will use intelligent automation service techniques, lowering the annual cost of commodity services by 15% to 25%.

-----------

ROBOTIC DESKTOP AUTOMATION (RDA)

Robotic Desktop Automation (RDA) is a type of automation that focuses on increasing the productivity of individual users by automating and integrating processes on the desktop. It requires human triggers and works through the User Interfaces or programmable interfaces of applications.

Similarly to RPA, RDA will increasingly include AI-enabled features by leveraging AI and ML. Those will i.e. include identifying patterns based on customer behavior, improving decision making (next-best-action), and conversational interfaces.

RDA (or attended RPA) is automation that focuses on increasing the productivity of individual users by automating and integrating processes on the desktop. RDA is especially useful when a user must be involved, and must deal with multiple systems, duplicated data entries and older legacy systems not connected or only loosely integrated. Typically, RDA is used to automate some processes alongside the user's regular, manual use of the same applications.


Automation through simulating user actions on the application UI, or through programmable interfaces provided by the application
Augments regular use through cross-application process shortcuts
Low cost
No special infrastructure
Fast implementation, even by users
Does not require changes to applications
Automation of multiple processes in a single project
Consistency across users requires an automation distribution solution

RDA is a mature technology that is relatively simple to implement and offers quick results with a low set-up cost
RDA brings immediate savings, especially in standard interactions-heavy B2C industries
RDA is especially useful when users must deal with multiple systems, duplicated data entries and older legacy systems
RDA can be a great door opener for RPA and further digital transformation, not just a band-aid on legacy corporate inefficiencies


Automation is limited in terms of throughput as it is dependent on the user
RDA automations can be fragile as they are sensitive to underlying application changes and poor data quality
Must target established, high-volume, repetitive and non-ambiguous processes to create significant value
Deploying consistent, high-quality processes across a large number of user desktops is challenging as desktop environments are notoriously hard to standardize
Competes in the same space as user-assisting Virtual Agents, creating c

RDA market is counted by analyst companies as a part of rapidly growing RPA marketplace. According to Everest Group, the global RPA independent software vendor market surpassed US$ 1billion in 2018, with over 2/3 of the revenue coming from software licenses. RDA licenses accounted for over 30% of the total RPA software market. While growth is being driven by both unattended and attended RPA (RDA) market segments, RDA witnessed increased traction in 2018 (RPA software market noted 90-100% YoY growth, whereas RDA – 12-130%).

RDA has evolved from screen scraping in the front office where disparate sets of information were integrated to support call centre agents. Large number of processes flowing across various unconnected domains (back-office ERP, CRM, Billing, Core Banking systems etc.) await automation. RPA-related engagement carries a relatively low cost of licenses and implementation and offers quick results.

RDA is focused on swift deployments on desktop level, typically customer contact agents. In RDA, activities are being automated, while in RPA (back-office) processes are automated. Leading RDA providers like NICE and Pega offer both RDA and RPA solutions. More and more RDA solutions are being sold into back-office operations, further blurring the line between RDA and RPA, with the first becoming rather an RPA functionality than separate line of software. RDA’s process of maturing will take the simple robot tasks to be enhanced with virtual agent interaction, ML-based exception frameworks, more sophisticated human and robot interactions, and real-time guidance for humans.

---------------

ROBOTIC PROCESS AUTOMATION (RPA)


Robotic Process Automation (RPA) is the use of an on-server application to replicate the activities involved in the interactions between a human being and a UI interface of a computer. Main feature of RPA is the ability to repeat the actions of a human user at high volumes and no errors. It is cost effective and improves productivity, compliance and auditability. RPA can be easily configured (code free) with minimal disruption to existing IT systems. As core RPA capabilities are mature, the software is increasingly being infused with AI, i.e. text analytics, natural language understanding, and conversational intelligence to help companies expand the automation use cases outside the isolated tasks.

Robotic process automation (RPA) is the application of technology that allows employees in a company to configure computer software or a “robot” to use existing applications for processing a transaction, manipulating data, triggering responses and communicating with other digital systems.



RPA is useful for automating repeatable tasks that can be captured in a detailed step-by-step process and executed without user intervention
Process execution simulates a human user using an application through mouse clicks and typing
User-friendly configuration tools, no coding required in most cases
Non-disruptive, requiring no changes to the applications used

Drive efficiency benefits, along with improvements in quality, scalability and resiliency in a cost-effective way with potential investment payback in up to 3 months
Improve productivity with average handling times reduced by up to 40% and 24/7 monitoring, and overall throughput increased up to 6-7x
Eliminate human errors and improve compliance/auditability
Reduce processing costs by up to 80%
Efficiently implement RPA across processes with re-usable process elements
AI-based capabilities will grow the value RPA delivers, i.e. by adding next-best-action recommendations, intelligent workload balancing, predictive SLA monitoring, and IT infrastructure monitoring.


Common standards across the enterprise can become fragmented and difficult to impose
Centralized change and automation delivery management can be challenging to implement across business silos
Repeatability and volume requirements of the tasks to be automated can be surprisingly high
Need very stable applications, as surprisingly small changes to the underlying applications' interfaces or their data may break the automation
Rapid RPA implementations must be supported by proper governance and security structures
RPA may reduce the pressure to change the legacy applications and hence indirectly contribute to growing technological debt.

RPAs are more relevant for high volume, highly transactional process functions such as: purchase order issuing, creation of online access credentials, or business processes that require access to multiple existing systems. RPA can be easily configured (code free) and with minimal disruption to existing IT systems. With no application integration required, simplified training, scaling, and auditing, RPA offers low-risk, high-value automation.

RPA is a mature technology that is seeing rapid adoption by organizations as they look for ways to cut costs and quickly link legacy applications together. More than 70% of organizations will adopt RPA solutions over the next two years.

Forrester expects RPA software market to reach US $4.2bn in 2023, up from US $1.7 bn in 2019. RPA services market totaled US $3.9 bn in 2018, and will be more than triple by 2023, reaching US $12 bn.

Finance & Accounting, Customer Care, and Help Desk are seeing more demand for RPA. ROI from RPA is entirely dependent on the applicability of each use case in organizations.

Although RPA initially focused on high volume, rules-based processes, the future of the technology is tied to intelligent automation or fusing RPA with machine learning/cognitive capabilities. Consequently, bots will become self-learning and will be dedicated to complex functions.

According to IDC, by June 2019, the major RPA vendors have introduced marketplaces that extend capabilities through partnerships that include AI and are integrated with these AI-based solutions. They are also building their own AI capabilities and packaging them together with RPA to automate discrete process roles.

------------

BUSINESS PROCESS MANAGEMENT (BPM) SUITES

Business Process Management (BPM) provides companies with an integrated approach to process management from design to execution and control, using process models, modeling software, and repositories.
The primary goal of BPM  today is to drive digital transformation, passing the traditional focus on cost reduction in the enterprise.  While core BPM capabilities are mature and stable, BPM is ageing as a standalone product – vendors are increasingly competing on value-added elements, i.e. integrated developer environments (IDEs) for API connectivity, UI creation, organizational models, task design, and reporting which enables to construct full and complex process applications.




Business process management (BPM) is the discipline of improving an end-to-end business process by analyzing it, modelling how it works in different scenarios, executing improvements, monitoring the improved process and continually optimizing it.

Business Process Management Suites (BPMS) are both design-time and run-time applications that give users model-driven tools to design and orchestrate the execution of business processes and business rules.


Business Process Management provides transparency, increases efficiency and improves agility
Business Process Management helps to coordinate people, machines and things
Business Process Management can also give organizations the tools and skills they need to quickly respond to marketplace developments and capitalize on emerging opportunities before competitors

Intelligent Business Process Management Systems (iBPMSs) are evolving to meet the needs of dynamic/real-time processes.

BPM enables effective enterprise performance through improvements of specific work activities within a particular department, across the enterprise, or between organizations.
Enables organizations to standardize, optimize, automate & innovate business processes.
Can yield significant quantitative & qualitative results through a more agile connection between people, process & technology.
BPM helps to establish process governance & management.

Cross-organizational and cross-functional BPM efforts make it difficult to ensure ownership and ongoing focus.
Broad skill needs, insufficient training and lack of career path guidance make business process management roles challenging to staff and execute.
Complex and expensive BPM software due to the reliability and performance requirements of critical processes.
New interfaces like voice and chat place new demands on the BPM software vendors.
Core BPM capabilities are mature and stable, and BPM is ageing as a standalone product. Vendors are increasingly competing on value-added elements (iBPM suites).



Traditionally, BPM platforms have focused on orchestrating structured processes - those core capabilities are now mature and stable. However, the demand to address the need for non-routine work is increasing and can be augmented by some degree of automation. As a result, Intelligent Business Process Management System (iBPMSs) bring in capabilities such as validation (process simulation, including "what if" scenarios), verification (logical compliance), optimization, ability to gain insight into process performance, enhanced support for human collaboration, streaming analytics and real-time decision management.

With the increasing adoption by enterprise of lean/agile methodologies and digitally-driven frameworks the role of BPM is shifting towards that of digital process automation and vendors competing on capabilities which strongly broaden the traditional BPM functionality.

Advances in technology such as cloud computing, service-oriented architecture and software-as-a-service have created more flexible, configured application environments that can deliver tremendous value if properly used.

-------------

RULE ENGINES

Rule engines are valuable for applications where business rules usually change more often than the rest of the application. Therefore, rules are managed separately from the logic. Rules can be modified instantly as business conditions change, without editing the rest of the application, and can be shared between multiple applications. Rule engines automate decisions for rapid business actions. 

Rule engines’ core capabilities are mature and are not emphasized by vendors as a differentiation or market advantage. Those capabilities are increasingly delivered as part of broader Decision Management Suites (DMS) or modern, intelligent Business Process Management (iBPM) suites.

Allow professional and nonprofessional developers to rapidly build and maintain applications.
Business experts can easily write and maintain the business rules and logic in a business rule management system (BRMS).
Business rule management systems can help ensure that applications remain current and well-aligned with business objectives as market conditions change.
Automate decisions for rapid business actions, determining appropriate actions to be taken at a given time.
Express complex logic and actions in a language that is executable and that everyone can understand.
Make business decisions and processes transparent and auditable.

Growing rulesets become increasingly difficult and costly to maintain, requiring strong regression testing.
Rules are negatively impacted by unmanaged changes to integrated systems and data.
Complex rules often require coding to be expressed fully, reducing the benefit of investing in a rule engine.

Rule engines have been successful in organizations dealing with complex, often compliance-related rules and provide a useful resource for more advanced automation. In many cases, they can be used to replace the need for custom-built software. Rule engines can be purchased independently or come embedded in a BRMS. A rule engine enables IT and/or business staff to define rules using decision trees, decision tables, programming-like code or other representation techniques.

While rule engines and event processing systems have existed for decades (companies like ILOG SA, Blaze AdvisorTM and PegaSystems Inc. have been developing business rule engines since the late 1980s) and manage customer interactions, business processes, and applications, BRMS have received attention as an important software category to help automate businesses.

Rule engines’ core capabilities are mature and are not emphasized by vendors as a differentiation or market advantage. Those capabilities are increasingly delivered as part of broader Decision Management Suites (DMS) or modern, intelligent Business Process Management (iBPM) suites.

Rule engines are increasingly being embedded into broader automation platforms or iBPMS offerings – i.e. IBM Operational Decision Manager (ODM), the vendor’s business rules management system (BRMS), is now an element of IBM Digital Business Automation Platform. ODM is supported by the IBM Decision Composer, an automated, cloud-based low-code tool to model, author, validate, share and run business. Pega offers its business rules engine either as part of a stand-alone business rules platform, or as a core element of the business process management platform.

-----------------

DECISION MANAGEMENT SUITES(DMS)


Rule engines’ core capabilities are mature and are not emphasized by vendors as a differentiation or market advantage. Those capabilities are increasingly delivered as part of broader Decision Management Suite (DMS) or intelligent Business Process Management (iBPM) suites. DMS products have evolved beyond business rule management systems (BRMS) by providing better support for analytics and decision modeling – they allow to take data from multiple sources and develop analytics-based models to predict and react to specified business rules.

Decision Management Suites are a software product which supports conventional development and runtime tools for applications which include complicated decisions or frequently changing logic. DMS allows to take data from multiple sources and develop analytics-based models to predict and react to specified business rules, and measure the outcomes. Modern DMS are successors of Business Rule Engines (BRE), and can be used as standalone products or as part of iBPM suites.

Comprise several elements, including rule engines, analytics, and automation capabilities.
Embrace low-code approach.

Rules are usually expressed declaratively (if, then statements) or as mappings between values.
Can ideally be maintained without programming skills, enabling e.g. direct maintenance of discount rules by product management.
Can either be tied to an application program or run independently.


Business experts can easily write and maintain the business logic, rules, and decision flow and easily adjust those to changing market conditions.
Express complex logic and actions in a language that is executable and that everyone can understand.
Data analytics-based models enable predicting and measure the business impact of the settled rules and decision strategy.
Make business decisions and processes transparent and auditable.

Allow professional and citizen developers to rapidly build and maintain applications.
Automate decisions for rapid business actions, determining appropriate actions to be taken at a given time.


As Digital Decisioning Platforms combine rules management, analytics, and business process automation components, not all DMS vendors cover each segment equally well.
According to Forrester Research, few vendors pursue digital decisioning use cases as their primary mission – most of them focus on discrete parts of digital decisioning use cases.

Rules are negatively impacted by unmanaged changes to integrated systems and data.
Growing rulesets become increasingly difficult and costly to maintain, requiring strong regression testing.



Decision Management Suites (DMS) are successors of Rule Engines, enriched with strong analytic and automation capabilities. DMS products have evolved beyond business rule management systems (BRMS) by providing the ability to include data from multiple sources and develop analytics-based models to predict and react to specified business rules.

A mature mix of algorithms based on both rules and analytics models enables companies to develop mature decision models, implement business rules and decision flows, deliver context-aware predictions, and hence enter the ongoing cycle of improvement iterations.

Major DMS product vendors examples include: FICO, IBM, InRule, and Pegasystems. FICO and InRule are DMS-concentrated vendors, whereas IBM and Pega offer those software both as a standalone product, or as part of broader automation offerings.

--------------

INTELLIGENT BUSINESS PROCESS MANAGEMENT(IBPM) SUITES

Just like its evolutionary predecessor (BPM), intelligent Business Process Management (iBPM) suites provide tools to design and orchestrate model-driven process discovery, analysis, modelling, execution and monitoring of business rules and processes, and continuous improvements. And while those core BPM capabilities are mature and stable, BPM is ageing as a standalone product, as vendors increasingly compete on value-added elements, i.e. integrated developer environments (IDEs) for API connectivity, UI creation, organizational models, task design, and reporting which enables to construct full and complex process applications. 

Business process management (BPM) is the discipline of improving an end-to-end business process by analyzing it, modelling how it works in different scenarios, executing improvements, monitoring the improved process and continually optimizing it. BPMs are both design-time and run-time applications that give users model-driven tools to design and orchestrate the execution of business processes and business rules.

Intelligent Business Process Management Suites (iBPMS) are legacy BPM successors which embraced a number of modern-day capabilities: low-code development to deliver more agile working environment and real-time collaboration, process discovery, analytics, visualization and monitoring, build-in integration platform and process workflow.

Characteristics
Transparency, increased efficiency and improved agility. It helps to coordinate people, machines and things.
Gartner lists several critical capabilities of iBPM suites: interaction management, high-productivity app authoring, monitoring and business alignment, rule and decision management, analytics, interoperability, intelligent mobility, process  discovery and optimization, and context and behavior history.


Enable companies to establish mature process governance and management.
Grow enterprise performance through improvements of specific work activities within a particular department, across the enterprise, or between organizations.
Enable organizations to standardize, optimize, automate & innovate business processes.
iBPM suites are adapting low-code approach which makes it easier for citizen developers and business people to leverage.
iBPM suites are increasingly being offered as a part of broader automation platforms which means scaling enterprise automation portfolios will be less complex.

iBPM suites and Low-code Application Platforms (LCAPs) are converging on feature parity, which may obscure the best choice.
iBPM suites may turn out to be overly complexed for some of the basic BPM needs. Companies should carefully analyze how intelligent their desired solution supposed to be.
Cross-organizational and cross-functional BPM efforts make it difficult to ensure ownership and ongoing focus.


Traditionally, BPM platforms have focused on orchestrating structured processes - those core capabilities are now mature and stable and are no longer emphasized as differentiators, as vendors now compete on value-added elements, i.e. integrated developer environments (IDEs) for API connectivity, UI creation, organizational models, task design, and reporting which enables to construct full and complex process applications.

Also, iBPM-class solutions are being offered as parts of broader natively integrated automation platforms, which enable relatively easy scaling, i.e. IBM consolidated its broad automation portfolio – including BPM – into IBM Automation Platform offering. This model follows ecosystem-centric business and application architecture models, as companies have to face rapid changes in customers’ demands and market context. Gartner lists the following DigitalOps Toolbox elements: RPA, iBPMS, process discovery and process mining, low-code, and rule engines.

Overall, iBPM suites market growth rate is modest. According to Gartner, a growing share of inquiries regarding iBPMS and business process automation are coming from small organizations. According to IDC, process-centric application platforms will reach 5.9% CAGR during the 2018-2023 period, compared to 36% of RPA platforms.


---------------

AUTOMATION PLATFORMS


As established tools like BPM or RPA evolve and mature, IT vendors increasingly position their automation offerings as easy to scale, natively integrated platform suites rather than standalone products.

Automation Platform offerings are comprised of increasingly intelligent and natively integrated solutions, which enables relatively easy scaling – a form of single platform with an integrated offering. This follows ecosystem-centric business and application architecture models, as companies have to face rapid changes in customers’ demands and market context. 


Automation Platforms are broad automation portfolio offerings comprising of several key elements. As the traditional automation solutions tended to deliver many overlapping products, automation platforms’ main differentiation is based on their holistic approach and the ease of scaling automation thanks to APIs and native integration among the offering elements.

Characteristics
Automation Platforms are natively integrated and relatively easy to scale
Automation Platforms offering provides a unified go-to-market execution.
Automation Platforms bring the ability to automate workflows related to their core functions.

Native integration and modern, cloud-native architecture enable companies to scale their automation portfolios easier than ever before.
Automation Platform offerings come with increasingly consistent interfaces and common data models.
Standardized design and end-user experiences are additionally leveraged by adopting the low-code to enable less tech-savvy employees.

Automation Platforms are not a clearly defined product category, but rather a suite of strongly integrated – but not overlapping – products.
As Automation Platform offerings are broad, choosing the right area or business case to start with may prove challenging.
Choosing the best offering for specific industry or business area may also be problematic, as major Automation Platform vendors grew out of different segments of automation portfolio (i.e. Appian – out of BPM, Pega – out of RPA)

Automation Platform offerings are comprised of increasingly intelligent and natively integrated solutions, which enables relatively easy scaling through APIs, consistent interfaces and common data models. Platform approach and native integration are clear market directions, which derive their growing popularity from the wide spreading cloud-first application distribution model. Research house IDC predicts that architectures supporting deployment portability will become the de facto software deployment paradigm.

Gartner listed hyperautomation as a no.1 technology trend for 2020. This trend refers to the combination of multiple machine learning, packaged software and automation tools to deliver work; not only to the breadth of the palette of tools, but also to all the steps of automation itself (discover, analyze, design, automate, measure, monitor, reassess). Gartner claims that 2020 onwards, many vendors will provide a single platform with an integrated offering. This will be followed with vendors consolidating their automation portfolios under a single executive and engineering teams and developing a unified go-to-market execution. Automation Platforms are a Business Process Automation offerings following that pattern.

--------------------------


OPERATIONAL IN-MEMORY DBMS

An operational in-memory database management system (IMDBMS) is a DBMS (including both relational and NoSQL) used for transactions where all necessary data is stored in memory (DRAM) on a server — not simply an in-memory disk-block cache. It has all the necessary structure in memory and all DBMS operations, such as select, update and delete, are performed in memory. An operational IMDBMS can scale vertically (in a single server) or horizontally (in a cluster of servers).

The increasing maturity of technology and growing acceptance for operational IMDBMSs, such as SAP HANA and Microsoft SQL Server is creating opportunities in the IMDBMS space. IBM and Oracle both have analytic IMDBMS offerings today, and can be expected to release in-memory versions in the near future.
Declining costs of in-memory infrastructure required for IMDBMS offerings is also driving the market.

Growing quantities of DBMS vendors are offering IMDBMS, accelerating the adoption of these offerings. Consolidation, which drives down the total cost of ownership (TCO), is also key. Furthermore, hybrid transactional/analytic processing (HTAP) is combining operational and analytic processing in the same physical database, driving new real-time applications.

High acquisition costs for some required servers.
The need for new persistence models that support the high levels of availability required to meet transaction service level agreements. For high availability, IMDBMSs require new infrastructure models, which not all enterprises are well positioned to adopt.


Operational In-Memory DMBS has the potential to make a considerable impact on business value in several ways. It can speed up transactions by two or three orders of magnitude, according to Gartner. Hybrid transactional/analytical processing (HTAP) enables an entirely new set of real-time applications (such as real-time repricing and power grid rerouting, planning and forecasting) since latency issues do not prevent the use of transaction data for analytics. The value of operational IMDMBS is also magnified by consolidation because most organizations have few applications, if any, that require so much speed. A cluster of small servers running an operational IMDBMS is often enough to support most or all of an organization’s applications, reducing costs for cooling, power and support and maintenance resources. This results in a lower total cost of ownership (TCO), offsetting any higher total cost of acquisition from more expensive servers.

 

Operational IMDBMSs from vendors such as Microsoft and SAP are already in-market, while players such as Oracle and IBM are likely to follow suit soon. The speed at which these providers roll out offerings will dictate how long it takes for the IMDBMS market to reach high levels of adoption. Newer market entrants, such as MemSQL and VoltDB, are also providing operational IMDBMS functionality at attractive price points, often with more flexible implementations. 

 

However, to achieve greater market adoption, IMDBMSs require new infrastructure models. DRAM can be volatile because data is lost if the power is lost. Solid-state drives (SSDs) and hard-disk drives (HDDs) must be synchronous with transactions to achieve consistency, and can degrade performance. Non-volatile RAM (NVRAM) has very high delivery speeds and is well suited for high availability and disaster recovery scenarios, but is immature and expensive.

------------------

VISUAL DATA DISCOVERY

Visual Data Discovery is a BI platform architectural style that blends data from multiple sources into a proprietary in-memory store that is tightly coupled with an interactive visualization layer. It contrasts with the traditional BI platform that relies on a more modular architecture dependent on three distinct technologies to integrate, store and present data.

The market has dramatically shifted towards visual data discovery architecture over the past four to five years, driven by growth of roles such as the business analyst.
A majority of new BI deployments now leverage a visual  data discovery architecture over the traditional BI platform architecture.
New sales models are making it easier for individuals to try visual data discovery, build analytic views and easily share them more broadly across their organizations.
Ease of use is a clear advantage of visual data discovery, while the rise of self-service data preparation technology is making the technology more and more relevant, particularly for complex and disparate data sources.



Visual data discovery architecture is fairly manual and will see increasing competition from the augmented data discovery architecture.
Visual data discovery tools do not offer all of the features of traditional BI platforms, particularly with respect to governance and manageability. 


Visual Data Discovery is a combination of the in-memory analytics and interactive visualization technologies. Growing buyer interest in visual data discovery technologies has meant that most traditional BI platform vendors have refocused on visual data discovery as a key component of their go-to-market offerings.

 

It is now a very mature technology, presenting many opportunities for users. Ease-of-use and self-service are key selling points, for example. However, users should make an effort to provide manual processes to more closely manage end-user-created analytic content. Analytic programs need both visual and augmented data discovery – augmented data discovery can be used to automate the analysis process, while visual data discovery can be used to communicate those findings.

 

Finally, visual data discovery is typically being used for more-agile rapid prototyping, making it possible for end users to blend datasets together quickly. This is not only offering more self-service, but also offering deeper diagnostic analytic capability to most organizations.


-------------------

DATA QUALITY TOOLS

Data quality tools are packaged software tools that provide critical capabilities to enable an organization to address its data quality issues and deliver fit-for-purpose information to its business consumers. Functionality typically includes profiling, parsing, standardization, cleansing, matching, enrichment and monitoring.

Organizations with more mature information management processes are making the connection between the quality of data and good decision making, process efficiencies, reduced risk and increased revenue.
Growing interest in decision science, advanced analytics and digital business initiatives is also making data quality more and more relevant. Automation or machine-learning support for data quality is often necessary given the volume, complexity and speed of movement of the data in question, creating opportunities for these types of vendors.
Considerable investment and interest in information governance and master data management (MDM) initiatives is also driving adoption of data quality tools.


The important data quality roles such as Chief Data Officers (CDOs), data stewards, data quality analysts, data architects, and data quality developers, are still not available in organizations. Many organizations do not know what would be the best place in the organization structure for such key roles.
Most organizations do not annually measure the financial impact that poor quality data can have in their businesses.


According to Gartner, the data quality tools market is one of the fast growing enterprise infrastructure software markets. Data quality issues are considered significant inhibitors to gaining value from investments in digital business and analytics, making them often top-of-mind for executives. Organizations are increasingly identifying data-related issues, including poor quality, as a root cause of their inability to optimize the performance of people and processes, manage risk and reduce cost. Furthermore, a Gartner study of over 350 companies on data quality usage and adoption indicated that organizations estimated an average of $9.7 million loss annually as a result of data quality issues.

 

Data quality tools have a vast range of functional capabilities (data profiling, parsing, matching, merging etc.) so users must consider how readily an offering can be embedded into business process workflows or technology-enabled programs or initiatives such as governance, MDM, analytics and IoT. Roles such as ‘information steward’ are becoming increasingly important when it comes to managing the goals, rules, processes and metrics associated with data quality improvement initiatives.

 

Data quality tools are at a very mature stage, and there exist many comprehensive product offerings from a range of vendors, and use cases across industries. Best practises include the effective uses of data quality tools to proactively measure, monitor and track data quality flaws, as well as to provide automated and machine-learning support for remediation of data quality flaws.

----------------

DATA VIRTUALIZATION


Data virtualization technology consists of the execution of distributed queries against multiple data sources, federation of query results into virtual views, and consumption of these views by applications, query/reporting tools or other infrastructure components. It is a specific style of data integration supported by features from tools in the data integration tool market.


Data virtualization can be used for broad data integration needs (some vendors are pushing data virtualization as a potential replacement for ETL and other forms of batch data integration) and provide a data access layer between applications, processes and data stores.

It can also be a flexible and agile operation for data integration that allows users to quickly connect to heterogeneous data sources for faster time to data delivery.

‘Third-party’ API layers can be built for accessing data stores, in particular to deliver virtualised data services (services that formulate an integrated view of data from multiple datasets and enable this view to be accessed via a service interface).

There may be some confusion between broader data virtualization offerings, more frequently being offered by vendors, and the more narrow semantic virtual tiers and data preparation solutions.

Data virtualization is a mature technology and users may be challenged when choosing between a plethora or new or incumbent vendors.

According to Gartner, 25% of organizations surveyed use virtualization in a significant manner. Data Virtualization enables many use cases, across many industries, often specifically related to connecting to data instead of always connecting the data. Businesses that identify a demand for rapid accessibility to newly added data for experimental or pilot data integration for analytics, and to a lesser extent operations and transactional data use cases, will be able to use data virtualization as a flexible and agile operation for data integration.  This allows businesses to quickly connect to heterogeneous data sources for faster time to data delivery.

 

Data virtualization is a mature technology with proven use in data and analytics. It’s value is often inherent to new users (e.g. data engineers, data miners, data analysts) where data virtualization can provide a flexible way of integrating siloed data sources to provide data services stand-alone virtualization tools are now being introduced by vendors and incumbent data integration tool vendors adding data virtualization to their broader portfolio of data delivery styles to break into a lucrative market, creating a competitive and dynamic environment.


----------------

MULTI-DOMAIN MDM


Multiple-Domain MDM (master data management) encompasses multiple instances of single-domain-centric or use-case-centric MDM technologies. This scenario describes a dedicated technology stack (for example, a single instance) per data domain mastered, whether from the same vendor or different vendors.


Multi-domain MDM solutions provide the enterprise with the capability to achieve a consistent, trusted semantic view of some or all of its key master data domains.
Many opportunities emerge from strategic, enterprise-level information governance, management of consistent business data objects across organizational units, alignment of business data definitions and the effective execution of information stewardship. Prominent examples include better data quality, improvements to internal procedures, improved product management and optimization of sales and marketing.

Finally, multi-domain MDM solution support more advanced, enterprise information strategies that seek to support market differentiation using information as a business driver.

Market adoption has been slower than expected because organizations have demonstrated varying levels of coherence in what they mean by ‘multi-domain’, while MDM vendors have demonstrated little consistency in what constitutes a ‘multi-domain MDM solution’.

Organizations are challenged when it comes to assessing the extent to which their organizational culture and design supports multi-domain MDM capabilities and whether effective information governance, and enterprise metadata management can be established across different business areas and data domains in the enterprise.
Vendors may struggle to meet the necessary complexity across different data domains and provinces for desired use cases, implementation styles and industries.
Governance maturity, political or cultural readiness, and the ability to establish organizational leadership across multiple business areas spanning multiple data domains, are also pivotal issues.


Master data management (MDM) is a technology-enabled business discipline in which business and IT work together to ensure the uniformity, accuracy, stewardship, governance, semantic consistency and accountability of the enterprise's official shared master data assets. Master data is a consistent and uniform set of identifiers and extended attributes that describes the  core entities of an enterprise, such as existing or prospective customers, suppliers, sites or products. Multi-Domain MDM is an enabling technology that supports the management of any number of master data domains across the full spectrum of MDM implementation styles, which can be achieved via either implementation of a single, integrated offering, or a portfolio MDM offering. It provides much opportunity for business, but comes with many caveats.

 

Even organizations able to articulate their multi-domain requirements and identify truly multi-domain MDM solutions, may find that MDM alone cannot deliver the promised additional benefits. Realization of multi-domain benefits will come from an assessment of requirements on a business process basis, along with the resultant organizational and process changes that the enabling technology will ultimately support. In other words, technology adoption alone does not ensure success, because greater effort is needed in terms of design, governance, business process and organizational change management. As increasing numbers of multi-domain MDM solutions come to market, users need to explore and fully validate their business and IT readiness, as well as their MDM maturity, to ensure that the right kind of technology is matched to their organizational culture and business goals.

-----------------

INFORMATION GOVERNANCE


Information Governance (IG), also known as Data and Analytics Governance, is the specification of decision rights and an accountability framework to ensure appropriate behaviour in the valuation, creation, storage, access, use, archiving and deletion of information. IG includes the principles, guidelines, standards, procedures and metrics that ensure the effective and appropriate use of information in enabling an organization to achieve its goals.

Data and analytics leaders are increasingly concerned with how to govern data and analytic assets such as master data, application data, algorithms, analytics models, KPIs, external social data, records and content.

Growing numbers of Chief Data Officers (CDOs) are putting data and analytics governance at the centre of business priorities as a key enabler of many data and analytics initiatives. IG is now understood to serve as an additional means to drive value, not solely to mitigate risk through compliance.
IG can reduce the cost and risks associated with data and analytics, increase the value of that information and support improved decision making.

IG is technically complex, organizationally challenging and often politically sensitive.
It can be difficult to get executive-level sponsorship for governance programs, since too many organizations equate the work with compliance.
There is no clear product or market for IG platforms, only tools and newer solutions that address some of the requirements.

IG today is becoming more about enablement, rather than compliance. Business drivers for IG programs are shifting away from regulatory compliance and a need to decrease litigation costs. Digital businesses today are more focused on the business value of data and the idea of ‘governing the least amount of information with the least effort but maximum business impact’, combined with a need to mitigate risk associated with unmanaged data. IG therefore needs to be designed by the business, for the business.

 

Effective governance can lead to changes in business behaviour so that data and analytics enables better decision making and improved business outcomes. IG helps enterprises to utilize information assets to create business value, gain efficiency and service customers better. It can also help reduce the cost of litigation and speed up responses to regulators. Consistent, transparent and accurate information is foundational to every business process, including ERP, CRM, BI and the management of all corporate documents. Organizations should be aware, however, that IG is complex and organizationally challenging. With no clear market for IG platforms, and a habit amongst users to equate IG directly with compliance, there is some way to go before IG reaches maturity.


--------------------

GEOSPATIAL AND LOCATION INTELLIGENCE


Geospatial and Location Intelligence (GLI) includes applications, infrastructure and tools, and best practices that enable access to and utilization of geospatial and location data of people, things and information for location-referenced information analysis.

Location is one of the most important pieces of information for contextualizing a range of data types, including transactional, social, mobile, user and sensor data. Analyzing business data in the context of location can allow users to uncover spatial trends, dependencies and patterns that are otherwise undetectable.
GLI can be used to better understand and engage customers and citizens, optimize asset usage, and improve operational efficiencies.
The IoT is driving greater adoption, broader utilization, and new GLI technologies and services. New initiatives, such as Industry 4.0, smart cities, smart mobility and smart shopping are also using location-referenced information as a core component.

The emergence of newer GLI tools is also key, which eliminate the need for GIS (Geographic Information System) specialization skills and offer self-service capabilities for nontechnical users to more easily analyze location data.



Differences and changes in data protection and privacy laws and regulations across different geographies can impede the implementation of GLI.
A lack of resources and properly trained employees required to perform location analysis and interpret results can limit an organization’s ability to leverage GLI.
Interoperability issues across vendor platforms caused by different standards for diversified types of geospatial and other location data, as well as disparate methodologies of data acquisition and transmission, pose challenges when sharing data and functionality across solutions.



Interest in GLI will continue to grow as organizations increasingly comprehend the value of location to better understand and engage customers, optimize asset usage, and improve operational efficiencies. Companies like Uber, Lyft, Didi and Grab use their ride-sharing apps to shape real world behaviour, creating new gathering points to change how we get around efficiently. Companies like Snap generate revenue from on-demand revenue geo-filters while Twitter and Facebook target advertising by location. According to Gartner’s 2017 business intelligence (BI) and analytics spending intentions survey, GLI was one of the reported top areas for investment.

 

GLI can uncover previously unseen spatial/location relationships, resulting in improved operational efficiencies and decision making. While there is increasing interest in GLI, creating a single solution that offers all the capabilities is extremely challenging and this concept will likely cease to exist, with vendors increasingly specializing in either macro (e.g. focused on spatial analytics for use cases such as store planning, environmental analysis and traffic planning) or micro (e.g. focused on close proximity use cases, such as in-store real-time customer engagement experiences) location intelligence. Some vendors will continue focusing on expert GIS users, while other emerging providers and analytics and BI platform vendors will offer GLI capabilities for business users.

------------------
TEXT ANALYTICS


Text Analytics is the process of deriving analytical findings from textual data sources. This process can include determining and classifying the subjects of texts, summarizing texts, extracting key entities from texts, and identifying the tone or sentiment of texts.

Opportunities for Text Analytics are increasing due to growing data volumes and the need to make sense of the underlying context of the data.
Text Analytics has become increasingly important when handling unstructured data, which has grown in volume and prominence as written communications have shifted from print to digital forms.

There is a growing desire to complement insights gleaned from analysis of structured numerical data with text-based facts for more-robust predictive modelling.


The Text Analytics market is fragmented and far from saturated. Text Analytics is also known as text mining, natural-language processing or entity extraction, and these alternatives often receive more individual hype than the areas as whole.
The technology is still maturing and differentiation between the many overlapping vendors is nuanced.
The diversity of available uses cases means that it is challenging to incorporate solutions into an organization’s wider digital platform.
Text Analytics needs specialist skills to utilize and gain benefit.


Text Analytics, when combined with various other analytic capabilities, can be of significant benefit to an organization in many ways, including but not limited to:

Processing unstructured data for analysis
Discovery and insight (ie. indexing reports)
Voice of the customer analysis (ie. call centre notes)
Automated document matching and classification
Medical records analysis
Market research (ie. surveys)
Analysis of insurance claim narratives
Analysis of brand loyalty (ie. analysing Twitter feeds)
Litigation (forensic analysis of a body of documents to gather an audit trail)
Future technological advances could lead to highly automated solutions capable of accurate and deep understanding of contexts. Currently, however, Text Analytics has many challenges, such as a fragmented market, such a diverse set of use cases and the specialist skill-sets required.

-----------------------------


EVENT STREAM PROCESSING

An event stream is a sequence of event objects arranged in some order, typically by time. Event Stream Processing (ESP) is any kind of computing performed on event streams. The purpose of some ESP systems is to ingest, process and store event streams for later use.


The growth of the IoT market, the proliferation of connected devices and the increasing number of digital interactions is resulting in event streams that are more ubiquitous, creating a wealth of opportunity for ESP.

Further, vendors are increasingly bringing out new products, many of them open source or partly open source.
Alongside the data deluge, businesses are demanding continuous intelligence for better situation awareness and faster, more precise decisions.
ESP technology is maturing rapidly and is set to eventually be adopted by multiple departments within every large company, embedded in SaaS solutions and off-the-shelf package applications.

The ESP market is highly specialized, divided between vendors predominantly offering products for analytics and those offering products for data ingestion and movement, creating challenges for customers with broader requirements.
Incumbent on-premises vendors are being disrupted by rapidly maturing cloud-native offerings, which are in turn being disrupted by edge-based deployments, particularly for IoT, resulting in a dynamic but volatile market landscape.
Identifying employees with the necessary skillsets for highly technical and data-intensive ESP tasks.


Companies now have access to more streaming data than ever, from internal sources (such as sensors, meters, control systems and transactional sources) and from external sources (such as social computing platforms, news and weather feeds, other data brokers, government agencies and business partners).

 

The main purpose of ESP is stream data integration or stream analytics, which can be used to improve the quality of decision making by presenting information that could otherwise be overlooked. It can enable smarter anomaly detection and faster responses to threats and opportunities, and help shield business people from data overload by eliminating irrelevant information and presenting only alerts and distilled versions of the more important information.


ESP is one of the key enablers of continuous intelligence and other aspects of digital business, and will be essential for businesses able to navigate a complicated market landscape and deal with skill-set gaps. Much of the growth in ESP usage during the next 10 years will come from three areas where it is already somewhat established: IoT, customer experience management and fraud detection applications. Between 2018 and 2023, Gartner estimates that revenue for ESP platforms will grow 15% year over year. The overall growth rate of ESP usage will be even higher when considering no-cost, open-source event stream processing software.

-----------------

CITIZEN DATA SCIENCE


Citizen Data Science is an emerging set of capabilities and practices that allows business users to extract advanced analytic insights from data without the need for extensive data science expertise. Central to enabling citizen data science are rapidly progressing augmented analytics capabilities that streamline data preparation, provide user guidance for data science operations (correlations, clustering, predictions), augment user insight through automated modelling and pattern detection, and enable collaboration and sharing.


Citizen Data Science tools can guide the user through the end-to-end modelling process by automating manual and bias prone tasks, such as feature selection. Citizen Data Science tools also augment the user’s discovery capabilities by automatically generating and prioritizing statistically meaningful insights for users.
Insights from data science and machine learning can be made more accessible and pervasive in the enterprise. Citizen Data Scientists can be leveraged to fill the data science and machine learning talent gap that is currently being experienced due to the shortage and high cost of data scientists.


Businesses are often challenged to ensure that Citizen Data Scientists are deployed at exactly the right stages of projects, and in the correct fashion. Training business users in Citizen Data Science techniques can still be challenging for companies, and requires time and investment, and there may be some push-back from some business users.
Citizen Data Scientists are often not allowed to work fully independently of IT experts, resulting in overlap and sometimes duplication of work, which is less effective.


Current business intelligence (BI) and analytics approaches enable users to carry out manual data preparation, data exploration and pattern detection in some cases. Creating data science and machine learning models, however, requires expert data scientists who are often expensive to hire and difficult to find. Citizen Data Science has the potential to form the foundation of next-generation analytics, making insights from data science and machine learning more accessible and pervasive in the enterprise. Incorporating citizen data scientists into specific phases of the analytics life cycle can enable more scalable and focused use of data science and machine learning resources across the organization. Citizen Data Scientists can be used in the exploratory phase of a project to enable more skilled scientists to focus their expertise on more cutting-edge model building phases, or act as ‘business translators’.

 

Data and analytics users should look for opportunities for Citizen Data Science to complement and collaborate with existing user-orientated modern analytics and BI and expert data science initiatives.

 

Citizen Data Science will likely be a key driver of analytics adoption over the next several decades. According to Gartner, by 2020 more than 40% of data science tasks will be automated, resulting in increased productivity and broader usage by Citizen Data Scientists. By 2024, Gartner also predicts, a scarcity of data scientists will no longer hinder the adoption of data science and machine learning in organizations.

-------------------

SERVWARE

Servware is converged data and analytics services and software in the form of customizable assets containing business domain content and analytics expertise. It is protected by intellectual property (IP), from a single vendor, and comes as a packaged product (platform or application).


Servware allows for an organization’s domain analytics to be used to monetize its own expertise and insight, and monetize analytical IP.
By favoring pre-packaged servware over custom-built solutions, businesses will see a reduced need for in-house skills.
Businesses using servware will see an improved agility that comes with faster implementations and shorter development cycles. Servware also incorporates ready-to-use models and domain-specific data sources to support immediate data interrogation.

Opportunity for reduced total cost of ownership (TCO) for data and analytics, as servware is often based on a pay-per-use or subscription pricing model. SaaS-based packaged analytic applications can also be funded with operating expenditure instead of capital expenditure.
Servware applies an array of analytics techniques, including advanced and specialized analytics, for specific use case.


Servware disrupts familiar vendor-buyer relationships, causing uncertainty for analytics leaders concerning business models, pricing options and support. Data and analytics leaders face a more complex future in evaluating analytics providers and their offerings, business models and support.
Servware, by accelerating the shift of analytics to business units, can loosen analytics leaders’ control and fragment alignment of strategic business goals.
Business users often lack the analytics expertise, policy awareness and enterprise-wide perspective that can be essential in optimizing a servware solution.


The convergence of services and software means that the traditional model of delivering data and analytics is changing considerably. Vendors are more frequently adding software applications to their services, as services and software are more suited to addressing specific business needs when combined. Servware can therefore be viewed as packaged intellectual property, coming in the form of both data and analytics platforms and applications. Servware is often prioritized over traditional data and analytics service when speed is a deciding factor and the solution is considered tactical or low-risk. Many service providers are now actively developing and offering servware to the market.



Moving forwards, data and analytics leaders should be looking to explore the impact of servware on their data and analytics operating models, and establish business-based, cross-functional teams to increase the data and analytics performance of their business units. Businesses should also be defining domain requirements, running proofs-of-concept comparisons against current solutions with servware, and establishing a ‘co-development’ relationship with servware vendors to generate analytics IP that can be monetized together. Servware can disrupt vendor practices and create prospects for organizations to differentiate competitively. Data and analytics leaders should embrace this disruption to leverage converged solutions across the organization.

-----------------

DATA AS A SERVICE

Data as a Service (DaaS) is a design approach that delivers data on demand via consistent, prebuilt access with the aid of standard processing and connectivity protocols, one component of an overall information architecture. Originating data remains local to its storage platform and, following various steps to access, format, evaluate and contextualize it across any deployment option, it is presented as output for use in a subsequent process or delivery endpoint.


Service-enabled approaches can be used to access disparate data from various solutions and combine for delivery to a new use case - data aggregation companies are existing as third-party data providers.

The growing trend for collaborative and open data, alongside data marketplaces and data brokers who curate and sell data, is boosting DaaS growth. Advancements in automation and machine learning is also driving further adoption.

DaaS provides opportunities for overcoming the binding of data to specific applications. DaaS can also support other data integration styles, such as bulk/batch or data visualization, or participate in message queue management. DaaS can be used to perform simple data delivery, but it can also include complex data contextualization.
DaaS architectures offer several benefits to developers, who may otherwise have to learn specialized data techniques and protocols, enabling faster time to value for business and more users to have access. It can also be used to migrate complex data warehouses and silos to more federated structures that obscure their complexity with a more simplified interface. DaaS also makes coordination between business units and between business partners more seamless.


There are many challenges associated with the difficulty of building DaaS in-house.
The complexity of DaaS – it is made up of disparate governance frameworks that need to be resolved, varying levels of detail, in various integrated and non-integrated models – makes obtaining successful results challenging for many businesses.


DaaS is complex and can range from a simple access API coupled with simple filtering, all the way up to the rationalizing of multiple data assets and the application of context. As such, successful implementations are difficult and cannot be achieved by just placing data in the cloud or creating data lakes. Well designed DaaS environments provision data that is integrated, enriched, well-described and reliably accessible before endpoint delivery. Data sources must also have well-embedded security, privacy and quality controls; DaaS should not just solely be equated with technology implementation. Despite these challenges, analysts do predict healthy growth for the DaaS market. According to TechNavio, the DaaS market is expected to reach $1.3bn by 2020, growing at a strong CAGR of nearly 44%, driven by demand for data management and increasing need to enterprise mobility.


Some vendors offer tools for simplifying and unifying database and content data store access but DaaS is ultimately a style of architecture, not any one technology. Data and analytics leaders should consider DaaS-style architecture as an option to expand and complement the existing data management strategy and infrastructure. DaaS has many potential benefits and can help to address the growing variety of data, the increased demand for data from more users while evolving from traditional, physically manifested hub-and-spoke data management and integration approaches, such as delivering master data and supporting analytic extracts and B2B use cases for sharing data. Larger scale adoption could be limited by the challenging nature of DaaS, in terms of user skills and training and the difficulty of building DaaS in-house.


---------------

PERSONAL ANALYTICS

Personal analytics is the analysis of contextually relevant data to provide personalized insight, predictions and/or recommendations for the benefit of individual users. Personal analytics can take the form of software agents acting on behalf of users with a varying amount of autonomy. Examples include virtual personal health assistants, financial advice assistants and shopping assistants.


Individuals are increasingly engaging with technology and generating greater and greater amounts of data, including photos, social interactions, purchases, preferences and health indicators, creating opportunities for personal analytics vendors.
Opportunities are also being created due to a growing user willingness to combine and share personal data, in exchange for visible personal benefits derived from advanced analytics, via more mature life-logging applications.


Privacy concerns are always particularly prevalent when personal data is concerned, creating a challenging market environment for personal analytics vendors.
Integrating personal ecosystems of sensors and data feeds associated with individual users, is also complex.
The business models required to support product development and marketing are immature and not fully understood.

Personal analytics includes a nascent, non-standardized collection of proprietary algorithms which offer varying levels of functionality and broadly focus on self-reflection, personal improvement and benefit. Personal analytics, when combined with chatbot functionality, is also a market driver for emerging technologies such as virtual personal assistants. The analytics layer leverages personal data to generate contextual and relevant insights, applying rules and preferences to generate recommendations or actions.

 

The speed of adoption for personal analytics will largely depend on the ability of analytics tools to address key user pain points, including disease/symptom diagnosis or personal financial management, which go beyond the services offered today such as calendaring/reservations, geolocation and recommendation services. The integration of advanced analytics to drive decisions, change behaviours and execute tasks automatically, will be key.

 

Data and analytics leaders should be developing personal analytics tools, applications and APIs for customers as a way of deepening brand interactions consistently across different touchpoints and channels. Leaders should also prepare for a future where customers use many different types of personal analytics on different devices where there is no single solution that meets everyone’s needs – analytics solutions of the future must be device-agnostic and machine learning approaches must provide differentiation. As the proliferation of connected devices continues at pace, and as the amount of data generated increases exponentially, the opportunities for personal analytics will also grow rapidly. How best to address privacy concerns regarding the use of personal data is a key challenge, and must be anticipated and tackled by technology leaders.

-------------------

GRAPH ANALYTICS

Graph analytics consists of models that determine the “connectedness” across data points to create data nodes/clusters and their demarcation points. Nodes are connected explicitly or implicitly, indicating the levels of influence, frequency of interaction, or probability. Graph analytics is typically portrayed via a visualization for business user consumption.


Strong desire in the market to find insights within exponentially large quantities of heterogeneous data, and the demand to analyze it, is providing opportunities for graph analytics and driving adoption.

Expanded capabilities, increased computational power and adoption of graph databases are presenting an ideal framework for storing, manipulating and analyzing graphs.
While relational analytics is typically ideal for structured, static data in columns and rows in tables, graph analytics is better suited for exploring fluid and indirect relationships between entities across multi-structured data, delivering insights that are difficult to reach with SQL-based relational analytics.
Graph analytics processing is a core technology which underlies many other advanced technologies, such as virtual personal assistants, smart advisors and other smart machines. Growth of these technologies is therefore providing further opportunities for graph analytics.


A lack of broad awareness among potential users is creating challenges for extensive adoption of graph analytics.
The unique methods required to store and process data within many graph databases, combined with the need for new skills and graph-specific knowledge, can also limit adoption. Examples of knowledge and experience needed include the Resource Description Framework (RDF), SPARQL Protocol and RDF Query Language (SPARQL), and emerging languages such as Apache TinkerPop or the recently open-sourced Cypher.


The commercialization of graph analytics is at a fairly early stage, with a small number of emerging players. Analytics experts are beginning to claim specialization in graph analytics, while some traditional analytics vendors are offering capabilities that enable users to build interactive network graphs as an additional feature of their products. Platforms such as Cambridge Semantics, Digital Reasoning, Ayasdi and Maana use graph analytics to identify important findings.

 

Data and analytics leaders should be looking to evaluate opportunities to incorporate graph analytics into analytics portfolios and strategies, which will enable businesses to address the high-value use cases which are not fully addressed by SQL-based queries and visualizations - including computing and visualizing shortest paths or the relationship between and influence of two nodes or entities of interest in a network, for example. Graph analytics can also be used to enhance pattern analysis. Users can interact directly with the graph elements to find insights, while the analytics results and output can also be stored for repeated use in a graph database. Graph analytics can be an effective tool for both assessing risk and responding to it, for example for fraud analysis, route optimization, clustering and outlier detection. A recent, and quite prominent, example of identification of networks in relationships is the research by the International Consortium of Investigative (ICIJ) research which revealed the Panama Papers.

 

Graph analytics is effectively utilized across a range of industries. There is a lot of potential for increased uptake however growth can be hindered by a lack of awareness among potential users, or a deficiency in relevant skills or knowledge.

------------------


ENTERPRISE SOCIAL NETWORKING APPLICATIONS

Enterprise social networking applications enable, track and organize open conversations and sharing of information between individual workers and groups within an organization. Enterprise social graphs are used to keep track of the network of relationships between application users, while news feeds and activity streams are used to deliver personalized updates about events or conversations to individuals.


The popularity of consumer social networking platforms is driving adoption within business.
There is a strong supply side with several options available for businesses. Many enterprise software vendors deliver social networking applications.
A large range of app functionality (e.g. video streaming, chat, bot framework).
A broad array of use cases and activities that can be supported by these applications (e.g. project completion, decision making, knowledge capture, innovation acceleration, strategic alignment and employee engagement).


The market is a fragmented one, with a host of vendor offerings.
Some deployments fail because they lack a clearly defined and appropriate purpose to make them relevant to the work activities of targeted participants.
Other potential challenges include organizational culture clashes, privacy issues, questions about workers’ productivity and content quality.


Enterprise social networking applications have a lot of potential within the enterprise. They can improve communication, provide the ability to influence behavior and discovery, not to mention provide the opportunity to gain insight into specific activities around which social interaction occurs. Valuable information can be created, shared and refined through self-selection, collaboration, social incentives and decentralized control, as opposed to top-down resource allocation and instruction.

Most organizations have now moved from tools such as micro-blogging and wikis to broader applications with multiple capabilities. The sheer number of solutions available is likely to provide a headache for decision-makers. Some vendors are providing social and collaborative capabilities within specific business applications designed for this purpose (e.g. Salesforce Chatter, SAP Jam Collaboration), while other offerings form part of more general-purpose applications (e.g. Microsoft Office 365, G Suite by Google Cloud). Some products offer very basic social networking functionality, including content collaboration platforms (e.g. Box, Dropbox), collaborative work management (e.g. Asana, Smartsheet, Trello, Wrike) and workstream collaboration (e.g. Slack, Cisco Webex Teams).

--------------------

CLOUD OFFICE

Cloud Office refers to a collection of the most broadly used suites of SaaS-based personal productivity, horizontal collaboration and communication tools. These suites generally include email, IM, file sharing, conferencing, document management and editing, search and discovery, and collaboration.


A growing preference in the market for cloud deployments has created opportunities for Cloud Office vendors.
Customer demands for cost reduction, redeployment of IT staff, greater simplicity and more user functionality.

Migrations from on-premise deployments to cloud or hybrid can be challenging.
Some organizations struggle to realize all of the potential value that Cloud Office suites can provide.


Cloud Office adoption in the enterprise is growing as technologies are becoming better understood. According to Gartner, 30-40% of enterprises have made the move, although many face challenges. The volume of cloud office projects can lead to frustration, migrations can prove difficult and some organizations fail to realize the potential value that these suites can provide.

Microsoft Office 365 and Google G Suite are the biggest vendor examples. Both Microsoft and Google are investing heavily to attract enterprises to their cloud office offerings and are responsible for the majority of enterprise deployments. Other vendor offerings include IBM Connections Cloud, Amazon WorkDocs/WorkMail/Chime and Zoho Office Suite. Vendors generally can provide the most attractive new features (e.g. mobile apps, content discovery tools, AI) from cloud deployments only.

Looking forward, Cloud Office solutions will become more and more difficult for businesses to ignore. The wide scope of Cloud Office workloads means that this technology has the potential to affect everyone in the organization, and these suites support a wide variety of collaboration styles including video, conversational and social as well as email and IM.

---------------------

BRING YOUR OWN THING

Just as bring-your-own-device (BYOD) drove the mass adoption of interaction models based on smartphones, tablets and personal computers, bring-your-own-thing (BYOT) will stimulate adoption and stabilize new business models around the engagement with physical products like wrist bands, personal assistants, smart watches, and many others.

Bring-your-own-thing (BYOT) refers to individuals bringing personal IoT devices or wearables into the working environment for reasons of convenience or productivity. Initially, BYOT will involve objects such as fitness bands, smart lights, air filters, wireless webcams, voice assistants, smart ear buds, remote-controlled power sockets, consumer virtual reality headsets, drones, or coffee machines. In the long term, it could include highly sophisticated devices, such as self-driving cars and domestic robots.


Opportunity for the organization to formally adopt new devices to improve quality of life or productivity.
The emergence of technologies such as Amazon’s “Alexa For Business” will make it more practical for technology and service providers to create more enterprise-oriented versions of consumer products to provide alternatives to BYOT.
Future technologies such as embedded AI will enable “things” to become more intelligent and will increase the demand for BYOT.

Many BYOT security challenges can’t be managed technologically because there are no tools equivalent to the enterprise mobile management products used in the “bring your own device” and “bring your own application” domains.
As employees will add their own objects to a smart workspace environment, organizations will need to adjust BYOT policies accordingly.

Bring-your-own-thing (BYOT) is immature and few consumer things are found in workplaces today. However, as individuals adopt smart homes and use technologies such as voice assistants to control personal IoT, more BYOT devices will enter the workplace. BYOT will pose new security risks for organizations, as “things” can be an attack pathway into devices such as smartphones or corporate wireless networks. Also, information may leak through the use of public services such as voice assistants to control things.

In the short term, technologies such as smartwatches, air filters, personal smart lighting, smart ear buds, or fitness bands might help workers be more effective or productive, albeit in a somewhat indirect way. In the longer term, some BYOT devices will enable new working practices. Overall, BYOT will be a diverse domain and will continue to grow through 2025 as the range and capabilities of consumer IoT devices grow.

---------------------

DESKTOP AS A SERVICE


Desktop as a Service (DaaS) is a cloud-based way of hosting employees workstations, giving them the flexibility to work when and where they need to. DaaS is an essential component of a comprehensive business continuity strategy and, in addition, it is crucial for organizations with remote-based workers that need to ensure employees can access critical information whether they are at home or on the road.


Desktop as a service (DaaS) - also called hosted desktop services - is frequently delivered as a cloud service along with the apps needed for use on the virtual desktop.

In the desktop-as-a-service delivery model, the cloud computing provider manages the back-end responsibilities of data storage, backup, security and upgrades.

Typically, an end user's personal data is copied to and from their virtual desktop during logon and logoff, and access to the desktop is device, location and network-independent.


The adoption of Cloud Office (especially Microsoft Office 365) and SaaS increases the viability of a DaaS solution as an organization’s data and services become increasingly externalized.

Long-term cost of the service remains high
DaaS can create application performance challenges, especially when dependent on access to on-premises data and services.

In the past, DaaS has suffered from the challenges associated with the technologies that power it, namely server-based computing (SBC) and Virtual Desktop Infrastructure (VDI). However, with more organizations looking to deliver user-centric services across different devices and locations with an ever-increasing consumption of cloud services, DaaS is now being considered a strategic solution.

The DaaS market has definitely picked up momentum in 2019 with Microsoft expected to launch their own DaaS solution - Windows Virtual Desktop. But Microsoft won’t be the only DaaS choice, some of the more well-known desktop-as-a-service providers include Amazon, Desktone (a VMware company) and Citrix.

There are at least five reasons why companies should consider DaaS solutions when reformulating their IT strategy:

Short-term employees
Third parties and contractors 
Remote workers
Business continuity plans
Merger and acquisition (M&A)

--------------

ADAPTIVE LEARNING PLATFORMS


Adaptive learning is the purposeful use of data, algorithms and content to provide the right support in real time and help each individual employee to improve their workplace performance.


Adaptive learning tools are defined as education technologies that can respond to a learner’s interactions in real-time by automatically providing the employee with individual support. The platform collects specific information about individuals’ behaviors by tracking how they answer questions, and then responds to each learner by changing the learning experience to better suit that individual’s needs, based on their unique and specific behaviors and answers.


For macro-adaptive platforms, content and delivery are tailored to a specific audience or segment based on preferences or learner profiles.
For micro-adaptive platforms, learning delivery is continuously tailored based on assessment of employee mastery of the content.

Adaptive learning platforms supplement and augment, versus replace, the work of faculty
A tool to enhance the learning experience
Transform the learner from passive receptor of information to collaborator in the educational process


Difficult to implement at scale as it requires an organizational cultural change
Dependent on large-scale collection of learning data

Across industries, T&O leaders are coming to the realization that the traditional approach to workplace learning simply can’t keep up with business needs. In addition, while one-size-fits-all programs may be an easy way to provide training to large groups, they can’t address the changing needs of the individual employee. People all come to their roles with different backgrounds, knowledge and experience. Therefore, if they are expected to perform at their best, all employees deserve a more personalized approach.

The ultimate aim of adaptive learning is to enhance the learning experience and empower employees. These changes will lead to tangible results such as improved learning outcomes, re-skilling and higher retention rates, all of which are important accountability measures in education & training.

Due to the culture change required to implement adaptive learning platforms at scale, the rate at which they have been implemented by organizations so far remains low. Nevertheless, Accenture expects transformative benefits from adaptive learning that will achieve mainstream adoption in two to five years.

--------------------

INTERNAL TALENT MARKETPLACE

The Internal Talent Marketplace uses ‘gig economy’ principles in the enterprise, relying on marketplace platforms to match internal employees and, in some cases, a pool of contingent workers, to short-term project and work opportunities, without recruiter involvement. Some key features include marketing, matching algorithms and feedback functionality.


Easier to connect workers directly with role vacancies.
Increased flexibility and agility in the workplace, without requiring changes to employment categories.
Reduces the pressure on large enterprises caused by heavy management and control structures.
Establishes trust through feedback mechanisms, while allowing for worker-led innovation and for workers to fully take control of their own careers.


A lack of organizational readiness is a key barrier to adoption. Traditional hierarchical structures are often too rigid for the more agile and lean concepts behind the Internal Talent Marketplace. Companies must ask whether they are allowing workers to explore growth opportunities and build a portfolio of work that they can use to market themselves for further work opportunities.
Segmenting jobs and allowing for employees to bid for and win such jobs represents a significant change to management practices and company culture.
There are many technical challenges associated with the Internal Talent Marketplace – adoption depends on the performance of matching algorithms within the platforms. Accurate detection and tagging of skillsets, knowledge, experience, cognitive skills and other competencies and attributes is difficult and time-consuming


The gig economy and the need for business agility has opening up new ways of working and challenged established notions of employment. Preferences on how individual workers manage their careers and build their portfolio of work experiences are changing.

Market readiness levels for Internal Talent Marketplace solutions are low, and few providers have built the relevant functionality. Freelancer-related solutions have advanced more quickly but are focused exclusively on external networks in most cases. Some organizations have experimented with self-forming teams, adaptive structures and more innovative career development methods. Some have mirrored crowdfunding campaigns to gain organizational support for ideas, allowing employees to contribute work hours as payment into such projects.

Fundamentally, proven practices have not yet been established and careful measurement of the impact on worker engagement and productivity along with team performance will be very important. Stakeholders should pay particular attention to the data science skills required from both the technology provider and buyer organizations, and working out more granular and context-specific understanding of skills, knowledge, experience, cognitive skills and competencies will be essential for creating effective user experiences in these platforms.

-----------------------

SMART BADGES

Smart Badges are pocket-sized integrated circuit cards, including wristbands or other wearables, with built-in sensors and the ability to transmit data wirelessly. Key examples include infrared sensors, accelerometers, microphones and scanners.


Smart Badges, unlike smart cards or smart IDs, are wearable solutions that aim to provide more advanced features, such as location-based contextual services and analytics.
These features can improve workplace communication and operational efficiency, in addition to employee well-being and performance.
The potential for integration with technology such as mobile payments will provide further opportunities for providers and the enterprise.

Privacy and data protection are key challenges. Some companies may be slow to let employees know about Smart Badge initiatives, explain clearly in advance what is being tracked and analyzed, and make participation optional.
Still in nascent stages of development.


Smart Badges are emerging as a new type of endpoint device for the connected worker. Common modern-day use cases include security authentication within organizations, such as access control (E Ink technology, for example, uses customizable context on smart badge display for visitors or particular events), but the market is showing signs of providing value beyond purely authentication.

Some key use cases in this vein include:

Restaurants; Smart Badges do not have to be swiped like standard POS systems, saving time.
Healthcare; Smart Badges can provide location-contextual services to staff, such as alerting the nearest available staff member on where to go when help is needed.
Event management; gives organizers insight into what occurs at their events, and helps to provide more personalized experiences.
Retail; can improve customer experience by employees being able to immediately check price or inventory using the Smart Badge.
Construction; use of sensors that alert management or monitoring services in case of a fall or similar.
Companies can analyze aggregated data from Smart Badges to propose improvements to organizational communication and evaluate efficiency (by analyzing employee movement data, for example). Employees can use this information for personal evaluation and comparison to benchmarks. Looking forwards, a key benefit (and challenge) will be the ability for solutions to easily integrate with sources of data across the enterprise and other third-party systems.

-----------------

IMMERSIVE WORKSPACES

Immersive Workspaces are collaborative work environments that leverage visual reality (VR), augmented reality (AR) or mixed reality (MR) technologies.

Remote collaboration can help to reduce costs such as travel expenses.
An Immersive Workspace can improve the connection between office-based workers and remote workers.
Increased employee engagement and effectiveness of training through the ability to mirror real-life situations. Some solutions are building gamification into learning.
Lots of potential in design work in particular; users can collaborate from anywhere and view the same thing, annotate and manipulate designs in real time, and overlay digital designs on physical objects.


Immersive Workspaces are at a very early stage of development, with limited functionality.
It is difficult to pick a winning solution, in terms of technology and platform.


The Immersive Workspace, although at an early stage of development, has the potential to provide opportunities for more natural collaboration, knowledge sharing, onboarding and training thanks to immersive technologies and sensory elements. Immersive Workspaces are starting to appear within platforms such as Oculus VR, which can create 3D virtual offices and desktops in a virtual reality (VR) world. Using augmented reality (AR), digital objects such as monitors can be placed on walls in virtual representations of physical objects, creating new visualization and productivity opportunities. According to Tractica, enterprise spending on VR/AR will be approximately 35% greater than consumer spending on VR/AR entertainment by 2020, excluding hardware-related revenues. Furthermore, ABI Research expects the enterprise VR training market to grow to $6.3bn by 2022, from $216m in 2018.

Clearly the market is at very early stages of development, with limited functionality; immersive technologies are being deployed by large enterprise in pilots and proofs of concept (POCs). Many organizations are likely evaluating the market and testing virtual desktop applications through VR ecosystems (e.g. HTC Vive, Oculus VR) with employees testing immersive technologies such as VR conferencing. The Oculus Virtual Desktop, for example, turns the user’s desktop into a 720-degree ‘command center’ that provides better access to information needed to do the job. Glancing further into the future, providers of virtual meeting, conferencing and training solutions should be looking to add Immersive Workspace capabilities to their products. Enterprise stakeholders, on the other hand, need to identify meaningful use cases, design a compelling use experience and choose the most appropriate technology.

------------------

SMART WORKSPACES

A Smart Workspace leverages the increasing proliferation of connected and digitalized devices, known as the Internet of Things (IoT), to deliver new ways of working, collaborating, scheduling resources, sharing information and coordinating facility services. Any location where people work can be a Smart Workspace.

A growing enterprise focus on facility modernization, more agile work environments and employee experience value is driving the Smart Workspace market. Digitalization and programmability of the workplace will create new integration opportunities. Electronic whiteboards, for example, are becoming integrated with traditional collaboration and content software systems, providing more opportunities for experimentation.
Data from meetings is better captured in a Smart Workspace, and is more widely searchable.
Beacons and sensors placed in key locations can interact with mobile apps to deliver personalized information to workers, based on proximity. This can be used to improve employee learning or communicate safety procedures based on employee location.
Smart Workspaces have the potential to improve employee productivity and cultural perception of the workplace. Customer experience can also be improved as employees make better use of the Smart Workspaces to serve clients.
Cost reductions, as office utilization data can guide decisions about what types of workspaces are most conducive to employee effectiveness.

Employee privacy concerns.
Organizational impacts create challenges - IT organizations need to work much more closely with real estate and facilities teams, in ways not previously imagined.
The Smart Workspace market is emerging at an uneven pace as organizations prioritize potential solutions independently of one another.
ACCENTURE POINT OF VIEW
The Smart Workspace is fast becoming a key part of any digital workplace initiative. Several trends intersect at the Smart Workspace, most notably the IoT, AI-related technologies, digital signage/electronic whiteboards, indoor mapping, smart buildings and integrated workplace management systems.

The Smart Workspace market is at an exciting stage, with many potential use cases. Vendors such as AgilQuest, Condeco, Estimote, Oblong Industries and Prysm are offering a range of solutions. Organizations can use these solutions to better understand how people participate in physical spaces, and use these insights to create new capabilities, concerning seating and room management, for example.

Adoption rates are likely to vary based on the ability of organizations to support flexible working models that optimize aspects of the work environment, and how well organizations are able to manage employee privacy concerns. Alongside privacy, security and identity/access management will play a critical role, and anonymizing data is key to help promote adoption of new services. Technological advancements outside the enterprise, in consumer electronics and appliances or smart homes/cities, for example, will also determine adoption levels. Increasing numbers of Smart Workspaces will likely also trigger more forms of consumerization, or BYOx, as employees add their own objects to Smart Workspace environments, and organizations will need to update their policies accordingly.

------------

MEETING SOLUTIONS


Meeting Solutions are real-time collaborative tools that support interactions between participants over a network. Enterprise offerings are available for workers in meeting spaces, at their desks or when mobile, with integrated voice, video, messaging and content sharing.


A strong array of core features, most notably video, audio, content sharing.
Ease of use, and consistent experience across mobile, desktop and meeting space.
Potential for additional business value via new technologies such as meeting virtual personal assistants, natural-language processing and meeting transcription.
Can lead to a more efficient use of resources, for example by removing the need for separate audio conferencing expenses, reducing the costs associated with business travel or improving the time taken to interview job candidates.
Potential to increase engagement and visibility of remote workers, and facilitate the delivery of remote training at scale.

While many IT users prefer a one-size-fits-all approach for strategic meeting platforms, not all vendor solutions are strong enough or broad enough to fulfil this.


The Meeting Solutions market emerged from two formerly distinct markets, web conferencing and group video systems, in response to buyer preferences for converged solutions. What exists now is a richer and more flexible experience compared to limited alternatives such as traditional conference bridges or video communications. These solutions provide an upgrade path from legacy video conferencing silos, enabling faster decision making during internal collaboration.

Many vendors have offerings in this space; some segment their products lines to target and scale to a specific use case, while others offer broad solutions that work for each purpose. Future innovation is likely to come from vendors putting meetings at the center of a platform experience with integrations to other collaboration tools, new methods of intelligence and automation processes, or industry specialization. Cloud office vendors likely have the most potential to dominate the market over the next five years, by integrating content creation products and AI with their meeting solutions, while other vendors focus on particular processes or industries.

-----------------

COLLABORATIVE WORK MANAGEMENT

Collaborative Work Management (CWM) technology supports planning and work execution in teams by combining task and activity planning with conversations, notifications and information sharing along with reporting, analytics and automation.


CWM technology is multi-faceted; supporting structured and preplanned work while also supporting flexible and open-ended collaboration and replanning where necessary.
CWM technology is multi-functional; supporting task scheduling and resource allocation (e.g. timelines and budgets) while also supporting conversations, notifications, dynamic reports and information sharing during execution, ensuring that plans are carried out and amended as needed.
CWM technology often includes sophisticated analysis for optimization, integration, as well as automation of repeated tasks or sequences of tasks.
It can potentially be used by everyone, empowering employees to collaboratively carry out the planning, execution, optimization and increasingly automation of day-to-day work. The user-centric design and persistent interaction capabilities of CWM will appeal to agile work teams that want to coordinate and stay connected at all times.

Some CWM solutions are niche and overly targeted at professional planners and process modelers.

The use of CWM comes with many benefits – project management, process modelling and even automation is brought closer to everyday work, enabling users to coordinate activities and communicate effectively. Teams can work more flexibly than they could when using classic project management or process modelling tools which relied on predefined roles and workflows. The ability to combine preplanned and structured tasks with tasks involving more variability and creativity is an advantage of CWM technologies; dynamism is key, allowing users to ease the burden of managing non-routine work. CWM is increasingly important for increasing business agility.

The CWM market is dominated by specialist products, such as those from Asana, Smartsheet, Basecamp, Redbooth, Trello, Workfront and Wrike. However, conventional project management and business process management vendors are adding more flexible, dynamic and collaborative capabilities.

CWM technologies are similar to, but separate, from Workstream Collaboration tools. Workstream Collaboration creates a persistent, shared conversational workspace that helps groups initiate, organize and complete work – integrating direct and group messages, alerts, notifications, activity streams, files, tasks, bots, and real-time audio and video into searchable groups or channels. These solutions solve the problem of fragmented team communication, while AI-related services are adding greater levels of collaboration automation. Cloud office vendors such as Microsoft (Teams) and Google (Hangouts Chat) compete in the market with Workstream Collaboration specialists (e.g. Slack, Atlassian) and unified communications vendors (e.g. Cisco, Atos, RingCentral).

-------------------

CONTENT COLLABORATION PLATFORMS

Content Collaboration Platforms (CCPs) enable content-centric productivity and collaboration for individuals and teams, either inside or outside the organization. Key functionalities of a CCP include mobile access to content stores, file synchronization, sharing and file search functions.


Benefits including increased productivity, application rationalization, cost savings and digital workplace transformation can all be achieved using a CCP. CCP’s are more secure than email or legacy FTP services for file transfers, or personal services.

File synchronization and sharing capabilities are at the core of CCP products, and are now commodities. Integration with cloud office productivity and collaboration suites (e.g. Microsoft Office 365, Google G Suite) is commonly available, while some products offer native collaborative authoring (e.g. Box Notes, Dropbox Paper). CCP vendors are increasingly integrating their products with workstream collaboration platforms (e.g. Slack, Microsoft Teams, Workplace by Facebook, Cisco Webex Teams) for more conversationally driven collaboration styles.

A CCP can help organizations gain more control of unstructured data and meet GDPR requirements on personal data.
Emerging capabilities are also transforming CCP capabilities - e.g. APIs to build new applications and automate processes, or machine learning functions to automate content tagging and classification.

The market is a fragmented one, with a host of vendor offerings.
Some deployments fail because they lack a clearly defined and appropriate purpose to make them relevant to the work activities of targeted participants.
Other potential challenges include organizational culture clashes, privacy issues, questions about workers’ productivity and content quality.

CCPs, formerly known as enterprise file synchronization and sharing (EFSS) products, have gained a greater range of functionality in recent years. Now supported is collaborative document creation, teamwork, content management and workflow automation, either natively or through integration with third-party tools. Deployments can be in public, private or hybrid cloud, or on-premises. There are many benefits for organizations using a CCP – enabling more modern and collaborative real-time workplaces, more agile data infrastructure and centralized governance, and perhaps most notably, reducing or avoiding inherent security and compliance risks associated with using personal cloud services.

Fundamentally, CCPs are central to digital workplace initiatives and also increasingly relevant for strategic initiatives on digital transformation. Choosing the correct solution is essential for organizations partaking in these initiatives. For example, organizations looking to leverage cloud storage agility could look to vendors such as Box, Dropbox, Google and Microsoft. Those with more stringent data control requirements or a large storage infrastructure should focus on hybrid solutions (e.g. Citrix, Syncplicity by Axway, Egnyte) that allow for greater control over the data and use existing storage investments. Strong requirements for data protection, strict regulations or complex data manipulation requirements could lead to private cloud or on-premises deployments.

---------------------

ON PREMISE ENTERPRISE PLATFORMS

On-premise Enterprise Systems are installed and run on computers on the premises of the organization, rather than at a remote facility such as a server farm or cloud. Off-premises software is commonly called “software as a service" 

----------

BLOCKCHAIN POWERED ERP

Blockchain pushes the existing benefits of ERP systems by providing secure data sharing across multiple organizations and enabling optimization of all operations of several different organizations. A cross-continent ledger sharing architecture enables frictionless business in a unified ecosystem and allows them to control their resources and growth far better. Even though Blockchain ERP technologies offer new ways to exchange value, represent digital assets and implement trust mechanisms, successful enterprise production examples remain rare.


By 2020, 90% of ERP vendors will enhance their offerings with blockchain-inspired capabilities, but these will be largely immature and unproven.
By 2022, 15% of firms involved in B2B commerce will use ERP-based blockchain platforms to transfer value from or to themselves in collaboration with trading partners, rather than have banking or other financial institutions conduct the transfer.

Designing a standard blockchain-ERP system that is confidential and secure is a big task. Thus, companies need to agree on standards and practices for seamless connectivity through such blockchain-based ERP models. Moreover, the user must overcome the slower processing rate to avoid interaction failure between IoT devices and other systems.
80% of the world's corporate data resides in silos. Integrating such amount of data and bringing it to a common platform remains a challenge for companies.


Blockchain continues to generate interest, but from the perspective of ERP is more hype than reality. It is unlikely that blockchain will become a significant part of ERP deployments anytime in the next several years, but tangential applications are possible. Some large organizations are already using blockchain technologies for tracking items outside of ERP processes

Consistency and standard definitions of data are important to support blockchain within ERP systems, across shared systems to track financial transfers or associated contractual terms in smart contracts

Blockchain itself is not a business model but an enabler for efficient and trusted ERP. In a few years, it will be included in integrated solutions by software vendors like Oracle, SAP, and Microsoft for ERP, asset and inventory management, HR, and finance software or service offerings
Blockchain services are expected to grow on a worldwide basis at a CAGR of 71.3% from 2017 through 2022. In effect, by 2025, the business value added by blockchain will grow to slightly over $176 billion, and then surge to exceed $3.1 trillion by 2030.

--------------------
SOFTWARE DEFINED ENTERPRISE SYSTEMS

Moving beyond cloud native ERP, Microservices/ Software defined ERP systems are architecture patterns in which the system is composed of small, independently deployable units communicating with each other using language-agnostic APIs and protocols. A supplemental benefit is the ability to choose and update implementation technology on a per-service basis

Microservices ERP, typically through PaaS or containers are used to build and deliver service-oriented business applications with three primary objectives: agility of delivery, flexibility of deployment and scalability

The term microservices architecture has already built the momentum among technology architects due to the likes of Netflix and Amazon. This has driven improved support for microservices use cases in both commercial and open-source tools and platforms.
Infrastructure as code (IaC) standardizes the setup of infrastructure so there is consistency in the infrastructure deployment process and hence lowers risk in deployment


Businesses expect faster development and configuration speed to deliver. Therefore before use of microservices in business, companies need to build maturity into Agile development and DevOps practices before committing to build use cases

Majority of organizations have manual IT processes and methods which need to be automated to transition for IaC. Significant time and skill is required by IT operations into the foreseeable future for transitioning manual processes into infrastructure as code or even automation


By 2022, at least 30% of current ERP SaaS customers will implement an adaptive application governance and support model, thereby minimizing improvement bottlenecks, moreover 30% of large enterprises will have moved to a platform and product-centric approach with standardized ERP capabilities at the platform core.

At present, the market for microservices ERP and platforms is fragmented. A few vendors aspire to provide a full, opinionated platform stack that covers a majority of the outer architecture capabilities. However, most platform and framework providers focus on a subset of features, and legions of open-source projects and startup vendors are providing niche solutions for independent capabilities. As a result of this state of the market, application leaders looking to adopt microservices for ERP must plan to compose their overall architecture and evolve it over time as the market and available capabilities mature.

The ideal situation for enterprise IT may be a 100% cloud-native application portfolio built using containers and microservices. But most enterprises are still a long way off this ideal position, and the success of their digital transformation efforts may depend largely on how quickly and effectively they can modernize their applications, and house them alongside any remaining legacy apps.

-------------

INTELLIGENT ENTERPRISE SYSTEMS


i-ERP applications use machine learning and advanced analytics built on a large, curated data set to forecast, track, learn, route, analyze, predict, report, and manage these resources and business processes. They feature an assistive and conversational user experience, by automating a set of high-volume repeatable tasks and augmenting (via human-machine interaction) the performance of less frequent, more novel tasks. They are capable of processing, analyzing, and acting on massive volumes of data in real time, using in-memory computing (IMC) technologies. As a system that learns, an i-ERP application must allow for ongoing reconfiguration to enable process refinements and user experience adaptation..


Driven by the need to innovate and turn data into value, 50% of ERP applications will combine templatized industry/line-of-business best practices and artificial intelligence by 2021

IDC predicts that, by 2019, 60% of organizations will measure the changes and impact of resource optimization across people, processes, and new i-ERP technology

Skills for supporting new applications — both packaged i-ERP and custom-built applications — must be designed around augmentation of capabilities and decision making while increasing efficiency. There will be strong competition for IT staff that has demonstrable experience in human machine interfaces, and there will be a rise in the demand for programming skills that will assist with this transition and/or integration. There is a huge skill gap for the companies to be ready for that change.


The evolution of intelligence in delivery is driving organizations to measure the change and impact of resource optimization across people, processes, and new i-ERP systems and applications.

Public cloud ERP providers are engaged in a multipronged approach to provide AI for their consumers. Some are providing AI platforms to allow customers to build their own applications and services to best fit their needs. Others are building AI features into the applications, dictating the usage of AI for their customers while providing a clear path for adoption.

i-ERP and i-apps are rapidly changing the business processes such that they can immediately change a business outcome when deployed. However, technology vendors must continue to invest in SaaS and cloud-enabled software and innovation such as artificial intelligence, machine learning, natural language process, and advanced analytics to help the enterprise be successful in the digital economy

By 2022, 40% of intelligent applications will be part of an intelligent suite of applications. Driven by i-ERP, 65% of the G2000 organizations will have refreshed their systems by 2023  through a process of rationalization, modernization, and transformation.

-------------------

EMBEDDED DECISION ARCHITECTURE (RISK MANAGEMENT & DECISION SYSTEMS)


An embedded decision architecture ERP system moves towards real time ERP by  incorporating event-centric processing rules and decisions. It uses decision models (such as decision tables) and more-complex algorithms (such as neural nets) to process  contextual data coming from various trusted sources and provide for a high rate of events to be analyzed for business events to drive machinery decisions in real time, resulting in a higher consistency of quality in products.


By 2022, 50% of organizations will invest in new intelligent applications with capabilities to facilitate enterprise wide end-to-end workflows, removing white space between existing applications.

Decision support systems will provide real-time visibility into the enterprise and opens the avenue for predictive and proactive mitigation of risk
AI and machine learning can provide insights into how Overall Equipment Effectiveness (OEE) can be improved that aren’t apparent today

Enterprises need to identify core decision areas that need machine learning to be plugged in. There has to be a risk reward for each investment in the automated decision making architecture

Human+Machine need to supplement each other in decision making, with machine handling all repeatable asks in day to day operations while delegating complex queries to user. Skill shortage may arise in such a scenario

As machine learning will be built into the core of enterprise systems, there has to be a governing mechanism to ensure correctness or validity of decision support systems and its recommendations

Companies have turned to structured machine learning to speed up/streamline business processes including financial closing, fraud detection, inventory transaction processing, and automated purchase orders. Software application end users spend roughly 80% of their time on lower-level tasks like invoice matching, transactional processing, and repeat purchase orders. Artificial intelligence can be leveraged to automate many of these tasks, freeing up valuable organizational resources to focus on higher-level strategic tasks. Increasingly, software vendors are going beyond offering a standalone intelligence product and instead embedding intelligence into workflows within software systems.

By 2021, 90% of new packaged enterprise applications will have embedded decision architecture that automatically detects and evaluates conditions and makes decisions about how to respond. Software vendors must work to embed intelligence within the business workflows to unleash the full power of artificial intelligence. Vendors are using embedded intelligence to guide decision making and automate lower level-tasking within the applications

The worldwide enterprise resource management (ERM) applications market is forecast to reach $88.1 billion by 2022, growing at a five-year CAGR of 7.5%.

---------------

IOT ENABLED ERP, THINGS AS CUSTOMERS


IoT enabled ERP solutions help companies improve business outcomes, such as the collection of data from sensors on manufacturing equipment. They integrate with one or more existing applications (e.g. field service management and enterprise asset management) to extend and enhance the execution of specific business processes. It provides for higher ecosystem visibility, improved situational awareness, process efficiency and decision making.


By 2024, at least 50% of enterprise applications in production will be IoT-enabled.
By 2023, at least 35% of midsize to large enterprises will leverage a hybrid cloud-to-edge computing deployment model for at least one IoT project..
Linking the associated IoT endpoints to the business processes flowing through ERP systems represents a new opportunity for both users and vendors. Initiatives such as Industry 4.0 and the Smart Manufacturing Leadership Coalition will help to accelerate the development of new use cases.

As per the market analysts at McKinsey, 40% to 60% of the total value lies in the ability to achieve interoperability between different IoT systems. With numerous vendors, OEMs, and service providers, it becomes really difficult to maintain interoperability between different IoT systems.

IoT workstreams have traditionally focused on dedicated hardware and systems, with little general interfacing to ERP systems. As IoT opportunities increase, however, we are seeing ERP vendors, and some niche PaaS providers, build IoT workflows into their ERP products — for example, Oracle’s JD Edwards Enterprise One Internet of Things Orchestrator, SAP HANA Cloud Platform for the Internet of Things/SAP Vora, Infor Intelligence Open Network, and IFS’s partnership with Microsoft for the Azure cloud platform.

IoT solutions are providing new opportunities to extend and augment business management systems. Modern ERP systems, such as Microsoft Dynamics 365 can take advantage of powerful, secure and easily scalable cloud platforms like Microsoft Azure. Building on this platform, the Azure IoT suite can be quickly deployed through easily adaptable preconfigured solutions for predictive maintenance, remote monitoring and stream analytics, allowing businesses to rapidly arrive at proof of concept and scale to production.

-----------------

CITIZEN INTEGRATION TOOLS

Business users are increasingly performing self-service tasks to integrate data and applications using integration software as a service (iSaaS). Easy-to-use, configurable technologies that enable citizen integrators to build and consume integration flows using cloud-based integration platforms allow business users to solve simple integration issues through direct, "hands-on" involvement.



By 2021, at least 50% of large organizations will have incorporated citizen integrator capabilities into their strategic integration infrastructure.
By 2022, at least 65% of large and global organizations will have implemented a hybrid integration platform..


Main challenge in citizen integration is the ability to integrate with different types of endpoints - the location where a service or application resides. The diversity of characteristics of such endpoints is growing especially with the advent of compute-enabled things such as sensors in IoT scenarios, apps in cars, or home appliances.
Citizen integrators seek to perform integration tasks as a whole, and generally do not differentiate the processes of integrating data and applications, thus spurring application leaders to re-evaluate traditional integration practices.


Demand for seamless integration patterns has driven iSaaS providers to offer citizen integrator tools that combine data and application integration for shared and sustainable benefits to support Hybrid Integration Provider  capabilities.

Citizen IT should focus on areas that can provide maximum value at minimal risk. For citizen integrators, exporting and reimporting via spreadsheets can be a viable option for many cases, yet not an overly secure one. For more-complex needs, new-generation integration platforms have become available that offer more-secure and easier ways to build integration flows between applications.

For Citizen integrators and Tools, the net balance needs to be positive. Augmenting small development teams with citizen developers and integrators is an effective tactic for midsize enterprises to improve capacity and quickly deliver simple but useful features. But to mitigate risk, CIOs must ensure that proper boundaries and operational processes are in place.

------------------

CLOUD NATIVE ERP

Cloud-native ERP solutions allow enterprises to outsource ERP operations, handing the upgrades and maintenance issues over. Cloud-native ERP uses Software as a Service (SaaS), delivered through the web, to provide one set of program code that is delivered exclusively in the cloud. Priority reasons for shift to cloud ERP remain top business agility, faster deployment, cost, and the ability to remotely maintain the system.


By 2021, at least 40% of new service-centric ERP deployments in large enterprises will be cloud SaaS.
By 2025, at least 50% of large enterprises in service-centric industries will successfully implement an "all in" cloud SaaS strategy and demand for operational ERP deployments delivered as cloud services will account for more than 65% of total market spending.

Changing technology adoption scale: Organizations struggle to balance new technologies, ERP investments and changing business needs. ERP strategies must become more flexible and agile to support increasingly dynamic business needs as digital business models evolve
Multicloud or platform integration and underlying security remains a key concern for buyers as well as loss of control over data and compliance issues arising out of data distribution over multiple geographies leads to high risk of failure or regulations.

A 2018 Gartner cloud ERP adoption survey found that 47% of participating organizations had already moved at least one ERP function to the cloud . It also found that 55% plan to move all their ERP functions to the cloud in the next two to three years. Additionally, most participants found that implementing a cloud ERP suite took less time than setting up an on-premises ERP suite, which means that a two-tier ERP deployment can be simpler than it once was
The adoption of SaaS ERP solutions will continue to increase among large and global organizations. Vendors will target customers in that market segment by offering single tenant private cloud deployments. These solutions have more update flexibility for customers to upgrade on their own timeline.
Overall, the ERP software market is expected to grow from $27 billion in 2017 to $35.3 billion in 2022, producing a solid five-year CAGR of 5.5% while the five-year CAGR for cloud ERP software is an exceptional 21.7% through 2022 as on-premise declines by 0.9% in the same forecast period.

-----------------

SOCIALLY INTEGRATED ERP


Social ERP apps mirror the functionality of online social networking tools. The interface acts like Facebook, but it is secured and maintained within the ERP solution. Social ERP tools facilitate collaboration and communication among employees and partners and enables them to proactively solve business problems together.


Personalization of customer profiles and interests based on their social presence
Social initiatives differ from traditional ERP deployments in that workers engage with social media platforms in a “pull” approach, as opposed to traditional business software deployments that are “pushed” onto employees. The “pull” approach – or any approach where an employee has to opt-in – rarely sees 100% adoption amongst a workforce. However, given that traditional ERP systems are deployed internally with a “push” approach, an integrated social media platform would automatically be adopted by all ERP end users, ensuring maximum employee utilization and uniformity.

Companies need to address security concerns while integrating social media into ERP. Using social media can compromise the ability to ensure that critical and potentially damaging information is not accessed by unauthorized people, in addition, the preservation of content from conversations for legal and regulatory purposes is at risk.
Integration of social media tools do not guarantee that workers will share information with others, increasing the degree of intellectual capital entering an ERP solution.

While delivering social software that improves communication, information sharing and collaboration in the context of a work activity can lead to better results; it also creates a gap when people have to work across the organization with other groups, departments and divisions, or even outside the enterprise. The need for some sort of "activity hub" that acts as common ground for these types of interactions is one of the missing pieces in this market.

The proliferation of social media applications means that there are now more options available that can help businesses ensure early adoption. By including social media-type functionality into the features and experience of the ERP user, vendors can make information exchanges and collaboration as gratifying and rewarding as collecting ‘likes’ and ‘followers’.

Organizations recognize the need for improved collaboration and as the next generation of ERP systems emerges, social media applications are proving their value in connecting businesses internally and externally with customers and suppliers. Research shows that organizations with social ERP are 2.5 times more likely to have real-time collaboration across divisions and departments and will have seen profit margins improved by up to 22%.


-------------------
2 TIER ERP

A two-tier ERP strategy is the use of different ERP systems at different layers of the organization. One system potentially serves as the shared-service global backbone (the hub), often for such processes as financials, HR and procurement. A second-tier ERP solution supports geographical or divisional needs (spokes), such as smaller operations — with functionality such as sales, field service and local manufacturing and operations capabilities. Further complex designs may lead to rise of N-tier ERP systems.


Enabling lower cost delivery through the application of “regional or local” ERP solutions where a highly capable higher cost solution is not required.
Organizations are in constant need for more distributed applications and N-tier ERP helps to take advantage of low-cost hardware and the power of internet as a platform for application delivery
The move to n-tier ERP is driven by scalability requirements - the applications needed to support higher transaction volumes, and the increasing use of web/internet technology make the browser the primary interface for application presentation

Visibility to transaction level detail at Tier 2 is more difficult (subsidiaries, product lines, etc.). Higher levels of maturity are required for application and data integration.

Master data management can be a concern for organizations deploying 2 tiers of ERP. Capable MDM is required to maintain single version of truth
Stronger governance is required to manage demand and service providers across multiple tiers.

Two-tier ERP provides companies with the flexibility to preserve the technology supporting the core enterprise while accommodating cloud ERP platforms capable of supporting different business models (including digital and innovative modes), different geographies or shorter release cycles.

Different parts of a business run and change at different speeds and with different capabilities. Forcing all entities to use the same ERP applications may be inappropriate and lead to frustration and undesirable tensions.

Postmodern ERP offers more flexibility in the deployment of business capabilities because they are no longer tied to a single megasuite. Two-tier strategies are facilitated by the advent of cloud ERP, which, for example, minimizes the burden and overhead of setting up multiple data centers.
By 2022, at least 70% of global enterprises will have revisited their single-instance ERP approach because it is not responsive enough to meet requirements for digital business transformation.

-------------
IN MEMORY COMPUTING

ERP system where all the data is loaded in the main memory, and the transactional processing, planning, reporting and analytics are performed against this dataset in real time. In-memory computing (IMC) provides ERP systems with a hybrid transaction/analytical processing architecture where sophisticated analytics can be performed, in real time, directly on the ERP data.


In-Memory Computing is an architecture style where applications can assume all the data that is required for processing is located in the main memory (usually based on DRAM technology) of their computing environments, without compromising data availability, consistency, integrity and durability. IMC-style applications typically experience faster performance and greater processing scalability than those implemented using conventional technologies — even when working on large datasets or event streams.


The expected availability of IMC platforms in the form of cloud services will make IMC accessible to even midsize organizations that currently cannot afford the cost and complexity of aggregating multiple IMC technologies to empower their digital transformation ambitions
Distinct IMC-enabling application infrastructure software is converging to become unified IMC platforms, thus reducing the barriers to adoption for user organizations and independent software vendors

Use of IMC technologies requires a profound reconsideration of some fundamental assumptions in the traditional system or application design and architecture. Ample empirical evidence shows that applications built according to traditional paradigms must be significantly re-engineered or even completely redesigned to fully deliver the potential business value of IMC

IMC is expected to blur the boundaries between ERP, FP&A and financial close capabilities like financial consolidation, planning and forecasting as they leverage live transaction data. Companies have a challenge of replacing existing products and modules by IMC-enabled equivalents


IMC is a key building block of the digital enterprise. The elimination of disk latency, batch processing, and the onerous extraction and synchronization of analytics has enabled large amounts of data to be sifted, correlated and updated in seconds, with no real latency or historical reconciliation issues.

Simplification of ERP platform should also eliminate some of the need for expensive infrastructure investment for vendors and users alike. Adoption will increase as prices fall, and the range of vendors increases.

Using IMC to accelerate analytics processes — such as complex, strategic financial planning and profitability analyses — is likely to deliver moderate to high benefits in near-term. Enabling more-sophisticated analytics directly against transactional data is likely to deliver high benefits. Allied with AI-enabled platforms, IMC may provide several new possibilities for business execution that will go further than simple financial management

In Memory Data Grid adoption will continue to grow because of the ever-increasing need for greater scalability and performance, driven by digital transformation initiatives and by the increasing vendor focus on analytics and hybrid transaction/analytical processing (HTAP).
IMDG market is expected to grow to US$2.15B by 2022 from US$1.1B in 2018.

------------------------
------------

FRAUD DETECTION


Fraud detection applications have continued to mature as adoption grows across market sectors. Demand for fraud detection is highest in markets where money can be lost and criminals can profit, such as banking, insurance, e-commerce and gaming. Several other sectors can benefit substantially from fraud detection, including government benefit programs, collection agencies and traditional treasury applications.   


Fraud detection ensures the safety and reliability of information, assets, accounts and transactions belonging to consumers and businesses. It performs real-time, near-real-time or batch analysis of activities submitted by human and electronic actors.

Fraud protection systems examine access, endpoint profile, behavior and transaction patterns, and compare them with those expected for the individual, role and peer group. Transactions are flagged if they exceed organizational risk tolerance. Online fraud detection market is growing in complexity and demand, and its tools are being used for risk-based authentication and new account fraud prevention.

 
Augmenting identity proofing processes to deter new account fraud.
Protecting the integrity of the login process to prevent account takeover (ATO).
Monitoring payment transactions to detect suspicious activity.
 

Fraud detection continues to grow in banking/payments and large e-commerce markets due to increasing fraud attacks across all channels, including digital and traditional.
Strong authentication systems which have rolled out across the world, have been circumvented and beaten by hackers, thereby strengthening the need for background fraud detection systems that can spot transaction anomalies after a user has been authenticated
The use of vendor solutions for online fraud detection is evolving beyond the traditional use cases of preventing account takeover and assessing the integrity of a payment transaction, as the range of digital brand/customer interactions increases.
 

As fraud detection operates in the context of an application, it cannot detect rogue and potentially fraudulent processes that are external to the application.
Sectors such as government, collection agencies, administration (treasury) and other have been slow in embracing fraud detection as there has been a lack of incentives (i.e., fraud costs are built into the programs) or because of a lack of off-the-shelf fraud detection software for these markets.

Adoption of fraud detection is increasing in countries and industries that previously relied primarily on strong user authentication to control access to sensitive applications and functions. The solutions need to be implemented across all channels, as criminals will gravitate to the least protected channel.

Clients need comprehensive fraud management strategy by viewing fraud detection, identity proofing and authentication capabilities as a cohesive toolset. Future-proofing fraud management processes is also important. This can be achieved by defining the orchestration layer as the key strategic component, then implementing a suite of interchangeable components that can deliver core capabilities around that.

The lines between online fraud detection, identity proofing and user authentication use cases are increasingly blurring with regard to the techniques that can be applied to increase trust in an identity assertion and identify malicious or anomalous activity.

-------------

APPLICATION SECURITY AS A SERVICE

Application security as a service has gained significant momentum as most application security solution vendors offer their capabilities in the cloud. Continued rapid acceptance and use of the products is expected, particularly as organizations adopt DevSecOps approaches more broadly. It also promotes use of multiple technologies at different points throughout the software development life cycle (SDLC) to more completely test the application.


Application security as a service is a group of various application security testing technologies delivered over cloud. It blends static application security testing (SAST), dynamic application security testing (DAST), software composition analysis (SCA) and often interactive application security testing (IAST) or secure coding training into a single offering. The individual tools are integrated within a single enterprise console and reporting framework.

 

High benefits could be derived from Application Security as a service as organizations look for application security services to be delivered through the cloud to support an increasing number of application development projects in an era of shorter-development cycles and cloud-based applications
These solutions can thus be a significant enabler in implementing DevSecOps initiatives, and they promise substantial benefits to the organization in terms of more consistent testing and smoother operations.

Organizations that lack application security skills and resources or that wish to offload the work are considering application security as a service instead of relying on point solutions.
 

The effort to integrate multiple AST types into the software delivery lifecycle (SDLC) is complicated when multiple disparate tools are used.
As application security technologies are at varying levels of maturity, the application security as a service suites may be particularly strong in one technology but lacking in another.
 

Application security as a service enables use of multiple testing technologies at different points throughout the software development life cycle (SDLC) to more completely test the application.

Many vendors have expanded their offerings through new development or acquisition. One-stop-shop application security testing is increasingly the preferred choice for organizations.

Application Security as a service makes it easier for organizations to leverage multiple AST tools because they are managed and operated through a single platform and support comes from a single vendor.

---------------
MANAGED DETECTION AND RESPONSE


Managed Detection & Response (MDR) solutions continue to gain market awareness as an alternative to using an managed security service provider (MSSP). Both large and midsize enterprises are evaluating MDR solutions as means to fill the gaps in their existing capabilities or as a turnkey solution respectively. MDR providers are starting to stake out new territory in response to buyer demands and increasing competition, such as, focusing on specific verticals with niche expertise.


Managed detection and response (MDR) solutions leverage a combination of technologies deployed at the host and network layers, as well as advanced analytics, threat intelligence, and human expertise to deliver 24/7 monitoring, detection and response to threats against customers. MDR providers undertake incident validation and investigation, and remote response services such as threat disruption and containment.


MDR is more focused on threat detection, rather than compliance
Services are delivered using the provider's own set of tools and technologies
Managed detection and response relies heavily on security event management and advanced analytics
While some automation is used, managed detection and response usually involves human intervention
Managed detection and response service providers also perform incident validation and remote response


Demand from midsize enterprises has been particularly strong, as MDR services are viewed as a better fit than procuring security event monitoring services from an MSSP.
Managed endpoint detection and response (EDR) is becoming primarily associated with MDR services. Containment and disruption of threats is becoming a popular response offering and differentiator among managed detection and response (MDR) providers.

Skilled workforce is the biggest challenge faced by MDR providers as well as enterprises.
Cloud service coverage (SaaS and IaaS) is generally immature with a majority of MDR providers.
Unavailability of log management monitoring, monitoring of cloud delivered services and adoption of direct monitoring of SaaS and IaaS are some of the challenges

Organizations of all sizes, particularly those that are less mature and have not invested in threat detection and response capabilities across people, processes and technology, are at risk of being impacted due to the increasingly hostile external threat environment.

Enterprises lacking in- house security resources, as well as enterprises looking to augment their threat detection and response capabilities, should consider MDR services. According to Gartner, the MDR market grew 20% year over year to approximately $600 million in 2018

Threat detection capabilities are still heavily weighted toward the end of the cyber kill chain, with most MDR providers starting at the installation or command and control stages.

MDR providers are not racing to compete with MSSPs here, but customers are pushing the MDR providers to help with other essential security operations functions. Pure-play MDR providers are expanding into complementary areas to fill other gaps in customer security operations capabilities, like vulnerability management and cloud security.

---------------

API SECURITY

API Security is gaining importance as API attacks and breaches are increasing as well as the number of web APIs inherent in architecting or integrating modern applications. API security incidents occur frequently, particularly in the form of data leaks. Many organizations don’t even have an inventory of the APIs they provide, or the APIs they use which makes the issue of API Security even more complicated. 


API Security is defense of web APIs from exploits, abuse, access violations and denial of service (DoS), including Layer 7 DoS. Some web application firewall (WAF) solutions have begun to add API protection, side by side with web application protection. Startups focused on API threat protection have emerged recently. In addition, some API threat protection is provided by API gateway products, including as part of full life cycle API management solutions.


Enterprises are realizing that the impact of an API breach is substantial, if not mission critical. Thus, there is increased awareness and product feature coverage for API Security

Very few established vendors are using machine learning to detect potentially harmful API usage patterns.
There is a need to design and execute an effective API security strategy that includes API security testing and the creation and application of reusable API security policies.

API security market is still immature and widely available solutions provide insufficient API protection, such as simply applying the same attack signatures that they apply to traditional web applications.

API threat protection technologies are making progress, but still lack in areas such as automated discovery and API classification.
Modern application architecture trends complicate API security even more since there is rarely a single “gateway” point at which protection can be enforced.


API Security requires existing API protection infrastructure to enable discovery and categorizations of APIs that need to be protected. A layered security model including volumetric distributed denial of service (DDoS) and bot detection at the edge (typically cloud-based) layer should be implemented.

Gartner estimates that API abuses will move from an infrequent to the most-frequent attack vector by 2022 resulting in data breaches for enterprise web applications.  Although data breaches are a significant concern, API protection is not only needed for APIs delivering data. Transactional APIs, such as in financial services or ad tracking, must also be protected.

Rules should be adaptable based on the client identity and the nature of the API itself. Static rate limits or IP white/blacklists are rarely useful in production or at scale.

--------------

IDENTITY AS A SERVICE (IDAAS)


Identity as a Services (IDaaS) has witnessed a significant growth over the past few years. With increase in remote working environments, cloud-based delivery of identity services is gaining prominence. Cloud-based authentication capabilities are becoming mainstream with many organizations comfortable consuming multifactor authentication (MFA) and other authentication services.


Identity as a Service (IDaaS) refers to cloud-delivered identity services and most IDaaS offerings are strongest in access management (AM) functionality. Other elements of IAM, like identity governance and administration (IGA) and privileged access management (PAM), are also being offered through cloud delivery models, however, they are not at the same maturity level as AM.


Provisioning of users to cloud applications and password reset functionality
User authentication, singe sign-on and authorization supporting federation standards
Identity access log monitoring and reporting

Continued adoption of cloud applications is forcing many organizations to assess the location of their authoritative user data and driving adoption of IDaaS solutions.
IDaaS vendors can't map and re-engineer business processes and navigate organization structures single-handedly. They need strong implementation partners that understand their clients' business and help with fitting IAM processes and tools to the clients' organizational DNA.

Organizations build and deploy new types of applications and services faster than ever. IAM teams are challenged to keep up and support other teams.
Many organizations still have a significant on-premises footprint and use privileged access management (PAM) tools to manage that. Often, privileged access to cloud services is managed by the same on-premises PAM instance.

Identity as a Service (IDaaS) has been undergoing a transformation in the past two to three years, with every IDaaS solution offering an application portal and some kind of multifactor authentication (MFA) capability.

IDaaS for consumer platforms will further evolve and drive integration with other marketing technology solutions as well as fraud management and identity analytics to automate and drive better contextual data mining that can provide more-compelling business insights.

IDaaS can provide standard, consistent IAM functions to support cloud-based applications and users for multiple lines of business with disparate IAM infrastructures inside an enterprise.

For organizations that manage frequent acquisition and divestiture activities, agile IDaaS solutions enable faster completion as compared to software-delivered IAM solutions.

According to Gartner, SaaS-delivered, converged IAM platforms will be the preferred adoption method for IGA, AM and PAM in over 45% of new IAM deployments by 2023.

--------------

DECEPTION TOOLS


Deception Tools can augment or potentially replace threat detection and response (TDR) approaches in important ways, capable of providing low false positive, high-quality data and telemetry that is very valuable and immediately actionable. They force a paradigm shift by approaching threat detection as a “right data” problem, rather than a “big data” problem like SIEM, UEBA or NTA vendors posit.


Deception tools are centrally managed systems for organizations to create, distribute, and manage an entire deceptive environment and its architectural elements. Fake assets such as documents, users, devices, applications, services, and infrastructure that can fool automated hacking tools, or even humans depending on their sophistication, are used to entice, engage and detect an attacker. Modern deception tools use analytics and automation methods to make them easier to deploy and manage as compared to simple honeypot systems of the past.


Decoys like fake networks, fake VLANs and fake subnets
Lures: Fake servers or fake laptops
Honeytokens: Fake data, folders, files, identities or users

Deception systems can service many different buyer types and needs, from augmenting a more robust detection practice at more mature firms to highly targeted vertical specialization needs, such as IoT and medical devices.

Deception tools can be an alternative threat detection solution for smaller organizations not equipped to manage big data platform tools such as SIEM and/or UEBA.

Vendors are still struggling to position their solution within organizations' technology stack and defense/detection arsenal, or lack the incentives for clients to initiate a deception project.

The regulatory landscape does not offer drivers for deception initiatives unlike PCI Security Standards Council clearly spelling requirements for log management
Security professionals are reluctant to engage in these initiatives because of misunderstanding and misperception of the complexity and their history with honeypots.

Deception tools market is still in its early stages but is emerging as a viable and valuable complement to more traditional threat detection solutions. They offer high-fidelity sensors that look real and useful, but are fake and created only for attackers to touch and engage with them.

According to Gartner, 25% of all threat detection and response projects will include deception features and functionality, either embedded in their current vendor’s threat detection technology stack or through pure-play deception platforms in next three years.

Contrary to more traditional approaches to security, where the defender has to be right 100% of the time and the attacker just needs to be lucky once, deception tools can turn this model upside down. Now the attacker has to be right 100% of the time or trip a mine, and the defender just needs to be lucky once and a mine trips.

Low maturity organizations not equipped to manage big data based platforms such as SIEM and/or UEBA should look at deception tools as their main detection tool. Medium maturity organizations already equipped with SIEM, UEBA or NTA solutions should look at deception tools as a complement to detect attacker movements. High maturity, forward leaning organizations should look at deception tools for advanced use cases such as generation local threat intelligence.

---------------

TRADITIONAL ENTERPRISE IDENTITY


Identity governance and administration (IGA) is at the center of most IAM initiatives. The IGA market has reached the point of maturity where mainstream products can fulfill all of the most common use cases. The basic functionality associated with core capabilities is delivered in a relatively consistent manner by products in the market. IGA provides hands-on identity governance capabilities to the business owners of applications for direct accountability of access to their business information.


Identity governance and administration (IGA) is a fundamental building block of an organization's IAM strategy. Core capabilities of IGA include: Identity analytics and reporting, entitlements management, access requests, workflow, access certification, role and policy management, access certification, auditing and fulfillment.


Identity life cycle i.e. maintaining digital identities, their relationships with the organization and their attributes
Entitlement management, maintaining the link between identities and access rights
Access requests - enabling users, or others acting on behalf of a user

IAM technologies are needed to manage customer identities, preferences, and profiles across channels, maintain customer privacy preference across locations, hosting models, and partners and provide adaptive, secure access to sensitive data for employees and partners.

IGA is a mature market in terms of tools delivered as on-premises software; however, adoption of IGA tools delivered as a service is picking up rapidly.
As organizations adopt more SaaS applications and migrate other workloads to the cloud, interest in deploying IGA capabilities in a cloud environment is increasing as well.

As customer-obsessed firms use digital technologies to create new sources of value for customers and to increase operational agility, vital business processes will traverse different user populations, hosting models, and access channels.

In the past two years, new identity types such as actual robots, machines, and robotic process automation (RPA) bots have emerged which introduce new challenges for identity governance and require the same level of management as traditional human identities.

IoT further complicates these requirements because it explodes the number and types of devices and amount of data. This will have an enormous impact on the interface design of IAM systems and the architecture of modern solutions that integrate with IAM systems.

To move away from clunky monolithic IAM solutions, vendors are breaking existing platforms into microservices- and API based offerings that can simplify integration, thus enabling organizations to adapt flexibly to evolving business requirements.

Features such as identity analytics, and indirect fulfillment via IT service management (ITSM) tools are becoming mainstream, and they should be used together with cloud-delivered approaches to reduce the overall complexity.

Improvements in advanced analytics techniques, including machine learning, will result in an emphasis on analytics-led authentication tools, on the model of existing online fraud detection tools.

Decentralized identity technology is emerging and many technology companies are building an open, decentralized trust fabric for identities.

------------------

SIEM

Security information and event management (SIEM) tools are central to threat detection technology even though they traditionally address the response phase after the detection. The security operations centers (SOCs) continue to be built with SIEM as the foundation for threat detection and response services. SIEM solution buyer requirements are driving demand for more sophisticated case and incident management features, as well as ways to measure, track, report on and improve the mean time to detect (MTTD) and mean time to respond (MTTR) to threats.


Security information and event management (SIEM) technology supports threat detection and security incident response through the real-time collection and historical analysis of events from a wide variety of event and contextual data sources. It also delivers compliance reporting and incident investigation through analysis of historical data from these sources.


Collect log data from many different types of operating systems, databases, network security devices and applications
Extract information (e.g. the reporting device, the ID of the user involved, source and target device addresses and ports, event descriptions, etc.)
Interpreting alerts from log activity and creating correlated log data
Notification and alerts based on specific data points found during the log collection and correlation phases
Reporting and dashboards for security metrics and ensuring compliance 

 

SIEM is moving towards cloud-based consumption model with predictable pricing models. This helps in targeting small and midsize business segment, which does not have, nor does it plan to have, the required expertise to manage SIEM 
Use of advanced analytics to supplement the lack of this skill set in organizations and addition of automation features helps augment the capabilities.
The need for early targeted attack detection and response is driving the expansion of new and existing SIEM deployments Advanced users seek SIEM with advanced profiling, analytics and response features.

 

Increasing data volume remains a challenge and adoption of big data technologies, such as Hadoop, NoSQL, Elasticsearch and Kafka to replace legacy data management capabilities is need of the hour.

More sophisticated analytics methods, such as machine learning are required to tackle the constantly evolving hostile threat environment.
Vendors face differentiation as one of the challenges in enhanced analytics detection along with differentiating competitively based on the results provided from their solutions.

Tools such as UEBA and Central Log Management (CLM) often used on top of SIEM tools are extending their capabilities to directly compete with SIEM.

SIEM of the future would be smarter with the growth in the use of algorithms, data science, and advanced analytics, such as machine learning. SIEM products will witness enhancements as vendors will integrate support for behavior analytics as well as integrations with third party technologies.

SIEM tool must have access to non-security data sources that would provide additional business context for security event monitoring (such as user directories, configuration management databases (CMDBs) and vulnerability assessment products).

The integration of tools such as endpoint detection and response (EDR) and network traffic analysis (NTA) with a SIEM solution which is a centralized point of monitoring and investigation for smaller organizations could also be a natural progression for smaller and less mature security operations team.

They require an ongoing investment in resources (budget, expertise and staffing) for both technology operations and security event monitoring to realize their true value.

--------------

DISASTER RECOVERY  AS A SERVICE(DRAAS)

Disaster recovery as a service (DRaaS) has made managed recovery in the cloud an extremely attractive option due to lack of skilled talent base. Enterprises often lack recovery data center, experienced IT staff and specialized skill sets needed to manage a disaster recovery (DR) program on their own. Also, as average numbers of virtualized x86 instances per customer are increasing, large enterprises are also investing in DRaaS solutions.


Disaster recovery as a service (DRaaS) is a productized, cloud-based service where the service provider is responsible for managing virtual machine (VM) replication, VM activation and recovery exercise orchestration.


Management of server image and production data replication to the cloud
Disaster recovery (DR) run book creation
Automated failover and failback between on-premises and the cloud
Network element and functionality configuration as needed during and for recovery operations

DRaaS is increasingly a preferred selection by larger organizations, as many vendors have reported an increase in the size of their largest or most complex customer environments, and an increased volume of servers per client
Growth for DRaaS will be strongest in areas where customers have limited public cloud options in highly regulated industries and where business processes consist of IT systems beyond virtualized x86 environments (such as bare-metal servers, as well as legacy Unix platforms for those providers that support it).
Demand for DRaaS solutions is higher as complexity of IT environments increases and the availability of experienced workforce decreases.

 

The number and diversity of approaches exhibited by DRaaS options on the market cause confusion for potential customers.
The level of complexity in the market is challenging. Each of the choices has different restoration capabilities, service support models, prices and pricing approaches, service levels, networking capabilities, and regional capabilities.

COTS products used by many DRaaS providers are becoming more intuitive and include more options, creating more complicated “build versus buy” decisions, especially as hyperscale clouds become a preferred location for development.
 
Early adopters of DRaaS were typically small organizations that either didn’t have secondary sites or were eliminating redundant data centers in favor of cloud-based solutions. In current scenario DRaaS is considered when planning disaster recovery for virtualized Windows- or Linux-based production applications.

It is used as an alternative to acquisition of additional servers and storage equipment for building out a dedicated or co-located recovery site.

DRaaS should be looked upon as just one possible alternative for addressing in-house recovery and continuity requirements and it should not be assumed that cloud-based recovery services will subsume the use of traditional DR providers or self-managed DR in future. DRaaS worldwide revenue is projected to reach  $3.7 billion by 2022. 

-----------------------
THREAT INTELLIGENCE PLATFORM


As threat intelligence expands beyond traditional security operations use cases, investment in threat intelligence platforms (TIPs) is increasing. End users are continuing to actively investigate opportunities to improve their ability to predict, prevent, detect and respond to issues presented by the prevailing threat landscape. Threat intelligence products and services are proving to be a credible option for all four of these high-level requirements.


Threat intelligence platforms (TIPs) are used to collect, correlate, categorize, share and integrate security threat data in real time to support the prioritization of actions and aid in attack prevention, detection and response. They also integrate and complement existing security technologies and processes like SIEM, incident response and vulnerability management. TIPs facilitate the sharing of machine-readable threat intelligence (MRTI) among multiple stakeholders and disparate groups at wire speed and support the extensive use of open standards like STIX/TAXII.


Accumulation of data from multiple sources like network, endpoints, systems, etc.
Standardization, enhancement and risk scoring of data
Integration with existing security systems
Analysis and sharing threat intelligence through reports and dashboards


Client interest in industry-led (ISAC’s), government (CERTs) and commercial TI has continued to increase and expand. They are now taking a more proactive community information-sharing approach to security.

Larger security teams with well-funded security programs, are the best candidates for a threat intelligence platform use cases
Finance and Government verticals are the main consumers of threat intelligence services from an industries perspective
 

The value of these services is sometimes constrained by the customer’s ability to afford, absorb, contextualize and, especially, use the information provided by the services.
Smaller organizations lack the funding, security maturity level or knowledge of the solution to take full advantage of cloud-based threat intelligence platforms.
The value of these services is sometimes constrained by the customer’s ability to afford, absorb, contextualize and, especially, use the information provided by the services.

The utilization of threat intelligence has expanded beyond traditional security operations use cases and is even being leveraged by other functions within the organization, such as fraud, risk management, human resources and marketing.

Threat intelligence platforms can assist with a number of existing IT security use cases, like threat detection and prevention, anti-phishing, incident response and fraud and threat analytics, as well as new use cases like TI sharing.

Numerous threat intelligence sources and formats are already in use, or there is a desire to increase usage to help orchestrate and automate the use of threat intelligence to help prioritize responses to the prevailing threat landscape.

Assessment of security skills, resources and maturity of the security organization is a prerequisite to investment in threat intelligence platforms to ensure there are viable use cases. The threat intelligence market remains large, with a sprawling number of vendors delivering a range of capabilities.

----------------

USER AND ENTITY BEHAVIOR ANALYTICS (UEBA)

User and Entity Behavior Analytics (UEBA) solutions get more traction and are deployed more widely in organizations, fueled by the demonstrated value of UEBA tools running in production in large environments for many advanced use cases. UEBA will reach organizations that simply do not have the proper skill set to manage the underlying complexity of the advanced analytics of UEBA tools.


User and Entity Behavior Analytics (UEBA) solutions use analytics to build the profiles and behaviors of users and/or entities (hosts, applications, network traffic and data repositories) across time and peer group horizons. Activity that is anomalous to these standard baselines is presented as suspicious, and packaged analytics applied to these anomalies can help discover threats and potential incidents.


A need for organizations to dig deeper into data beyond real-time and shorter-term analysis to longer analysis horizons enhances their requirement for UEBA
The overall spending on UEBA is growing as the market is shifting to embedded UEBA, due to the proliferation of vendors offering UEBA features embedded in their solutions.
The primary use case involves the detection of different categories of threats, achieved through visibility into and analysis of often-correlated user and other entity behavior.

UEBA also monitors unusual cloud resource access and usage, and supports better detection from existing security technology investments like CASB and IAM

Data is lacking on comparative effectiveness of various analytic algorithms (implemented in vendor tools) versus current, real-world threats and problems.
Stand-alone UEBA vendors will face pressure to align with one or two specific security markets, like SIEM platforms, IAM tools, and security incident response platforms (SIRPs).

UEBA deployment time and ongoing operations can be more time-consuming and labor-intensive than promised. Adding custom or edge use cases can be arduous, requiring expertise in data science and analytics

Advanced analytics and the type of machine learning used by vendors with UEBA functionality are key to their success and competitiveness.

Examples of where UEBA is being applied to specific use cases include SIEM Tools, Network Traffic Analysis, Agent-Based Employee Monitoring and Endpoint Detection and Response, Data-Centric Audit and Protection, Cloud Access Security Brokers and Identity Access Governance and Privileged Access Management.

UEBA capabilities are embedded in a wide range of adjacent security technologies, such as cloud access security brokers and identity governance and administration systems, whereas UEBA and security information and event management systems continue to converge.

Vendors are elevating their messaging away from the technical aspects of UEBA features and moving into describing their value proposition aligned to one or more use cases.

---------------
SOFTWARE-DEFINED SECURITY


Software-defined security is now a mainstream concept and security vendors are increasingly shifting more of the policy management out of individual hardware elements into a software-based and programmable management plane for flexibility in specifying security policy, regardless of location. In many cases, SDSec replaces many vendor-specific consoles with a single policy management engine, simplifying and unifying policies across disparate products and services, displacing legacy approaches.


Software-defined security (SDSec) is an umbrella term covering several security processes and controls that benefit when the security policy management is abstracted from the underlying security policy enforcement points, and becomes programmable along with the rest of the software-defined infrastructure. Organizations can use security software, through the SDN controller, to implement, control and manage threats from one single place.

The push towards automation and orchestration is resulting in increased demand for rich APIs for automation, management and control that can be managed by SDSec solutions.
There are multiple areas where SDSec is being applied: software-defined perimeters (also referred to as zero trust network access), software-defined segmentation (micro segmentation/zero trust network segmentation), software-defined data protection, cloud security posture management platforms and cloud workload protection platforms.
SDSec can be used to target specific security trouble spots such as access control and event monitoring.

The Open Networking Foundation (ONF) launched a study to determine how to make SDN more secure, and industry players are starting to incorporate security functionality into their solutions. Still, it is immature and will likely hamper adoption of SDN until addressed in a robust way.
While the technology is mainstream, SDSec solutions are expensive and also complicate the underlying architecture.

Software-defined security has controls designed independently of the physical infrastructure. Virtualizing in this way provides the flexibility and agility to control and quarantine small parts of the network/device.

Software-defined doesn’t mean "software only“ and security hardware will still be needed in some cases for deep inspection at demarcation points and for processor intensive workloads such as SSL/TLS in-line inspection.

Cloud-based consoles are favored for security policy management, removing the need to manage on-premises physical and virtual appliances.

SDSec-based offerings will bring speed and agility to the enforcement of security policy regardless of the location of the user, the information or the workload.

---------------
ARTIFICIAL INTELLIGENCE FOR SECURITY


Artificial Intelligence is being used to enhance cybersecurity defense by investing in machine learning-based tools to augment security analysts. As attackers are beginning to use machine learning (ML) and other AI techniques to power their attacks, enterprise architecture and technology also needs to leverage advanced technology to protect the systems and networks.



Artificial Intelligence for Security (AI)  uses machine learning to help automate threat detection and response which can ease the burden on employees, and potentially help identify threats more efficiently than other software-driven approaches. The relationship between modern cybersecurity solutions and AI has become inextricable.


 

Security industry is facing a deluge of cyberattacks just as the number of devices being connected to the internet is exploding. At the same time, there’s a massive shortage of skilled cyber workers.

AI/ML techniques are ideal for achieving cyber hygiene and shrinking the attack surface at scale, which requires an automated understanding of the intended state of an application.

The market for AI in cyber security is primarily driven by increasing data frauds and cyber-attacks worldwide. Also, increasing number of internet and social media users, digitalization of banking and finance sectors create immense opportunities for the industry players to develop advance AI systems to provide cybersecurity.
 


Even attackers use artificial intelligence in malicious attacks to assault security controls and machine learning for identity deception in phishing attacks.
Gartner estimates that 30% of all AI cyberattacks will use training-data poisoning, AI model theft or adversarial samples to attack AI-powered systems.
AI solutions are still immature and require a great deal of human input before they can be effectively deployed to achieve quantifiable results.

AI and machine learning will automate the response side and will help companies deal with "adversaries that are constantly changing techniques.“ AI will play a role in threat management, helping companies automate detection and incident response. Applying AI and ML in threat intelligence tools helps in identifying patterns to develop more robust and proactive security management systems.

Attacks need to be detected in real time, which requires comprehensive monitoring of physical and virtual infrastructure environments whether they are in the cloud or in an internal data center.

AI’s lack of causal reasoning is why human intelligence, especially from experienced security analysts and incident responders, is still critical. Highly-trained security teams play an important role in detecting, identifying and protecting against a wide range of cybersecurity threats, and will continue to do so for a long time.

--------------
QUANTUM CRYPTOLOGY

Quantum cryptology or Quantum Key Distribution (QKD) provides a quantum-safe mechanism for key delivery that is independent of advances in cryptoanalysis and computing capabilities, whether classical or quantum and is thus not dependent on algorithmic security. The various QKD protocols are designed to ensure that any attempt by an eavesdropper to observe the transmitted photons will indeed perturb the transmission.


Quantum cryptology, also called quantum encryption or Quantum Key Distribution (QKD), applies principles of quantum mechanics to encrypt messages in a way that it is never read by anyone outside of the intended recipient. It takes advantage of quantum’s multiple states, coupled with its "no change theory," which means it cannot be unknowingly interrupted. Performing these tasks requires a quantum computer, which have the immense computing power to encrypt and decrypt data. A quantum computer could quickly crack current public-key cryptography.


Quantum Cryptology is becoming increasingly important because quantum computing is now a reality and conventional methods of encryption no longer apply.
It may be possible to build entire encryption systems that are considered unbreakable, using quantum technology.
For high-value transactions like inter-bank communication and election result transmission, the benefits of QKD are sometimes worth the cost.
By using satellites equipped with high-quality optical links, satellite-QKD can achieve ultra-long-distance quantum communication in the 1000-km range. The significant potential of satellite-QKD for the creation of global quantum networks thus makes it a particularly interesting field of research.


Quantum cryptology is promising but extremely immature. The distance over which a quantum key can be transmitted has extreme limitations.
It lacks features such as identity verification, which make it a poor solution for a firm's eCommerce and general needs.
The system is very complex and changing the underlying protocols for encryption and authentication is highly time intensive.
 

Quantum cryptography has been demonstrated in the laboratory by IBM and others, but over relatively short distances. Recently, over longer distances, fiber optic cables with incredibly pure optic properties have successfully transmitted photon bits up to 60 kilometers.

While conventional, public-key cryptography can be cracked or circumvented in a number of ways, QKD offers companies and government agencies the ability to share confidential, mission-critical data with each other in a fully secure, unbreakable way.

The majority of the QKD options are in the point-to-point category. This means that they manufacture QKD devices as chassis to be installed in a client’s infrastructure.

Quantum Key Distribution (QKD) is a powerful solution to the key exchange problem and is not subject to the same weaknesses that current solutions are subject to. However, there are remaining challenges that must be kept in mind as QKD solutions are deployed into our ecosystems.

--------------
--------------
 

 
 PAAS
 
 Platform as a service (PaaS) is a cloud service that delivers application infrastructure (middleware) capabilities. There are multiple types of PaaS and PaaS capability can be delivered as a provider-managed public or virtual private service, or self-managed private service.


Platform as a Service, often simply referred to as PaaS, is a type of a cloud offering that delivers application infrastructure (middleware) capabilities as a service. Gartner tracks multiple types of PaaS (xPaaS), including: application platform as a service (aPaaS), integration PaaS (iPaaS), API management PaaS (apimPaaS), function PaaS (fPaaS), business analytics PaaS (baPaaS), IoT PaaS and database PaaS (dbPaaS). PaaS capability can be delivered as a provider-managed or self-managed, multitenant or dedicated.

Characteristics

Solution packaged
Shared/standard services
Elastic resource scaling
Self-service
Elastic, term-based pricing (no perpetual license)
Ubiquitous (authorized) network access
Standard UI technologies
Published service interface/API
 
New types of specialized xPaaS offerings are continuing to emerge as more innovation and more business is shifting to the cloud.
Emerging born-on-the-cloud xPaaS capabilities (such as fPaaS – Function PaaS) that are inherently cloud-only —  are increasing customers’ confidence and advancing adoption of PaaS overall. 


Customers that are adopting IaaS or SaaS have increasingly ambitious plans for their cloud investments and require PaaS capabilities to extend their acquired services. While the PaaS market remains significantly smaller in revenue compared to the IaaS and SaaS markets, PaaS is increasingly becoming a selling factor for the other services.


PaaS market remains short on standardization
Need to established best practices and sustained leadership
Slowing adoption by the more risk-averse organizations
Self-managed private PaaS is often difficult to carry out, for organizational and cultural reasons. Enterprises would prefer a provider-managed virtual private PaaS as a suitable alternative.

The leading PaaS providers, including Microsoft, Amazon Web Services (AWS), Google, SAP, Salesforce, IBM and Oracle are strengthening their integrated multi-xPaaS (comprehensive PaaS) service suites, building up their ability to support a broad spectrum of mainstream software initiatives. These and other innovations, plus the increasing synergy of IaaS and PaaS offerings (IaaS+PaaS), drive PaaS offerings forward.

While the less-complete products and confused attempts at the use of PaaS still can create some disappointment, the overall confidence of PaaS in the mainstream market is steadily increasing, putting PaaS onto the path to mainstream maturity.According to Gartner, from 2018 through 2022 the PaaS market is projected to double in size, and nearly half of all PaaS offerings in 2019 will be public cloud only.

IaaS+PaaS and SaaS+PaaS are becoming two differentiated cloud market segments, splitting the PaaS market focus into high productivity (for SaaS+PaaS) and high control (for IaaS+PaaS).

The adoption of various PaaS capabilities by organizations is shifting from cautious and tactical cost-reduction initiatives to more strategic and challenging innovation projects. This trend is a result of multiple drivers, including the increasing maturity of some PaaS offerings, the increasing user familiarity with PaaS, the expectation that PaaS will be the prevailing application platform model going forward and significantly, stronger endorsement of PaaS by the traditionally conservative enterprise mega vendors such as IBM, Oracle and SAP. 

Companies that delay strategic adoption of cloud platform technology and architecture, are at risk of losing loyalty of their customers. Users that delay adoption of cloud platform services (PaaS), will find themselves with expensive vendor lock-in and chaotic handling of their hybrid technology environment.

--------------

CONTAINERS

Containers give developers the ability to create predictable environments that are isolated from other applications. With containers, developers and IT Ops teams spend less time debugging and diagnosing differences in environments, and more time shipping new functionality for users. This also means fewer software bugs, since developers can now make assumptions in dev and test environments they can be sure will hold true in production.

Containers are able to run virtually anywhere, greatly easing development and deployment: on Linux, Windows, and Mac operating systems; on virtual machines or bare metal; on a developer’s machine or in data centers on-premises; and in the public cloud.

By packaging applications and all their operating system dependencies in one easy package, containers make it possible to move applications between cloud environments, just so long as administrators plan the operating environment and application definitions in advance. 


An Application Container is a standard unit of software designed to package and run an application or its components running on a shared operating system on a server. Containers run as resource-isolated processes, ensuring quick, reliable, and consistent deployments, regardless of environment.

Characteristics 
A container includes everything needed to run an application: code, runtime, system tools, system libraries and settings.
Containers are able to run virtually anywhere, greatly easing development and deployment: on Linux, Windows, and Mac operating systems; on virtual machines or bare metal; on a developer’s machine or in data centers on-premises; and of course, in the public cloud.
Containers virtualize CPU, memory, storage, and network resources at the OS-level, providing developers with a sandboxed view of the OS logically isolated from other applications.



For Developers:

Self-service; concentrate on building applications
Use any technology stack or application framework
Create and run any containers in self-defined environments (dev, prod, CI/CD, test, acceptance, performance)
For IT Management:

Run on any infrastructure, private, hybrid or public
Increase resource usage and control infrastructure spend
Centrally manage users, resource constraints and allocations
Enable standardized practices and patterns, e.g. secrets management, load balancing, deployments
For Business:

Faster time to market for new services and products
Lower IT costs
Reduce deployment failure



Customers need to be aware that containers introduce their own complexities. With containerized microservices – loosely coupled services within an application – the volume of objects and metrics to monitor increases substantially. Enterprise organizations adopting containers and microservices will need to invest in machine learning capabilities to effectively manage their IT infrastructure. The latter approach is sometimes termed “AIOps” for “artificial intelligence for IT operations”.


As enterprises increasingly digitize their applications, they have greater requirements around agility, elasticity and automation. Innovative firms want to move to cloud-native infrastructures, but legacy technologies and processes can be a hindrance to transformation.

Because they includes everything needed to run an application and run as resource-isolated processes, containers are highly portable, which enables applications to run in a consistent manner across the software development life cycle, as well as enabling to them to be portable across various on-premises and cloud environments. With their portability and reliability, and the benefits they provide developers in the context of digital transformation, it’s expected that containers will be one of the most widely adopted cloud tools.

----------------

IAAS

Infrastructure as a Service (IaaS) is a standardized, highly automated offering in which computing resources owned by a service provider, complemented by storage and networking capabilities, are offered to customers on demand. Resources are scalable and elastic in near real time and metered by use. 


IaaS is an instant computing infrastructure, provisioned and managed over the Internet. Quickly scale up and down with demand and pay only for what you use. Self-service interfaces, including an Application Programming Interface (API) and a Graphical User Interface (GUI), are exposed directly to customers. Resources may be single-tenant or multitenant, and are hosted by the service provider or on-premises in a customer's data center.

 

Characteristics
Resources are provided as a service
Allows for dynamic scaling and elasticity
Variable cost, usage based pricing model (pay per go and pay per use)
Multitenant architecture
Enterprise-grade infrastructure


IaaS is increasingly used as a general substitute for data center infrastructure, and may drive improved operations, efficiency and cost savings
It is typically used to host traditional business applications, and may even host complex enterprise applications, such as ERP
Increased adoption in non-US markets as the market leaders open data centers in more countries
Most enterprises have begun to adopt IaaS strategically — and have a broad range of workloads on IaaS, including production applications
Public cloud IaaS now represents more than 15% of overall workloads
Midmarket businesses are the most likely to believe that IaaS will replace nearly all of their data center infrastructures during the next five years



Data security and privacy is a major concern for enterprises considering implementing cloud IaaS services
Cloud IaaS is a distributed computing model with inherent ambiguity around where the data resides. This distributed model leads to a perception of higher risk and security 

IT managers are often concerned about becoming "locked in" to a cloud provider, and want the ability to move applications and data between environments


Cloud IaaS is one of the fastest-growing areas of cloud computing adoption. It is a maturing technology with mainstream adoption. Although most organizations use cloud IaaS for only a portion of their workloads, this percentage is growing rapidly. Gartner projects that by 2025, 80% of enterprises will have shuttered their physical data centers in favor of cloud infrastructure services, compared to just 10% today.

An increasing number of cloud IaaS providers also offer a range of PaaS capabilities. When IaaS and PaaS are offered in a fully integrated environment, customers obtain much greater value than with cloud IaaS alone. In theory, it would be possible for an IT organization to build such a service; future technologies such as Microsoft Azure Stack may enable such internal offerings. 

The benefits of IaaS have been driven primarily by the developer empowerment that comes from self-service, the flexibility offered by on-demand infrastructure, and the quality and efficiency of automation.

Gartner projects revenue in the cloud IaaS market to increase to $81.5 billion by 2022, up from $41.4 billion in 2019. Presently the market is highly consolidated to a few vendors but with increase in multi-cloud adoption, this is likely to change.

-------------------

EDGE COMPUTING

Edge Computing has become important because of the increasing requirement that data-intensive processing work be done as close to the location of need as possible, without latency (waiting time).  This may require full-fledged processing and data storage capacity be available at the edge of an enterprise network, rather than at the core of the network or remotely (e.g., in the cloud).

Use-cases vary, depending on the industry being considered. For example, verticals such as telecom are especially relevant due to the immediacy of latency requirements and the ongoing development of 5G networks. In healthcare, the need to have immediate analysis and results available to medical professionals may point to having Edge-based processing available in the hospital itself, rather than being remotely available from a public cloud, for example. In other use-cases, Edge Computing is seen as a way of managing IoT sensor-generated data pertaining to automobiles or devices implemented in smart cities, for example. Autonomous vehicles gather a tremendous amount of data from their surroundings and other devices nearby. If a vehicle’s reaction time is dependent upon instructions from computing resources at the core of the network, the slightest delay could literally be a matter of life or death. Smart cities, with vital data being collected from security cameras and other devices, may also require on-the-spot processing to manage a variety of security threats and other urgent situations.

With data-driven Digital transformation occurring in all sectors, the need for Edge Computing capabilities is only expected to grow over the next several years. 

Edge Computing is the deployment of IT resources at peripheral or edge locations, relative to where an organization's central or core IT is located. This includes locations such as remote office/branch office and mobile access points. Distance is not a critical aspect of defining edge versus core; for example, if video surveillance (supported by IT infrastructure) for a large datacenter is segregated from the core IT equipment in that datacenter, yet resides in the same physical building, albeit outside of the purview of the core IT team, then that can be thought of as an edge use case, since it is not part of the core infrastructure.


An "edge" is comprised of servers extended as far out as possible on a digital network, to reduce the time it takes for users to be expediently served.
Edge Computing encompasses various networking technologies including peer-to-peer networking or ad hoc networking, as well as various types of cloud setups and other distributed systems.
Another predominant type of Edge Computing is mobile Edge Networking or Computing, an architecture that utilizes the edge of the cellular network for operations.


Edge computing has been touted as one of the lucrative, new markets made feasible by 5G networks. 5G requires a vast, new network of (ironically) wired, fiber optic connections to supply transmitters and base stations with instantaneous access to digital data (the backhaul).  As a result, an opportunity arises for a new class of computing service providers to deploy multiple µDCs adjacent to radio access network (RAN) towers, perhaps next to, or sharing the same building with, telco base stations. 
These data centers could collectively offer cloud computing services to select customers at rates competitive with and features comparable to, hyperscale cloud providers such as Amazon, Microsoft Azure, and Google Cloud Platform.


Regarding the physical infrastructure of edge data centers, widespread application of the topology and explicit application and networking architectures are not yet common outside of vertical applications, such as retail and manufacturing.
Servers capable of providing cloud-like remote services to commercial customers, regardless of where they're located, need high-power processors and in-memory data, to enable multi-tenancy.  Probably without exception, they'll require access to high-voltage, three-phase electricity.  That's extremely difficult, if not impossible, to attain in relatively remote, rural locations.

For widely distributed Internet-of-Things applications such as Mississippi's trials of remote heart monitors, a lack of sufficient power infrastructure could end up once again dividing the "have's" from the "have-not’s.”


Think of Edge in terms of opportunities at multiple edges/endpoints: Edge Computing is evolving in tiers with opportunities at hardware to software, data hubs, and applications such as smart cities, autonomous transportation, and video orchestration.
Focus on tooling to deliver and drive businesses of future: Software and hardware developers need new tools, frameworks, and support to build next generation edge apps, services, and deliver hyper-personalized experiences.

Edge will continue to be an ecosystem play: Participate in building standards along with global standards bodies and industry specific consortiums globally for development of technology and advocacy/advisory.

Partner with Silicon vendors  as they are championing Edge as next big opportunity after smartphones: Vendors like ARM are developing new architectures, solutions by embedding AL/ML  and are eyeing a bigger role in distributed computing and data centers.
Aim to build and/ or source requisite skills and talent: Patents, developers, technologists, architects and UX specialists. Follow multi-pronged strategy like internal training & development acquisition, hackathon & crowdsourcing and collaboration with academia and industry, to fulfil skills/ talent needs.

-----------------

CLOUD-ENABLED AI SERVICES


loud-enabled AI services help simplify Artificial Intelligence (AI) efforts by eliminating the complexity of architecting a Machine Learning (ML) system and by providing pre-trained systems tailored to specific use cases. In most AI services, the pre-trained algorithm is a shared resource, with broad, generalized training based on data gathered by the provider. 


Cloud-enabled AI services run ML/AI within cloud-based infrastructure. These are purpose-built, multitenant, elastic, and wrapped in lightweight APIs and/or easy-to-use UIs. Examples of these services include NLP, sentiment analysis, image recognition and automated machine learning model creation.

 

Characteristics
Allows development teams to add conversational capabilities
Identify people and assets in images or video feeds
Automate the building of machine learning models


Continued traction and acceptance of AI applications using cloud services by data scientists and developers, will move this technology to next level of maturity.
Cloud service providers offer packaged APIs and tools that make it easier for developers to integrate AI capabilities into existing applications
Availability of specialized hardware instances with AI-optimized chips and large amounts of data storage makes cloud an ideal environment for enterprises to build/deploy AI applications without the risks, costs and delays of conventional on-premises procurement.
Pretrained AI cloud services often require no (or limited) data science expertise.
Features like automated algorithm selection and training-set creation will offload some of the complexity of the project and leverage existing expertise on operating cloud services.


Costly and Complex
Public cloud services can reduce the complexity of on-premise implementation of AI services, reduce costs, and make ML and AI more readily available to modern enterprises. However, enterprises struggle to realize exactly how they can or should apply these technologies.
Vendor lock-in is another concern for enterprises adopting AI cloud platforms. It may be difficult for clients to use another vendor's services or to use open source tools for other tasks.



This is a time of innovation driving market differentiation. At the forefront are AI services and technologies that are transforming business strategy, customer interactions, and workforce management, enabled by enormous data stores. AI services built on a cloud platform can be easily accessed, are highly scalable, and invite continued innovation due to the relatively low cost of development and delivery.

AI cloud services and resulting models allow businesses to unlock value in their data repositories that give more precision to business decisions, facilitate automation of business processes and accelerate workflows to enhance the responsiveness of the business to opportunities and risks surfacing in the business.

The ability of language services to make business information available for the asking or text analytics to discover hidden value in text documents or video content analysis to identify where specific people or information appear in a video are just a few examples.


---------------------

HYBRID / MULTICLOUD SOLUTIONS

Hybrid Cloud Solutions enable IT solutions to easily extend from data centers to public clouds or vice versa. One approach involves the vendor building or using infrastructure within a cloud provider's data centers. Another approach brings services that are available in a public cloud to a private location, used as-a-service experience consistent with public cloud consumption.


Hybrid cloud computing comprises public and private cloud services that operate as separate entities but are integrated. It has self-service interfaces and is delivered as a shared service using internet technologies. Multi-Cloud computing is a special case of "hybrid cloud computing" and it refers to the deliberate use of cloud services from multiple public cloud providers. 

Characteristics

Cost Reduction
Leverage Ecosystems
Lightweight Applications
Intelligent Capabilities
Speed


Hybrid Cloud Solutions allow enterprises to:

Move workloads to public cloud and take advantage of that platform’s scalability, while keeping operational consistency with its on-premises environment, enabling it to maximize the organization’s existing IT skillsets & tools. (VMware Cloud on AWS), or
Spin up the same cloud platform that the cloud provider runs in its public cloud inside the enterprise’s private, on-premises data center, thereby keeping a consistent set of tools and services available and making Hybrid Cloud management easier. (MS Asure Stack)
Multicloud computing has the potential to lower the risk of cloud provider lock-in, specify functional requirements that a business unit may have and provide service resiliency and migration opportunities.


Issues like cross-cloud storage performance remains critically limited because of insufficient WAN bandwidth and long latencies
Provide freedom from data center and deployment model choices for any given application, but not from vendor lock-in
Lack of common APIs among cloud service providers to support migration of a new application from one deployment model to another without changing the application


Hybrid cloud articulates enterprises' desire for consistencies between their public and private cloud environments. Until recently, a choice made at provisioning time became a near-permanent choice for the life of that workload. Over the years, moments of short-lived excitement to tackle this inconsistency have occurred. Since that time, there's been a desire for consistency between public and private cloud worlds. 

Hybrid solutions target the increasing desire for cloudlike operations, agility and operating expenditure (opex) models. Many organizations are challenged to execute on these desires, because they need to maintain private infrastructure for certain workloads and applications.

In addition, customers are seeking to augment their cloud skills, tools, processes and knowledge by leveraging a managed service provider to reduce the complexity of adopting hybrid and multicloud. Managed hybrid cloud hosting is a versatile option for workloads that have operational, privacy, security, location, dependencies or other issues that complicate the use of public cloud.

--------------------

SERVERLESS COMPUTING

Despite the name, serverless computing does not actually involve running code without servers. The name serverless is used because the business that owns the system does not have to purchase, rent or provision servers or virtual machines for the back-end code to run on.


Serverless Computing is an architecture where code execution is fully managed by a cloud provider, instead of the traditional method of developing applications and deploying them on servers. It adds another layer of abstraction atop cloud infrastructure, so developers no longer need to worry about servers, including virtual ones in the cloud. Serverless is also known as Function as a Service (FaaS/fPaaS).

Characteristics
Function-as-a-Service
Event-driven coding
Scalable services
Billing per invocation
API gateway integration
Support for DevOps and tooling


Serverless computing can accelerate the development process. It allows a developer to create arbitrary business logic independent of a specific application, package it and deploy the package to a runtime for execution without having to understand or specify the underlying compute infrastructure. The developer merely interacts with the serverless service's API or development tools

Some serverless providers offer function libraries to further accelerate development, with provisioning and autoscaling on-demand
Serverless architecture helps developers assemble digital experiences from granular parts by chaining units of arbitrary compute together and changing their execution order without having to recompile or redeploy an entire application

With “microbilling,” services like Function-as-a-Service charge fractions of cost for the compute resources. This model is the extreme of cloud’s pay-per-use economics and is popular as it reduces the upfront cost of experimentation and prototyping


Developers may be relieved of many DevOps infrastructure management issues, but they have to manage new higher-level application operation issues
No out-of-the-box local testing environments for many of these services, so teams start by editing and uploading in a repetitive cycle as they refine their code
Neither an easy migration path for existing applications, nor it is easy to operate the hardware, management and orchestration software to support serverless


The adoption of serverless computing in enterprise is still quite nascent, due to the immaturity of the technology for general purpose enterprise workloads as well as the fact that a majority of workloads today are "request-driven" rather than "event-driven."

However, event-driven workloads will grow in importance as next generation front-ends will be driven by new technologies like IoT devices, AI-enabled user experience platforms and virtual reality applications.

At their current capabilities, serverless offerings are not yet a true substitute for Virtual Machines (VMs) or containers for general purpose use cases. However, their mainstream adoption within the data center could be slowed by the fact that product innovation and DevOps teams are primarily driving the adoption of serverless models within the enterprise. Crucially, most of the deployments are happening outside the scope of I&O (Intelligence and Operations).

The serverless model is emerging as the native cloud platform architecture in digital business and these delivery models will eventually expand to encompass mainstream workloads.

The adoption is expected to increase, with many of the organizations planning to increase the frequency and scope of serverless technology application in their business within the coming year, and also some expecting to more than double their use year-over-year in each of the next five years.

----------------------

UCAAS

Cloud Unified Communications (UC), also known as Unified Communications as a Service (UCaaS), is provided over multitenant or virtualized infrastructure that is owned, maintained and hosted by the service provider. The core functions include integrated Instant Message (IM) and presence, telephony, messaging, meeting solution (includes audio, web, video conferencing) and mobility.

UCaaS can elevate the contact center experience by integrating customer contact across multiple channels. This allows a business to better see and analyze the customer journey, ensuring customer satisfaction and reducing churn. 


Unified Communications as a Service (UCaaS) refers to a service model where providers deliver different telecom or communications software applications or services, generally over the global IP network. The core functions of UCaaS include integrated — IM and presence, telephony, messaging, meeting solution (which includes audio, web and video conferencing) and mobility.

 

Characteristics
Unified Messaging
Presence Technology
Online Meetings
Collaboration Tools
Conferencing (audio, Web and video)


UCaaS involves the integration of real-time communication services (including voice, instant messaging and video conferencing) with less time-centric services (such as voicemail, email and text messaging) on a single platform that provides a consistent user interface and experience across multiple devices.
UCaaS solutions enable organizations to support BYOD, mobility, remote work and other workplace transformation initiatives.

Delivers continuity of operations through real-time proactive system monitoring, reporting and data traffic analysis; incident and problem resolution; fault tolerance with multi-site redundancy and high availability infrastructure Offers better integration with other SaaS offerings, such as Salesforce and ERP systems, allowing enterprises to leverage the best of clouds from one platform


Most UCaaS solutions work well for standard knowledge workers but some solutions are limited when compared to an on-premises and hybrid PBX for supporting more demanding or unique use cases (for example, supporting power users, high survivability or specialized devices)

Enterprises with locations outside the top 30 or so country markets may stay on-premises because they cannot find a UCaaS provider that adequately supports the countries they operate in UCaaS support in Eastern Europe, Latin America, the Middle East and Africa and parts of Asia/ Pacific is not consistently available


UCaaS adoption increased across all market segments with users adopting solutions that span multiple countries and global geographic regions. From 2017 through 2022, the rate at which organizations deploy cloud telephony will more than double, from about 14% of users to 32% of users.  AD&D professionals are increasingly sourcing Unified Communications technology as a cloud service rather than on-premises. In addition, hybrid configurations are on the rise among enterprises for investment protection and migration planning for their current UC&C solutions.

UCaaS providers are focusing on Microservice-based architectures built on public clouds to accelerate technology innovation, increase operational efficiency and reduce hurdles to global expansion.

The market is increasingly dominated by mega-vendors that couple their UC offerings with a bundle of capabilities not directly related to the core UC functions (such as office productivity tools, network services, professional services, network infrastructure, etc.). However, for many enterprises, these broader, mega-bundled solutions are not as strong a UC offering as those from more focused best-of-breed competitors.

Selling additional professional services to customers migrating from time-division multiplexing (TDM) voice to IP voice and unified communications and from on-premises platforms to hosted services represents opportunities for services providers.

----------------------
SAAS

Software as a Service (SaaS) is the application service layer within cloud computing. The application software is owned, delivered and managed remotely by one or more providers. The provider delivers a solution based on uniform application definition and on a sharing model at one or more layers of the application stack. SaaS is purchased on a pay-for-use basis or as a subscription based on usage metrics.

Software as a service (SaaS) is a model for the distribution of software where customers access software over the Internet.

 

Characteristics
Scalability
Easy to use
Saves Time/Costs
Enhanced Security


SaaS does not require a capital investment in hardware and licenses, which further reduces its initial costs.
SaaS is a perfect choice for organizations that do not have the IT resources to deploy and maintain on-premises software. 
Across nearly all categories of enterprise applications, new and add-on deals are SaaS. For systems of engagement — where collaboration, agility, speed, and usability are at a premium — SaaS has widespread and growing adoption

As enterprises continue to migrate core apps to the cloud, SaaS is an option for immediate deployment and compatibility in a cloud environment, replacing or extending legacy apps that would not function correctly in the public cloud
The largest and most mature SaaS markets are Customer Relationship Management (CRM), Enterprise Resource Planning (ERP) and Office Suites like Microsoft Office 365


Despite SaaS solutions having been in the market for two decades, many SaaS apps are still limited in what they expose for implementation and integration; use proprietary tools; and rely on a fragmented network of partners for implementation and integration.

Concerns like security, privacy, and integration hold back enterprises to deploy core business applications especially in regulated industries like healthcare and financial services SaaS landscape is still highly fragmented; the typical company uses dozens if not hundreds of SaaS solutions. This multivendor environment means additional costs in areas like vendor management, contracting, integration, provisioning, end user support, upgrade management, testing, and workflow.

Governance is a major challenge for SaaS. Usually the users access SaaS resources outside the knowledge and control of enterprise, introducing unapproved software that can lead to unnecessary risks. Shadow IT undermines the effort and poses a real challenge for an organization.
 

The utilization of SaaS, and the maturity of many existing offerings, will continue to evolve rapidly for at least another 10 years. Gartner indicates that the SaaS market will reach $144bn by 2022.

While SaaS gains mainstream acceptance, the range of applications and operational control technologies that leverage and underpin it are progressing at varying velocities and states of evolution.

SaaS is a double-edged sword — while it brings incredible benefits, it also creates formidable challenges that take the roles and responsibilities of IT to new extremes. In many ways, the IT operational challenges, risks, support service model, and gaps in controls stand in the way of enterprises looking to make big gains from SaaS.
SaaS is often considered as a single industry, but this is a false representation. In reality, SaaS is a single cloud tier, but it is broken into many different markets and segmented by the actual business process that a given solution satisfies.

--------------

CLOUD SECURITY

Cloud security addresses the processes, technologies and services used to control the security, compliance and usage risks of public and private cloud-based systems.

Cloud access security broker (CASBs) provide shadow IT discovery, a consolidated view of an organization’s cloud service landscape and details about the users who access data in cloud services from any device or location.  For sensitive data such as personally identifiable information, intellectual property, and data subject to regulatory compliance (e.g., HIPAA, PCI-DSS), Cloud Security Gateways (CSG) solutions can monitor data in transit.


Cloud Security refers to a broad set of policies, technologies and controls deployed to protect data, applications and the associated infrastructure of cloud computing.

 

Characteristics
Top-of-the-Line Perimeter Firewall
Intrusion Detection Systems with Event Logging
Internal Firewalls for Individual Applications and Databases
Encryption is a common cloud security mechanism, and customer-controlled keys are growing in significance.
Software-defined host and network security approaches are becoming the norm in virtualized private clouds and infrastructure as a service (IaaS).


Traditional security tools can’t effectively monitor data moves from and within the cloud. This can lead to a failure to identify fraudulent use of data, unauthorized downloads and malwares. Cloud security solutions offer the tools to keep cloud data and apps secure, especially when data moves between cloud workloads and apps.
Cloud-specific security tools (e.g., CSPMs and CASBs) and cloud-relevant security services (e.g., SaaS-delivered IAM) are providing new control approaches for cloud (and especially multi-cloud) environments that otherwise are not conducive to centralized analysis of security configuration, user behavior or threat monitoring.
Emerging cloud security technologies such as secure access service edge (SASE) will enable new digital business use cases (such as digital ecosystem and mobile workforce enablement) with increased ease of use, while at the same time reducing costs and complexity via vendor consolidation and dedicated circuit offload
Organizations are using SaaS to fill gaps in enterprise identity and access management (IAM) portfolios and IAM staffing functions and to achieve faster time to value.


Increasing cloud deployments and multiple cloud implementations are creating more challenges for cloud security, including data monitoring, anomaly detection and intercepting bad behaviors
Lack of cross-platform security support
Enterprises use cloud access service brokerage services for adding multifunctional and multi cloud security. It creates a dependency for the organization's cloud applications and creates performance, availability and functionality risks


Cloud Security becomes more important as mission-critical apps move to the cloud and use of cloud services has become more mainstream. 

As on-premises technology continues to be less effective at protecting data stored in the cloud against a new class of threats, improved cloud malware detection, monitoring, data loss prevention, and encryption will dictate which providers will lead.

Security within IaaS is a quickly shifting landscape. IaaS vendors are offering security features, but they are usually not as fully featured or easily integrated with third-party solutions. As IaaS adoption increases, so too do the numbers of enterprises sourcing IaaS from multiple vendors; such native solutions are not interchangeable. The ability for third-party security solutions to integrate with native controls is still emerging.

Firms increasingly prioritize cloud security investments to meet regulatory requirements, reduce security costs, and protect their critical data.

-----------------














 
 











































































----------------------------

-----------------------------
